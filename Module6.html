
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Module 6: Supervised Machine Learning - Classification</title>

  <!-- Icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css"/>

  <!-- Chart.js -->
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>

  <!-- MathJax -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <style>
    /************************************************************
     * THEME: Aurora with Day/Night Mode (KEPT EXACTLY THE SAME)
     ************************************************************/
    :root {
      /* Night Mode (Default) */
      --bg0: #070A12;
      --bg1: #0B1020;
      --panel: #0F172A;
      --panel2: #0B1226;
      --card: #101B34;
      --text: #EAF0FF;
      --muted: rgba(234, 240, 255, 0.75);
      --muted2: rgba(234, 240, 255, 0.62);
      --border: rgba(148, 163, 184, 0.14);
      --shadow: 0 24px 60px rgba(0, 0, 0, 0.35);

      --primary: #60A5FA;
      --secondary: #A78BFA;
      --accent: #F472B6;
      --success: #34D399;
      --warning: #FBBF24;
      --danger: #FB7185;
      --cyan: #22D3EE;

      --radius: 16px;
      --radius2: 22px;
      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;

      --ring: 0 0 0 4px rgba(96, 165, 250, 0.22);
      --ring2: 0 0 0 4px rgba(167, 139, 250, 0.20);

      /* Theme toggle variables */
      --mode-icon: '\f186'; /* moon */
      --mode-label: 'Night Mode';
    }

    /* Day Mode Theme */
    [data-theme="day"] {
      --bg0: #F8FAFC;
      --bg1: #F1F5F9;
      --panel: #FFFFFF;
      --panel2: #F8FAFC;
      --card: #FFFFFF;
      --text: #0F172A;
      --muted: rgba(15, 23, 42, 0.75);
      --muted2: rgba(15, 23, 42, 0.62);
      --border: rgba(148, 163, 184, 0.25);
      --shadow: 0 20px 40px rgba(0, 0, 0, 0.08);

      --primary: #2563EB;
      --secondary: #7C3AED;
      --accent: #DB2777;
      --success: #10B981;
      --warning: #F59E0B;
      --danger: #EF4444;
      --cyan: #06B6D4;

      --ring: 0 0 0 4px rgba(37, 99, 235, 0.15);
      --ring2: 0 0 0 4px rgba(124, 58, 237, 0.15);

      --mode-icon: '\f185'; /* sun */
      --mode-label: 'Day Mode';
    }

    * { margin: 0; padding: 0; box-sizing: border-box; }
    html { scroll-behavior: smooth; }

    body {
      font-family: system-ui, -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
      background: var(--bg0);
      color: var(--text);
      line-height: 1.7;
      min-height: 100vh;
      transition: background-color 0.35s ease, color 0.35s ease;
      overflow-x: hidden;
    }

    /* Dynamic background gradients */
    body:not([data-theme="day"]) {
      background:
        radial-gradient(1200px 520px at 15% 0%, rgba(96, 165, 250, 0.18), transparent 55%),
        radial-gradient(900px 520px at 90% 10%, rgba(244, 114, 182, 0.16), transparent 55%),
        radial-gradient(900px 520px at 75% 90%, rgba(34, 211, 238, 0.10), transparent 55%),
        linear-gradient(180deg, var(--bg0), var(--bg1));
    }
    body[data-theme="day"] {
      background:
        radial-gradient(1200px 520px at 15% 0%, rgba(37, 99, 235, 0.08), transparent 55%),
        radial-gradient(900px 520px at 90% 10%, rgba(219, 39, 119, 0.08), transparent 55%),
        radial-gradient(900px 520px at 75% 90%, rgba(6, 182, 212, 0.06), transparent 55%),
        linear-gradient(180deg, var(--bg0), var(--bg1));
    }

    .container { max-width: 1400px; margin: 0 auto; padding: 22px; }

    /************************************************************
     * THEME TOGGLE
     ************************************************************/
    .theme-toggle-container {
      position: fixed;
      top: 20px;
      right: 20px;
      z-index: 1000;
    }
    .theme-toggle {
      display: inline-flex;
      align-items: center;
      gap: 10px;
      padding: 12px 18px;
      border-radius: 50px;
      background: var(--panel);
      border: 1px solid var(--border);
      box-shadow: var(--shadow);
      cursor: pointer;
      font-weight: 600;
      color: var(--text);
      backdrop-filter: blur(10px);
      transition: transform 0.2s ease, box-shadow 0.2s ease;
      border: none;
      font-family: inherit;
    }
    .theme-toggle:hover { transform: translateY(-2px); box-shadow: 0 8px 25px rgba(0, 0, 0, 0.15); }
    .theme-toggle::before {
      content: var(--mode-icon);
      font-family: 'Font Awesome 6 Free';
      font-weight: 900;
      font-size: 1.1em;
    }
    .theme-toggle .label { font-size: 0.9em; opacity: 0.9; white-space: nowrap; }

    /************************************************************
     * HEADER
     ************************************************************/
    header {
      background: linear-gradient(135deg, rgba(15, 23, 42, 0.90), rgba(16, 27, 52, 0.92));
      border: 1px solid rgba(148, 163, 184, 0.18);
      color: var(--text);
      padding: 28px;
      border-radius: var(--radius2);
      box-shadow: var(--shadow);
      margin-bottom: 24px;
      position: relative;
      overflow: hidden;
      backdrop-filter: blur(10px);
    }
    body[data-theme="day"] header {
      background: linear-gradient(135deg, rgba(255, 255, 255, 0.95), rgba(248, 250, 252, 0.98));
      border: 1px solid rgba(148, 163, 184, 0.25);
    }
    .course-info h1 {
      font-size: clamp(24px, 2.8vw, 38px);
      margin-bottom: 8px;
      letter-spacing: -0.5px;
      line-height: 1.2;
      background: linear-gradient(135deg, var(--primary), var(--secondary));
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
    }
    .course-info h2 {
      font-size: clamp(16px, 1.8vw, 20px);
      font-weight: 400;
      opacity: 0.9;
      margin-bottom: 12px;
      color: var(--muted);
    }
    .badge {
      display: inline-flex;
      align-items: center;
      gap: 10px;
      padding: 10px 18px;
      border-radius: 50px;
      font-weight: 700;
      letter-spacing: 0.2px;
      border: 1px solid var(--border);
      background: rgba(255, 255, 255, 0.08);
      width: fit-content;
      margin-top: 10px;
    }

    /************************************************************
     * NAVIGATION
     ************************************************************/
    .module-nav {
      display: flex;
      gap: 10px;
      margin: 20px 0;
      flex-wrap: wrap;
    }
    .nav-btn {
      display: inline-flex;
      align-items: center;
      gap: 8px;
      padding: 12px 20px;
      border-radius: 50px;
      background: var(--panel);
      border: 1px solid var(--border);
      color: var(--text);
      font-weight: 600;
      cursor: pointer;
      transition: all 0.2s ease;
      text-decoration: none;
      border: none;
      font-family: inherit;
      font-size: 14px;
    }
    .nav-btn:hover { transform: translateY(-2px); border-color: var(--primary); box-shadow: var(--ring); }
    .nav-btn.active { background: linear-gradient(135deg, var(--primary), var(--secondary)); color: white; border: none; }

    /************************************************************
     * MAIN CONTENT
     ************************************************************/
    .module-content { display: none; }
    .module-content.active { display: block; animation: fadeIn 0.5s ease; }
    @keyframes fadeIn { from { opacity: 0; transform: translateY(10px); } to { opacity: 1; transform: translateY(0); } }

    .panel {
      background: var(--panel);
      border-radius: var(--radius);
      border: 1px solid var(--border);
      box-shadow: var(--shadow);
      overflow: hidden;
      margin-bottom: 24px;
      transition: transform 0.2s ease, box-shadow 0.2s ease;
    }
    .panel:hover { box-shadow: 0 20px 40px rgba(0, 0, 0, 0.1); transform: translateY(-2px); }
    .section { padding: 24px; }
    .section h3 {
      color: var(--text);
      margin-bottom: 20px;
      font-size: 1.4rem;
      display: flex;
      align-items: center;
      gap: 12px;
      padding-bottom: 12px;
      border-bottom: 1px solid var(--border);
    }
    .section h4 {
      color: var(--text);
      margin: 20px 0 12px;
      font-size: 1.2rem;
      display: flex;
      align-items: center;
      gap: 10px;
    }
    .section p { margin-bottom: 15px; color: var(--muted); }

    /************************************************************
     * EXPANDED CONTENT STYLES (SIMPLIFIED FOR STUDENTS)
     ************************************************************/
    .expanded-content {
      margin: 20px 0;
      padding: 20px;
      background: var(--panel2);
      border-radius: var(--radius);
      border: 1px solid var(--border);
    }

    .numeric-example {
      font-family: var(--mono);
      background: rgba(15, 23, 42, 0.15);
      padding: 15px;
      border-radius: 8px;
      margin: 15px 0;
      border: 1px solid var(--border);
    }
    body[data-theme="day"] .numeric-example { background: rgba(15, 23, 42, 0.05); }
    .example-title {
      color: var(--accent);
      font-weight: 600;
      margin-bottom: 10px;
      display: flex;
      align-items: center;
      gap: 8px;
    }

    .formula-with-calc {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 20px;
      margin: 20px 0;
    }
    @media (max-width: 768px) { .formula-with-calc { grid-template-columns: 1fr; } }
    .formula-box {
      background: rgba(15, 23, 42, 0.1);
      border: 1px solid var(--border);
      border-radius: 12px;
      padding: 20px;
      margin: 16px 0;
      font-family: var(--mono);
      text-align: center;
      position: relative;
    }
    body[data-theme="day"] .formula-box { background: rgba(15, 23, 42, 0.05); }
    .formula-label {
      position: absolute;
      top: -10px;
      left: 20px;
      background: var(--panel);
      padding: 0 10px;
      font-size: 12px;
      color: var(--muted);
      font-weight: 600;
    }

    /************************************************************
     * STUDENT-FRIENDLY EXPLANATION BOXES (SIMPLIFIED LANGUAGE)
     ************************************************************/
    .explanation-box {
      background: linear-gradient(135deg, rgba(96, 165, 250, 0.05), rgba(167, 139, 250, 0.05));
      border-left: 4px solid var(--primary);
      padding: 20px;
      margin: 20px 0;
      border-radius: 0 var(--radius) var(--radius) 0;
      position: relative;
    }
    .explanation-box::before {
      content: 'üìö In Simple Terms';
      position: absolute;
      top: -10px;
      left: 20px;
      background: var(--panel);
      padding: 0 10px;
      font-size: 12px;
      font-weight: 600;
      color: var(--primary);
      text-transform: uppercase;
      letter-spacing: 0.5px;
    }
    .explanation-box p:last-child { margin-bottom: 0; }

    .pro-tip {
      background: linear-gradient(135deg, rgba(52, 211, 153, 0.05), rgba(16, 185, 129, 0.05));
      border-left: 4px solid var(--success);
      padding: 20px;
      margin: 20px 0;
      border-radius: 0 var(--radius) var(--radius) 0;
      position: relative;
    }
    .pro-tip::before {
      content: 'üí° Study Tip';
      position: absolute;
      top: -10px;
      left: 20px;
      background: var(--panel);
      padding: 0 10px;
      font-size: 12px;
      font-weight: 600;
      color: var(--success);
      text-transform: uppercase;
      letter-spacing: 0.5px;
    }

    .common-pitfall {
      background: linear-gradient(135deg, rgba(251, 113, 133, 0.05), rgba(239, 68, 68, 0.05));
      border-left: 4px solid var(--danger);
      padding: 20px;
      margin: 20px 0;
      border-radius: 0 var(--radius) var(--radius) 0;
      position: relative;
    }
    .common-pitfall::before {
      content: '‚ö†Ô∏è Watch Out!';
      position: absolute;
      top: -10px;
      left: 20px;
      background: var(--panel);
      padding: 0 10px;
      font-size: 12px;
      font-weight: 600;
      color: var(--danger);
      text-transform: uppercase;
      letter-spacing: 0.5px;
    }

    /************************************************************
     * CLASSIFICATION VISUALIZATION
     ************************************************************/
    .classification-container {
      display: grid;
      grid-template-columns: 2fr 1fr;
      gap: 24px;
      margin: 20px 0;
    }
    @media (max-width: 1100px) { .classification-container { grid-template-columns: 1fr; } }
    .classification-chart {
      height: 400px;
      border-radius: var(--radius);
      border: 1px solid var(--border);
      background: var(--panel2);
      position: relative;
      overflow: hidden;
    }
    .classification-controls {
      padding: 20px;
      background: var(--panel2);
      border-radius: var(--radius);
      border: 1px solid var(--border);
    }
    .control-group { margin-bottom: 24px; }
    .control-group h4 {
      color: var(--text);
      margin-bottom: 16px;
      font-size: 15px;
      display: flex;
      align-items: center;
      gap: 10px;
    }
    .slider-container { display: flex; align-items: center; gap: 15px; margin-bottom: 15px; }
    .slider-container label { min-width: 160px; color: var(--muted); font-size: 14px; }
    input[type="range"] { flex: 1; height: 6px; border-radius: 3px; background: var(--border); outline: none; accent-color: var(--primary); }
    .slider-value { min-width: 50px; text-align: right; font-family: var(--mono); font-weight: 600; color: var(--text); }

    /************************************************************
     * DECISION BOUNDARY PLOT
     ************************************************************/
    .boundary-container {
      width: 100%;
      height: 400px;
      position: relative;
      background: var(--panel2);
      border-radius: var(--radius);
      border: 1px solid var(--border);
      overflow: hidden;
      margin: 20px 0;
    }
    .boundary-canvas {
      width: 100%;
      height: 100%;
      display: block;
    }

    /************************************************************
     * CONFUSION MATRIX
     ************************************************************/
    .confusion-matrix {
      display: grid;
      grid-template-columns: repeat(3, 1fr);
      gap: 2px;
      background: var(--border);
      border: 2px solid var(--border);
      width: fit-content;
      margin: 20px auto;
    }
    .matrix-cell {
      padding: 20px 30px;
      text-align: center;
      background: var(--panel2);
      font-weight: 600;
    }
    .matrix-cell.label { background: var(--panel); color: var(--muted); }
    .matrix-cell.true-positive { background: rgba(52, 211, 153, 0.2); color: var(--success); }
    .matrix-cell.false-negative { background: rgba(251, 113, 133, 0.2); color: var(--danger); }
    .matrix-cell.false-positive { background: rgba(251, 113, 133, 0.2); color: var(--danger); }
    .matrix-cell.true-negative { background: rgba(52, 211, 153, 0.2); color: var(--success); }

    /************************************************************
     * ROC CURVE
     ************************************************************/
    .roc-container {
      height: 300px;
      margin: 20px 0;
    }

    /************************************************************
     * ALGORITHM COMPARISON
     ************************************************************/
    .algorithm-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
      gap: 20px;
      margin: 20px 0;
    }
    .algorithm-card {
      background: var(--panel2);
      border-radius: var(--radius);
      border: 1px solid var(--border);
      overflow: hidden;
      transition: transform 0.2s ease;
    }
    .algorithm-card:hover { transform: translateY(-4px); border-color: var(--primary); }
    .algorithm-header {
      padding: 16px;
      background: linear-gradient(135deg, rgba(96, 165, 250, 0.1), rgba(167, 139, 250, 0.1));
      border-bottom: 1px solid var(--border);
      display: flex;
      align-items: center;
      gap: 12px;
    }
    .algorithm-icon {
      width: 40px;
      height: 40px;
      border-radius: 10px;
      background: linear-gradient(135deg, var(--primary), var(--secondary));
      display: flex;
      align-items: center;
      justify-content: center;
      color: white;
    }
    .algorithm-title { flex: 1; }
    .algorithm-title h4 { margin: 0; font-size: 16px; color: var(--text); }
    .algorithm-title p { margin: 2px 0 0; font-size: 12px; color: var(--muted); }
    .algorithm-body { padding: 16px; }
    .algorithm-metric {
      display: flex;
      justify-content: space-between;
      margin-bottom: 8px;
      font-size: 13px;
      color: var(--muted);
    }
    .algorithm-metric span:last-child { font-weight: 600; color: var(--text); }

    /************************************************************
     * CODE EDITOR STYLES
     ************************************************************/
    .code-editor-container {
      margin: 20px 0;
      border-radius: var(--radius);
      overflow: hidden;
      border: 1px solid var(--border);
    }
    .code-header {
      background: var(--panel);
      padding: 12px 20px;
      border-bottom: 1px solid var(--border);
      display: flex;
      justify-content: space-between;
      align-items: center;
    }
    .code-title { font-weight: 600; color: var(--text); display: flex; align-items: center; gap: 8px; }
    .code-actions { display: flex; gap: 8px; }
    .code-body { background: var(--panel2); padding: 0; overflow: auto; }
    .code-block { font-family: var(--mono); font-size: 14px; line-height: 1.6; padding: 20px; margin: 0; tab-size: 2; white-space: pre-wrap; min-height: 200px; }
    .code-block code { display: block; color: var(--text); }
    .keyword { color: var(--danger); }
    .function { color: var(--primary); }
    .string { color: var(--success); }
    .comment { color: var(--muted2); font-style: italic; }
    .number { color: var(--warning); }
    .operator { color: var(--cyan); }

    /************************************************************
     * METRICS DISPLAY
     ************************************************************/
    .metrics-container {
      display: grid;
      grid-template-columns: repeat(4, 1fr);
      gap: 15px;
      margin: 20px 0;
    }
    @media (max-width: 1100px) { .metrics-container { grid-template-columns: repeat(2, 1fr); } }
    @media (max-width: 600px) { .metrics-container { grid-template-columns: 1fr; } }
    .metric-card { padding: 20px; border-radius: var(--radius); border: 1px solid var(--border); background: var(--panel2); text-align: center; }
    .metric-value { font-size: 28px; font-weight: 700; margin: 10px 0; }
    .metric-label { font-size: 13px; color: var(--muted); text-transform: uppercase; letter-spacing: 0.5px; }
    .metric-good { color: var(--success); }
    .metric-warning { color: var(--warning); }
    .metric-danger { color: var(--danger); }

    /************************************************************
     * DETAILED TABLES
     ************************************************************/
    .detailed-table {
      width: 100%;
      border-collapse: separate;
      border-spacing: 0;
      overflow: hidden;
      border-radius: 12px;
      border: 1px solid var(--border);
      background: var(--panel2);
      margin: 16px 0;
      font-size: 13px;
    }
    .detailed-table th, .detailed-table td {
      padding: 12px 16px;
      border-bottom: 1px solid var(--border);
      text-align: left;
    }
    .detailed-table th { background: var(--panel); color: var(--muted); font-weight: 600; }
    .detailed-table td { color: var(--text); font-family: var(--mono); }
    .detailed-table tr:last-child td { border-bottom: none; }
    .highlight-cell { background: rgba(96, 165, 250, 0.1); color: var(--primary); font-weight: 600; }

    /************************************************************
     * BUTTONS
     ************************************************************/
    .btn {
      display: inline-flex;
      align-items: center;
      gap: 10px;
      padding: 12px 20px;
      border-radius: 12px;
      font-weight: 600;
      border: none;
      cursor: pointer;
      transition: all 0.2s ease;
      font-family: inherit;
      font-size: 14px;
    }
    .btn-primary { background: linear-gradient(135deg, var(--primary), var(--secondary)); color: white; }
    .btn-secondary { background: var(--panel); color: var(--text); border: 1px solid var(--border); }
    .btn:hover:not(:disabled) { transform: translateY(-2px); box-shadow: var(--ring); }
    .btn:disabled { opacity: 0.5; cursor: not-allowed; transform: none !important; }
    .button-group { display: flex; gap: 10px; margin: 20px 0; flex-wrap: wrap; }

    /************************************************************
     * INFO BOXES
     ************************************************************/
    .info-box { padding: 20px; border-radius: 12px; border: 1px solid var(--border); margin: 20px 0; }
    .info-box.warning { border-left: 4px solid var(--warning); background: rgba(251, 191, 36, 0.08); }
    .info-box.success { border-left: 4px solid var(--success); background: rgba(52, 211, 153, 0.08); }
    .info-box.danger { border-left: 4px solid var(--danger); background: rgba(251, 113, 133, 0.08); }
    .info-box.info { border-left: 4px solid var(--primary); background: rgba(96, 165, 250, 0.08); }
    .info-box h4 { color: var(--text); margin-bottom: 10px; display: flex; align-items: center; gap: 10px; }

    /************************************************************
     * CONCEPT CARDS
     ************************************************************/
    .concept-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin: 20px 0; }
    .concept-card {
      padding: 20px;
      border-radius: var(--radius);
      border: 1px solid var(--border);
      background: var(--panel2);
      transition: transform 0.2s ease;
    }
    .concept-card:hover { transform: translateY(-4px); border-color: var(--primary); }
    .concept-card h5 { color: var(--text); margin-bottom: 12px; font-size: 16px; display: flex; align-items: center; gap: 10px; }
    .concept-card p { color: var(--muted); font-size: 14px; line-height: 1.6; }

    footer {
      text-align: center;
      margin-top: 32px;
      padding-top: 24px;
      border-top: 1px solid var(--border);
      color: var(--muted2);
      font-size: 14px;
      line-height: 1.6;
    }

    .highlight { background: rgba(96, 165, 250, 0.1); padding: 2px 6px; border-radius: 4px; color: var(--primary); font-weight: 600; }
    .mono { font-family: var(--mono); background: rgba(15, 23, 42, 0.1); padding: 2px 6px; border-radius: 4px; color: var(--text); }
    .math-equation { font-family: "Times New Roman", serif; font-style: italic; font-size: 1.1em; }

    .library-badge {
      display: inline-flex;
      align-items: center;
      gap: 6px;
      padding: 6px 12px;
      border-radius: 20px;
      font-size: 12px;
      font-weight: 600;
      margin: 2px;
    }
    .sklearn-badge { background: rgba(231, 76, 60, 0.1); color: #E74C3C; border: 1px solid rgba(231, 76, 60, 0.3); }
  </style>
</head>

<body data-theme="night">
  <!-- Theme Toggle -->
  <div class="theme-toggle-container">
    <button class="theme-toggle" id="themeToggle" type="button" aria-label="Theme: Night Mode (click to toggle)">
      <span class="label" id="themeLabel">Night Mode</span>
    </button>
  </div>

  <div class="container">
    <header>
      <div class="course-info">
        <h1>üè∑Ô∏è Module 6: Supervised Machine Learning - Classification</h1>
        <h2>Supervised Machine Learning</h2>
        <div class="badge"><i class="fas fa-tags"></i> IAF 604: Learn to predict categories - spam, fraud, diseases, and more!</div>
        <p style="margin-top: 15px; color: var(--muted);">
          <strong>üéØ Your Learning Goals:</strong> By the end of this module, you'll understand how classification algorithms work,
          how to evaluate them properly (not just accuracy!), and when to use each one.
        </p>
      </div>
    </header>

    <!-- Navigation -->
    <div class="module-nav">
      <button class="nav-btn active" data-module="intro"><i class="fas fa-graduation-cap"></i> 1. What is Classification?</button>
      <button class="nav-btn" data-module="logistic"><i class="fas fa-sigmoid"></i> 2. Logistic Regression</button>
      <button class="nav-btn" data-module="knn"><i class="fas fa-users"></i> 3. K-Nearest Neighbors</button>
      <button class="nav-btn" data-module="naive-bayes"><i class="fas fa-chart-pie"></i> 4. Naive Bayes</button>
      <button class="nav-btn" data-module="svm"><i class="fas fa-vector-square"></i> 5. SVM</button>
      <button class="nav-btn" data-module="tree-classification"><i class="fas fa-tree"></i> 6. Decision Trees</button>
      <button class="nav-btn" data-module="ensemble-class"><i class="fas fa-layer-group"></i> 7. Random Forest & XGBoost</button>
      <button class="nav-btn" data-module="evaluation-class"><i class="fas fa-clipboard-check"></i> 8. Metrics (Beyond Accuracy)</button>
      <button class="nav-btn" data-module="hands-on-class"><i class="fas fa-laptop-code"></i> 9. Try It Yourself!</button>
      <button class="nav-btn" data-module="resources-class"><i class="fas fa-book"></i> 10. Practice Resources</button>
    </div>

    <!-- ================================================
         SECTION 1: INTRODUCTION
    ================================================= -->
    <!-- ================================================
     SECTION 1: INTRODUCTION TO CLASSIFICATION - ENHANCED
================================================= -->
<div id="intro" class="module-content active">

  <div class="panel section">
    <h3><i class="fas fa-graduation-cap"></i> 1.1 What is Classification?</h3>

    <div class="info-box info">
      <h4><i class="fas fa-brain"></i> The Basic Idea: Sorting Things into Categories</h4>
      <p>
        <strong>Classification</strong> is a type of supervised learning where the goal is to predict 
        which <strong>category</strong> or <strong>class</strong> a new observation belongs to, based on 
        labeled training examples.
      </p>
      
      <div style="display: flex; gap: 20px; flex-wrap: wrap; margin-top: 15px; justify-content: center;">
        <span class="badge"><i class="fas fa-check-circle"></i> Spam / Not Spam</span>
        <span class="badge"><i class="fas fa-check-circle"></i> Cat / Dog / Bird</span>
        <span class="badge"><i class="fas fa-check-circle"></i> Fraudulent / Legitimate</span>
        <span class="badge"><i class="fas fa-check-circle"></i> Disease / Healthy</span>
      </div>
      
      <p style="margin-top: 15px;">
        <strong>Key insight:</strong> Classification learns patterns from labeled examples to make predictions 
        on new, unseen data. It's like teaching a child to identify animals by showing them pictures with labels!
      </p>
    </div>

    <div class="explanation-box">
      <p><strong>üìö In Simple Terms:</strong> Imagine you're teaching a friend to identify spam emails. You'd show them examples: "This email says 'YOU WON $1,000,000' - it's spam" and "This email from your professor - not spam". The computer learns from these labeled examples to put new items into categories.</p>
      <p><strong>Real-Life Examples:</strong></p>
      <ul style="padding-left:20px;">
        <li>üìß Email providers: Spam or Not Spam</li>
        <li>üè• Medical diagnosis: Disease or Healthy</li>
        <li>üí≥ Banks: Fraudulent or Legitimate transaction</li>
        <li>üì∏ Social media: Cat, Dog, or Bird in photos</li>
        <li>‚≠ê Reviews: Positive or Negative sentiment</li>
        <li>üéµ Spotify: What genre does this song belong to?</li>
      </ul>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-sitemap"></i> 1.2 Types of Classification Problems</h3>

    <div class="concept-grid">
      <!-- Binary Classification -->
      <div class="concept-card">
        <h5><i class="fas fa-divide"></i> Binary Classification</h5>
        <p><strong>Two possible outcomes</strong> - the simplest and most common form</p>
        
        <div style="background: var(--panel); padding: 15px; border-radius: 8px; margin: 15px 0;">
          <div style="display: flex; justify-content: space-around;">
            <span class="badge" style="background: var(--primary);">Class 0</span>
            <span class="badge" style="background: var(--danger);">Class 1</span>
          </div>
        </div>
        
        <p><strong>Examples:</strong></p>
        <ul style="padding-left:20px;">
          <li>‚úÖ Spam (1) / Not Spam (0)</li>
          <li>‚úÖ Fraud (1) / Not Fraud (0)</li>
          <li>‚úÖ Sick (1) / Healthy (0)</li>
          <li>‚úÖ Churn (1) / Stay (0)</li>
          <li>‚úÖ Click (1) / No Click (0)</li>
        </ul>
        <p><em>Most algorithms start with binary - it's easier to understand and evaluate.</em></p>
      </div>

      <!-- Multi-class Classification -->
      <div class="concept-card">
        <h5><i class="fas fa-cubes"></i> Multi-class Classification</h5>
        <p><strong>Three or more possible outcomes</strong> - extension of binary</p>
        
        <div style="background: var(--panel); padding: 15px; border-radius: 8px; margin: 15px 0;">
          <div style="display: flex; justify-content: space-around; flex-wrap: wrap; gap: 5px;">
            <span class="badge" style="background: var(--primary);">Cat</span>
            <span class="badge" style="background: var(--success);">Dog</span>
            <span class="badge" style="background: var(--warning);">Bird</span>
            <span class="badge" style="background: var(--danger);">Fish</span>
          </div>
        </div>
        
        <p><strong>Examples:</strong></p>
        <ul style="padding-left:20px;">
          <li>üì∏ Image: Cat vs Dog vs Bird vs Fish</li>
          <li>‚≠ê Ratings: 1, 2, 3, 4, or 5 stars</li>
          <li>üî§ Handwriting: Digits 0-9 or letters A-Z</li>
          <li>üß¨ Cancer type: 10 different subtypes</li>
          <li>üì∞ News: Sports, Politics, Tech, Entertainment</li>
        </ul>
      </div>

      <!-- Multi-label Classification -->
      <div class="concept-card">
        <h5><i class="fas fa-tags"></i> Multi-label Classification</h5>
        <p><strong>Multiple categories can be true at once</strong> - more complex</p>
        
        <div style="background: var(--panel); padding: 15px; border-radius: 8px; margin: 15px 0;">
          <div style="display: flex; justify-content: space-around; flex-wrap: wrap; gap: 5px;">
            <span class="badge" style="background: var(--primary);">Car</span>
            <span class="badge" style="background: var(--success);">Tree</span>
            <span class="badge" style="background: var(--warning);">Sky</span>
            <span class="badge" style="background: var(--danger);">Road</span>
          </div>
        </div>
        
        <p><strong>Examples:</strong></p>
        <ul style="padding-left:20px;">
          <li>üñºÔ∏è Image tagging: A photo can have multiple objects</li>
          <li>üéµ Music genre: A song can be both "Rock" and "Electronic"</li>
          <li>üìÑ Document: Can be about multiple topics</li>
          <li>üé¨ Movie: Can be both "Comedy" and "Romance"</li>
        </ul>
        <p><em>Often solved by training multiple binary classifiers.</em></p>
      </div>
    </div>

    <div class="pro-tip">
      <p><strong>üí° Easy Rule:</strong> If your answer is a single category (like "spam" or "not spam") ‚Üí use classification. If it's a number (like price, temperature) ‚Üí that's regression from Module 5. If multiple categories can be true ‚Üí multi-label classification.</p>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-table"></i> 1.3 Anatomy of a Classification Problem</h3>

    <div class="expanded-content">
      <h4>Let's Look at Real Data: Email Classification</h4>
      
      <p><strong>Features (X):</strong> Characteristics we use to make predictions</p>
      <p><strong>Target (y):</strong> What we're trying to predict (the class label)</p>
      
      <table class="detailed-table">
        <thead>
          <tr>
            <th>üìß Contains "win"</th>
            <th>üí∞ Contains "money"</th>
            <th>üîó Has link</th>
            <th>üìè Length</th>
            <th>üè∑Ô∏è Target (Spam?)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Yes (1)</td>
            <td>Yes (1)</td>
            <td>Yes (1)</td>
            <td>50</td>
            <td class="highlight-cell"><span class="badge" style="background: var(--danger);">Spam (1)</span></td>
          </tr>
          <tr>
            <td>No (0)</td>
            <td>No (0)</td>
            <td>No (0)</td>
            <td>1200</td>
            <td class="highlight-cell"><span class="badge" style="background: var(--success);">Not Spam (0)</span></td>
          </tr>
          <tr>
            <td>Yes (1)</td>
            <td>No (0)</td>
            <td>Yes (1)</td>
            <td>80</td>
            <td class="highlight-cell"><span class="badge" style="background: var(--danger);">Spam (1)</span></td>
          </tr>
          <tr>
            <td>No (0)</td>
            <td>No (0)</td>
            <td>Yes (1)</td>
            <td>600</td>
            <td class="highlight-cell"><span class="badge" style="background: var(--success);">Not Spam (0)</span></td>
          </tr>
          <tr>
            <td>Yes (1)</td>
            <td>Yes (1)</td>
            <td>No (0)</td>
            <td>40</td>
            <td class="highlight-cell"><span class="badge" style="background: var(--danger);">Spam (1)</span></td>
          </tr>
          <tr>
            <td>No (0)</td>
            <td>Yes (1)</td>
            <td>No (0)</td>
            <td>900</td>
            <td class="highlight-cell"><span class="badge" style="background: var(--success);">Not Spam (0)</span></td>
          </tr>
        </tbody>
      </table>

      <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-top: 20px;">
        <div style="background: var(--panel2); padding: 15px; border-radius: var(--radius);">
          <h5>üîç Patterns we can see:</h5>
          <ul>
            <li>Short emails with "win" and links ‚Üí likely Spam</li>
            <li>Long emails from known patterns ‚Üí likely Not Spam</li>
            <li>"money" alone isn't decisive (appears in both)</li>
            <li>Length seems inversely related to spam probability</li>
          </ul>
        </div>
        
        <div style="background: var(--panel2); padding: 15px; border-radius: var(--radius);">
          <h5>üìä Data characteristics:</h5>
          <ul>
            <li><strong>Features:</strong> 4 (3 binary, 1 continuous)</li>
            <li><strong>Samples:</strong> 6</li>
            <li><strong>Classes:</strong> 2 (balanced: 3 each)</li>
            <li><strong>Task:</strong> Learn the pattern to classify new emails</li>
          </ul>
        </div>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-question-circle"></i> 1.4 Classification vs Regression</h3>

    <div class="info-box info">
      <h4>The Fundamental Difference: Categories vs Numbers</h4>
      
      <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 30px; margin: 25px 0;">
        <div style="text-align: center;">
          <div style="background: linear-gradient(135deg, rgba(52, 211, 153, 0.2), rgba(16, 185, 129, 0.2)); padding: 20px; border-radius: var(--radius);">
            <div style="font-size: 2rem; margin-bottom: 10px;">üìà</div>
            <h4 style="color: var(--success);">Regression</h4>
            <p><strong>Predicts NUMBERS</strong></p>
            <div style="text-align: left; padding: 15px;">
              ‚Ä¢ House price: <span class="highlight">$425,000</span><br/>
              ‚Ä¢ Temperature: <span class="highlight">72.5¬∞F</span><br/>
              ‚Ä¢ Sales: <span class="highlight">1,250 units</span><br/>
              ‚Ä¢ Stock price: <span class="highlight">$145.30</span><br/>
            </div>
            <div class="math-equation">y = Œ≤‚ÇÄ + Œ≤‚ÇÅx</div>
          </div>
        </div>
        
        <div style="text-align: center;">
          <div style="background: linear-gradient(135deg, rgba(251, 113, 133, 0.2), rgba(239, 68, 68, 0.2)); padding: 20px; border-radius: var(--radius);">
            <div style="font-size: 2rem; margin-bottom: 10px;">üè∑Ô∏è</div>
            <h4 style="color: var(--danger);">Classification</h4>
            <p><strong>Predicts CATEGORIES</strong></p>
            <div style="text-align: left; padding: 15px;">
              ‚Ä¢ Spam? <span class="badge" style="background: var(--danger);">Yes</span> or <span class="badge" style="background: var(--success);">No</span><br/>
              ‚Ä¢ Digit: <span class="badge" style="background: var(--primary);">0-9</span><br/>
              ‚Ä¢ Disease: <span class="badge" style="background: var(--warning);">Type A/B/C</span><br/>
              ‚Ä¢ Sentiment: <span class="badge" style="background: var(--success);">Positive</span> / <span class="badge" style="background: var(--danger);">Negative</span><br/>
            </div>
            <div class="math-equation">P(y=k|X) ‚àà [0,1]</div>
          </div>
        </div>
      </div>

      <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 15px; margin-top: 20px;">
        <div style="background: var(--panel2); padding: 15px; border-radius: var(--radius);">
          <strong>Output type:</strong><br>
          Regression ‚Üí Continuous numbers<br>
          Classification ‚Üí Discrete categories
        </div>
        <div style="background: var(--panel2); padding: 15px; border-radius: var(--radius);">
          <strong>Evaluation:</strong><br>
          Regression ‚Üí MSE, RMSE, R¬≤<br>
          Classification ‚Üí Accuracy, Precision, Recall, F1
        </div>
        <div style="background: var(--panel2); padding: 15px; border-radius: var(--radius);">
          <strong>Example algorithms:</strong><br>
          Regression ‚Üí Linear, Ridge, Lasso<br>
          Classification ‚Üí Logistic, SVM, Trees
        </div>
      </div>
    </div>

    <div class="common-pitfall">
      <p><strong>‚ö†Ô∏è Don't use regression for classification!</strong> If you code "spam" as 1 and "not spam" as 0 and use linear regression, you might get predictions like 0.7 (okay) but also -0.3 or 1.4 (nonsense!). Classification algorithms are designed to output valid probabilities between 0 and 1.</p>
    </div>

    <div class="pro-tip">
      <p><strong>üí° Sometimes they overlap:</strong> Some algorithms can do both! Decision trees can predict numbers (regression) or categories (classification). The difference is what's in the leaves: averages vs majority votes.</p>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-chart-line"></i> 1.5 The Classification Workflow</h3>

    <div style="display: flex; justify-content: center; margin: 30px 0;">
      <svg width="800" height="150" viewBox="0 0 800 150">
        <!-- Step 1: Data -->
        <rect x="50" y="50" width="120" height="60" rx="10" fill="var(--primary)" opacity="0.8" />
        <text x="110" y="85" text-anchor="middle" fill="white" font-size="12">1. Collect Data</text>
        
        <!-- Arrow -->
        <line x1="170" y1="80" x2="230" y2="80" stroke="var(--text)" stroke-width="2"/>
        <polygon points="230,75 240,80 230,85" fill="var(--text)"/>
        
        <!-- Step 2: Preprocess -->
        <rect x="240" y="50" width="120" height="60" rx="10" fill="var(--warning)" opacity="0.8" />
        <text x="300" y="85" text-anchor="middle" fill="white" font-size="12">2. Preprocess</text>
        
        <!-- Arrow -->
        <line x1="360" y1="80" x2="420" y2="80" stroke="var(--text)" stroke-width="2"/>
        <polygon points="420,75 430,80 420,85" fill="var(--text)"/>
        
        <!-- Step 3: Split -->
        <rect x="430" y="50" width="120" height="60" rx="10" fill="var(--success)" opacity="0.8" />
        <text x="490" y="85" text-anchor="middle" fill="white" font-size="12">3. Train/Test Split</text>
        
        <!-- Arrow -->
        <line x1="550" y1="80" x2="610" y2="80" stroke="var(--text)" stroke-width="2"/>
        <polygon points="610,75 620,80 610,85" fill="var(--text)"/>
        
        <!-- Step 4: Train -->
        <rect x="620" y="20" width="120" height="60" rx="10" fill="var(--danger)" opacity="0.8" />
        <text x="680" y="55" text-anchor="middle" fill="white" font-size="12">4. Train Model</text>
        
        <!-- Step 5: Evaluate -->
        <rect x="620" y="90" width="120" height="60" rx="10" fill="var(--accent)" opacity="0.8" />
        <text x="680" y="125" text-anchor="middle" fill="white" font-size="12">5. Evaluate</text>
        
        <!-- Loop arrow -->
        <line x1="740" y1="90" x2="740" y2="50" stroke="var(--text)" stroke-width="2" stroke-dasharray="5,5"/>
        <polygon points="740,50 735,55 745,55" fill="var(--text)"/>
      </svg>
    </div>

    <div style="display: grid; grid-template-columns: repeat(5, 1fr); gap: 15px; margin-top: 20px;">
      <div style="background: var(--panel2); padding: 15px; border-radius: var(--radius);">
        <h5 style="color: var(--primary);">1. Collect Data</h5>
        <p>Gather labeled examples. More data usually helps, but quality matters more than quantity.</p>
      </div>
      <div style="background: var(--panel2); padding: 15px; border-radius: var(--radius);">
        <h5 style="color: var(--warning);">2. Preprocess</h5>
        <p>Clean data, handle missing values, encode categories, scale features.</p>
      </div>
      <div style="background: var(--panel2); padding: 15px; border-radius: var(--radius);">
        <h5 style="color: var(--success);">3. Split</h5>
        <p>Divide into training (to learn) and testing (to evaluate). Usually 70-80% train, 20-30% test.</p>
      </div>
      <div style="background: var(--panel2); padding: 15px; border-radius: var(--radius);">
        <h5 style="color: var(--danger);">4. Train</h5>
        <p>Feed training data to algorithm. It learns patterns relating features to labels.</p>
      </div>
      <div style="background: var(--panel2); padding: 15px; border-radius: var(--radius);">
        <h5 style="color: var(--accent);">5. Evaluate</h5>
        <p>Test on unseen data. If performance is poor, tune hyperparameters and repeat.</p>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-check-double"></i> 1.6 When to Use Classification</h3>

    <div class="concept-grid">
      <div class="concept-card">
        <h5 style="color: var(--success);">‚úÖ Perfect for:</h5>
        <ul style="padding-left:20px;">
          <li><strong>Yes/No decisions:</strong> Approve loan? Spam? Fraud?</li>
          <li><strong>Recognition tasks:</strong> What digit is this? What animal?</li>
          <li><strong>Diagnosis:</strong> Does this patient have disease X?</li>
          <li><strong>Categorization:</strong> What genre is this movie?</li>
          <li><strong>Risk assessment:</strong> High/Medium/Low risk</li>
          <li><strong>Sentiment analysis:</strong> Positive/Negative review</li>
        </ul>
      </div>
      
      <div class="concept-card">
        <h5 style="color: var(--warning);">ü§î Consider alternatives when:</h5>
        <ul style="padding-left:20px;">
          <li><strong>Predicting quantities:</strong> Use regression instead</li>
          <li><strong>Grouping without labels:</strong> Use clustering (unsupervised)</li>
          <li><strong>Detecting anomalies:</strong> Specialized anomaly detection</li>
          <li><strong>Ordered categories:</strong> Consider ordinal regression</li>
          <li><strong>Ranking problems:</strong> Use learning to rank</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-chart-pie"></i> 1.7 Visual Intuition: Decision Boundaries</h3>

    <div class="info-box info">
      <h4>What Does a Classifier Actually Learn?</h4>
      <p>
        A classifier learns to draw boundaries between classes in feature space. Points on one side get one label,
        points on the other side get another label.
      </p>
    </div>

    <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 20px; margin: 25px 0;">
      <!-- Linear Boundary -->
      <div style="background: var(--panel2); padding: 20px; border-radius: var(--radius); text-align: center;">
        <svg width="150" height="150" viewBox="0 0 150 150">
          <line x1="30" y1="120" x2="120" y2="30" stroke="var(--success)" stroke-width="2"/>
          <circle cx="40" cy="110" r="8" fill="var(--primary)"/>
          <circle cx="60" cy="90" r="8" fill="var(--primary)"/>
          <circle cx="80" cy="70" r="8" fill="var(--primary)"/>
          <circle cx="100" cy="50" r="8" fill="var(--primary)"/>
          <circle cx="110" cy="40" r="8" fill="var(--danger)"/>
          <circle cx="90" cy="60" r="8" fill="var(--danger)"/>
          <circle cx="70" cy="80" r="8" fill="var(--danger)"/>
          <circle cx="50" cy="100" r="8" fill="var(--danger)"/>
        </svg>
        <p><strong>Linear Boundary</strong><br>Straight line separation</p>
      </div>

      <!-- Non-linear Boundary -->
      <div style="background: var(--panel2); padding: 20px; border-radius: var(--radius); text-align: center;">
        <svg width="150" height="150" viewBox="0 0 150 150">
          <path d="M 40 110 Q 75 60, 110 40" stroke="var(--success)" stroke-width="2" fill="none"/>
          <circle cx="30" cy="120" r="8" fill="var(--primary)"/>
          <circle cx="50" cy="100" r="8" fill="var(--primary)"/>
          <circle cx="70" cy="80" r="8" fill="var(--primary)"/>
          <circle cx="120" cy="30" r="8" fill="var(--danger)"/>
          <circle cx="100" cy="50" r="8" fill="var(--danger)"/>
          <circle cx="80" cy="70" r="8" fill="var(--danger)"/>
        </svg>
        <p><strong>Non-linear Boundary</strong><br>Curved separation</p>
      </div>

      <!-- Complex Boundary -->
      <div style="background: var(--panel2); padding: 20px; border-radius: var(--radius); text-align: center;">
        <svg width="150" height="150" viewBox="0 0 150 150">
          <path d="M 30 120 Q 50 60, 75 40 Q 100 20, 120 40" stroke="var(--success)" stroke-width="2" fill="none"/>
          <circle cx="20" cy="130" r="8" fill="var(--primary)"/>
          <circle cx="40" cy="110" r="8" fill="var(--primary)"/>
          <circle cx="60" cy="70" r="8" fill="var(--primary)"/>
          <circle cx="130" cy="30" r="8" fill="var(--danger)"/>
          <circle cx="110" cy="50" r="8" fill="var(--danger)"/>
          <circle cx="80" cy="80" r="8" fill="var(--danger)"/>
          <circle cx="70" cy="110" r="8" fill="var(--danger)"/>
        </svg>
        <p><strong>Complex Boundary</strong><br>Multiple curves</p>
      </div>
    </div>

    <p>
      Different algorithms are better at finding different types of boundaries:
    </p>
    <ul>
      <li><strong>Logistic Regression:</strong> Linear boundaries only</li>
      <li><strong>Decision Trees:</strong> Axis-aligned rectangular boundaries</li>
      <li><strong>SVM with RBF:</strong> Can learn very complex non-linear boundaries</li>
      <li><strong>KNN:</strong> Highly flexible, can approximate any boundary</li>
    </ul>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-graduation-cap"></i> 1.8 Quick Teaching Summary</h3>

    <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 20px;">
      <div style="background: linear-gradient(135deg, rgba(96, 165, 250, 0.1), rgba(167, 139, 250, 0.1)); padding: 20px; border-radius: var(--radius);">
        <h4 style="margin-top: 0;">What is it?</h4>
        <p>Predicting categories from labeled examples</p>
        <p><em>Spam? Cat or Dog? Fraud?</em></p>
      </div>
      
      <div style="background: linear-gradient(135deg, rgba(96, 165, 250, 0.1), rgba(167, 139, 250, 0.1)); padding: 20px; border-radius: var(--radius);">
        <h4 style="margin-top: 0;">Key Components</h4>
        <ul>
          <li>Features (X) - what we know</li>
          <li>Labels (y) - what we want</li>
          <li>Model - learns the mapping</li>
          <li>Decision boundary - separates classes</li>
        </ul>
      </div>
      
      <div style="background: linear-gradient(135deg, rgba(96, 165, 250, 0.1), rgba(167, 139, 250, 0.1)); padding: 20px; border-radius: var(--radius);">
        <h4 style="margin-top: 0;">Types</h4>
        <ul>
          <li>Binary (2 classes)</li>
          <li>Multi-class (3+ classes)</li>
          <li>Multi-label (multiple true)</li>
        </ul>
      </div>
    </div>

    <div style="margin-top: 30px; text-align: center; padding: 20px; background: var(--panel2); border-radius: var(--radius);">
      <p style="font-size: 1.2rem;">
        <i class="fas fa-lightbulb" style="color: var(--warning);"></i> 
        <strong>Remember:</strong> Classification is about <span class="highlight">sorting things into buckets</span>.
        The algorithm learns the rules from labeled examples, then applies them to new data!
      </p>
    </div>
  </div>

  <!-- Preview of what's coming -->
  <div class="panel section">
    <h3><i class="fas fa-arrow-right"></i> 1.9 What's Next?</h3>
    
    <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 20px;">
      <div style="text-align: center; padding: 15px;">
        <div style="font-size: 2rem;">üìä</div>
        <h4>Logistic Regression</h4>
        <p>The S-curve for probabilities</p>
      </div>
      
      <div style="text-align: center; padding: 15px;">
        <div style="font-size: 2rem;">üë•</div>
        <h4>K-Nearest Neighbors</h4>
        <p>Birds of a feather flock together</p>
      </div>
      
      <div style="text-align: center; padding: 15px;">
        <div style="font-size: 2rem;">üßÆ</div>
        <h4>Naive Bayes</h4>
        <p>The probability multiplier</p>
      </div>
      
      <div style="text-align: center; padding: 15px;">
        <div style="font-size: 2rem;">üìê</div>
        <h4>Support Vector Machines</h4>
        <p>Finding the widest street</p>
      </div>
      
      <div style="text-align: center; padding: 15px;">
        <div style="font-size: 2rem;">üå≥</div>
        <h4>Decision Trees</h4>
        <p>If-this-then-that rules</p>
      </div>
      
      <div style="text-align: center; padding: 15px;">
        <div style="font-size: 2rem;">ü§ù</div>
        <h4>Ensemble Methods</h4>
        <p>Wisdom of the crowd</p>
      </div>
    </div>
    
    <div style="text-align: center; margin-top: 20px;">
      <button class="btn btn-primary" onclick="switchModule('logistic')">
        <i class="fas fa-arrow-right"></i> Start with Logistic Regression
      </button>
    </div>
  </div>
</div>

    <!-- ================================================
         SECTION 2: LOGISTIC REGRESSION
    ================================================= -->
   <!-- ================================================
     SECTION 2: LOGISTIC REGRESSION (ENHANCED)
================================================= -->
<div id="logistic" class="module-content">

  <div class="panel section">
    <h3><i class="fas fa-sigmoid"></i> 2.1 What Problem Does Logistic Regression Solve?</h3>

    <div class="info-box info">
      <h4><i class="fas fa-bullseye"></i> The Core Purpose</h4>
      <p>
        <strong>Logistic regression</strong> is used for <strong>classification</strong>, despite its name! 
        It predicts the <strong>probability</strong> that an observation belongs to a particular class.
      </p>
      <p>
        Most commonly used for <strong>binary classification</strong>:
      </p>
      <div style="display: flex; gap: 20px; flex-wrap: wrap; margin-top: 15px;">
        <span class="badge"><i class="fas fa-check-circle"></i> 0 / 1</span>
        <span class="badge"><i class="fas fa-check-circle"></i> Yes / No</span>
        <span class="badge"><i class="fas fa-check-circle"></i> Default / No Default</span>
        <span class="badge"><i class="fas fa-check-circle"></i> Churn / No Churn</span>
        <span class="badge"><i class="fas fa-check-circle"></i> Spam / Not Spam</span>
      </div>
      <p style="margin-top: 15px;">
        <strong>Key idea:</strong> Instead of predicting the class directly, it predicts the <span class="highlight">probability</span> 
        that an observation belongs to class 1: \( P(Y=1|X) \)
      </p>
    </div>

    <div class="explanation-box">
      <p><strong>üìö In Simple Terms:</strong> Logistic regression answers the question: "Given these features, what's the probability this belongs to Category A vs Category B?" It's like a weather forecast that says "70% chance of rain" instead of just "rain" or "no rain".</p>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-exclamation-triangle"></i> 2.2 Why Not Just Use Linear Regression?</h3>

    <div class="formula-with-calc">
      <div class="formula-box">
        <div class="formula-label">Linear Regression (Wrong!)</div>
        <div style="text-align:center; padding: 15px;">
          <span class="math-equation">\(\hat{y} = \beta_0 + \beta_1 x\)</span>
        </div>
        <div style="text-align:left; padding: 0 15px 15px;">
          <p style="color: var(--danger);"><i class="fas fa-times"></i> Problems:</p>
          <ul style="padding-left:20px;">
            <li>Predictions can be &lt; 0 or &gt; 1 (nonsense for probabilities!)</li>
            <li>Errors are not normally distributed</li>
            <li>Violates key linear regression assumptions</li>
            <li>Example: Predicting 1.4 or -0.3 for spam probability</li>
          </ul>
        </div>
      </div>

      <div class="formula-box">
        <div class="formula-label">Logistic Regression (Correct!)</div>
        <div style="text-align:center; padding: 15px;">
          <span class="math-equation">\(P(Y=1) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x)}}\)</span>
        </div>
        <div style="text-align:left; padding: 0 15px 15px;">
          <p style="color: var(--success);"><i class="fas fa-check"></i> Benefits:</p>
          <ul style="padding-left:20px;">
            <li>Outputs constrained to <strong>[0,1]</strong></li>
            <li>Suitable for <strong>Bernoulli-distributed outcomes</strong></li>
            <li>Interpretable as probabilities</li>
            <li>Example: Spam probability = 0.73 (73% chance)</li>
          </ul>
        </div>
      </div>
    </div>

    <div class="common-pitfall">
      <p><strong>‚ö†Ô∏è Don't use linear regression for classification!</strong> It might give you predictions like -0.3 or 1.4, which don't make sense as probabilities. Logistic regression is specifically designed to keep outputs between 0 and 1.</p>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-wave-square"></i> 2.3 The Logistic (Sigmoid) Function</h3>

    <div class="info-box info">
      <h4>The Magic S-Curve</h4>
      <p>
        Logistic regression uses the <strong>sigmoid function</strong> to transform any real number into a probability between 0 and 1:
      </p>
      
      <div style="text-align: center; margin: 25px 0;">
        <span class="math-equation" style="font-size: 1.8rem;">\(\sigma(z) = \frac{1}{1 + e^{-z}}\)</span>
      </div>
      
      <p>Where:</p>
      <div style="background: var(--panel2); padding: 15px; border-radius: 8px; margin: 15px 0;">
        <span class="math-equation">\(z = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p\)</span>
      </div>
      
      <p><strong>Key properties:</strong></p>
      <ul>
        <li>As \(z \to \infty\), probability ‚Üí 1</li>
        <li>As \(z \to -\infty\), probability ‚Üí 0</li>
        <li>When \(z = 0\), probability = 0.5 (the decision boundary)</li>
      </ul>
    </div>

    <!-- Interactive Logistic Chart (already in code) -->
    <div class="regression-container">
      <div class="classification-chart"><canvas id="logisticChart"></canvas></div>

      <div class="classification-controls">
        <div class="control-group">
          <h4><i class="fas fa-sliders-h"></i> Play with the S-Curve</h4>
          <div class="slider-container">
            <label><i class="fas fa-arrow-left"></i> Slope (\(\beta_1\)):</label>
            <input type="range" id="logisticSlope" min="0.1" max="5" step="0.1" value="1">
            <span class="slider-value" id="logisticSlopeValue">1.0</span>
          </div>
          <div class="slider-container">
            <label><i class="fas fa-arrows-alt-v"></i> Intercept (\(\beta_0\)):</label>
            <input type="range" id="logisticIntercept" min="-5" max="5" step="0.5" value="0">
            <span class="slider-value" id="logisticInterceptValue">0.0</span>
          </div>
          <div class="slider-container">
            <label><i class="fas fa-bullseye"></i> Decision Threshold:</label>
            <input type="range" id="logisticThreshold" min="0.1" max="0.9" step="0.05" value="0.5">
            <span class="slider-value" id="logisticThresholdValue">0.5</span>
          </div>
        </div>

        <div class="button-group">
          <button class="btn btn-primary" onclick="updateLogistic()">
            <i class="fas fa-play"></i> Update
          </button>
          <button class="btn btn-secondary" onclick="generateLogisticData()">
            <i class="fas fa-sync"></i> New Data
          </button>
        </div>

        <div class="explanation-box" style="margin-top: 20px;">
          <p><strong>üëÜ Experiment with the sliders:</strong></p>
          <ul style="padding-left:20px;">
            <li><strong>Increase slope:</strong> Steeper S-curve, more confident predictions</li>
            <li><strong>Change intercept:</strong> Shifts curve left/right, changes the balance</li>
            <li><strong>Adjust threshold:</strong> Lower threshold = more "positive" predictions (higher recall, lower precision)</li>
          </ul>
        </div>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-chart-line"></i> 2.4 The Log-Odds (Logit) Interpretation</h3>

    <div class="info-box info">
      <p>
        Instead of modeling probability directly, logistic regression models the <strong>log-odds</strong>:
      </p>
      
      <div style="text-align: center; margin: 25px 0;">
        <span class="math-equation" style="font-size: 1.5rem;">\(\log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 x_1 + \dots + \beta_p x_p\)</span>
      </div>
      
      <p>Where:</p>
      <ul>
        <li>\(p = P(Y=1|X)\) = probability of class 1</li>
        <li>\(\frac{p}{1-p}\) = <strong>odds</strong> (e.g., odds of 2 mean "twice as likely to happen as not")</li>
        <li>\(\log\left(\frac{p}{1-p}\right)\) = <strong>log-odds</strong> or <strong>logit</strong></li>
      </ul>
    </div>

    <div class="expanded-content">
      <h4><i class="fas fa-key"></i> The Key Interpretation: Odds Ratios</h4>
      
      <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
        <div>
          <p><span class="math-equation">\(e^{\beta_j}\)</span> = multiplicative change in the <strong>odds</strong> for a one-unit increase in \(x_j\)</p>
          <p><strong>Example:</strong> If \(\beta_1 = 0.7\) for "contains_win", then:</p>
          <p>\(e^{0.7} \approx 2.01\) ‚Üí odds of spam <strong>double</strong> when an email contains "win"</p>
        </div>
        <div style="background: var(--panel); padding: 15px; border-radius: 8px;">
          <p><strong>üìä Interpretability in practice:</strong></p>
          <ul>
            <li>Healthcare: "Patients with this symptom have 3√ó higher odds of disease"</li>
            <li>Marketing: "Adding this feature increases odds of purchase by 50%"</li>
            <li>Risk modeling: "Each year of age increases odds of default by 5%"</li>
          </ul>
        </div>
      </div>
    </div>

    <div class="pro-tip">
      <p><strong>üí° Why this matters:</strong> This interpretability makes logistic regression the go-to choice in medicine, social sciences, and any field where you need to explain <em>why</em> a prediction was made.</p>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-calculator"></i> 2.5 How It's Estimated: Maximum Likelihood</h3>

    <p>Unlike linear regression (which uses Ordinary Least Squares), logistic regression uses:</p>
    
    <div class="info-box info" style="margin: 20px 0;">
      <h4>Maximum Likelihood Estimation (MLE)</h4>
      <p>Find the parameters that make the observed data most probable.</p>
      
      <div style="text-align: center; margin: 20px 0;">
        <span class="math-equation">Likelihood = \(\prod_{i=1}^{n} p_i^{y_i}(1-p_i)^{1-y_i}\)</span>
      </div>
      
      <p>For one observation:</p>
      <div style="background: var(--panel2); padding: 10px; border-radius: 8px;">
        \(P(y_i|x_i) = p_i^{y_i}(1-p_i)^{1-y_i}\)
      </div>
      <p style="margin-top: 10px;">If \(y_i = 1\): contribution = \(p_i\) (want this high)<br>
      If \(y_i = 0\): contribution = \(1-p_i\) (want this high)</p>
    </div>

    <p><strong>There is no closed-form solution</strong> ‚Üí we need iterative optimization:</p>
    
    <div class="algorithm-grid" style="margin-top: 20px;">
      <div class="algorithm-card">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-arrow-down"></i></div>
          <div class="algorithm-title">
            <h4>Gradient Descent</h4>
            <p>Take steps downhill</p>
          </div>
        </div>
        <div class="algorithm-body">
          <p>Most common in ML libraries. Follow the negative gradient to minimize loss.</p>
        </div>
      </div>
      
      <div class="algorithm-card">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-chart-bar"></i></div>
          <div class="algorithm-title">
            <h4>Newton-Raphson</h4>
            <p>Use second derivatives</p>
          </div>
        </div>
        <div class="algorithm-body">
          <p>Faster convergence but more computationally expensive per step.</p>
        </div>
      </div>
      
      <div class="algorithm-card">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-balance-scale"></i></div>
          <div class="algorithm-title">
            <h4>IRLS</h4>
            <p>Iteratively Reweighted Least Squares</p>
          </div>
        </div>
        <div class="algorithm-body">
          <p>Traditional statistical approach, equivalent to Newton-Raphson.</p>
        </div>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-grip-lines"></i> 2.6 Decision Boundary</h3>

    <div class="expanded-content">
      <p>After computing probability \(\hat{p}\), we classify using a <strong>threshold</strong> (commonly 0.5):</p>
      
      <div style="display: flex; justify-content: center; gap: 40px; margin: 25px 0;">
        <div style="text-align: center;">
          <span class="badge" style="background: rgba(52, 211, 153, 0.2);">If \(\hat{p} \geq 0.5\)</span>
          <p style="margin-top: 10px;">‚Üí <strong>Class 1</strong> (Positive)</p>
        </div>
        <div style="text-align: center;">
          <span class="badge" style="background: rgba(251, 113, 133, 0.2);">If \(\hat{p} < 0.5\)</span>
          <p style="margin-top: 10px;">‚Üí <strong>Class 0</strong> (Negative)</p>
        </div>
      </div>
      
      <p>The <strong>decision boundary</strong> is where \(\hat{p} = 0.5\), which happens when:</p>
      
      <div style="text-align: center; margin: 20px 0;">
        <span class="math-equation">\(\beta_0 + \beta_1 x_1 + \dots + \beta_p x_p = 0\)</span>
      </div>
      
      <p><strong>Important:</strong> This decision boundary is <span class="highlight">linear in feature space</span>! That's why logistic regression is called a <strong>linear classifier</strong>.</p>
    </div>

    <div class="common-pitfall">
      <p><strong>‚ö†Ô∏è The threshold can (and should) be tuned!</strong> 0.5 is just the default. In medical diagnosis, you might use 0.2 to catch more diseases (higher recall). In spam filtering, you might use 0.9 to avoid marking important emails as spam (higher precision).</p>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-clipboard-list"></i> 2.7 Assumptions of Logistic Regression</h3>

    <div class="concept-grid">
      <div class="concept-card">
        <h5><i class="fas fa-check-circle" style="color: var(--success);"></i> Required</h5>
        <ul style="padding-left:20px;">
          <li><strong>Binary dependent variable</strong> (or multinomial for extension)</li>
          <li><strong>Independent observations</strong> (no repeated measures without adjustment)</li>
          <li><strong>Linearity in the log-odds</strong> (not in Y!)</li>
          <li><strong>No strong multicollinearity</strong> among predictors</li>
          <li><strong>Large sample size</strong> for MLE stability (rule of thumb: 10-20 events per predictor)</li>
        </ul>
      </div>
      
      <div class="concept-card">
        <h5><i class="fas fa-times-circle" style="color: var(--danger);"></i> NOT Required</h5>
        <ul style="padding-left:20px;">
          <li>Normality of predictors</li>
          <li>Homoscedasticity (constant variance)</li>
          <li>Linear relationship between X and Y</li>
          <li>Normally distributed errors</li>
        </ul>
      </div>
    </div>

    <div class="info-box warning" style="margin-top: 20px;">
      <h4>‚ö†Ô∏è Checking the Linearity Assumption</h4>
      <p>The relationship between continuous predictors and the <strong>log-odds</strong> should be linear. You can check this by:</p>
      <ul>
        <li>Plotting log-odds vs predictor (should look roughly linear)</li>
        <li>Adding polynomial terms and checking if they improve fit significantly</li>
        <li>Using splines or GAMs for flexible fitting</li>
      </ul>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-expand-arrows-alt"></i> 2.8 Extensions of Logistic Regression</h3>

    <div class="algorithm-grid">
      <div class="algorithm-card">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-cubes"></i></div>
          <div class="algorithm-title">
            <h4>Multinomial Logistic Regression</h4>
          </div>
        </div>
        <div class="algorithm-body">
          <p>For problems with <strong>3+ categories</strong> (e.g., predicting species of iris: setosa, versicolor, virginica). Uses softmax function instead of sigmoid.</p>
        </div>
      </div>
      
      <div class="algorithm-card">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-sort-amount-down"></i></div>
          <div class="algorithm-title">
            <h4>Ordinal Logistic Regression</h4>
          </div>
        </div>
        <div class="algorithm-body">
          <p>For <strong>ordered categories</strong> (e.g., movie ratings: 1-5 stars). Respects the ordering while using logistic principles.</p>
        </div>
      </div>
      
      <div class="algorithm-card">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-sliders-h"></i></div>
          <div class="algorithm-title">
            <h4>Regularized Logistic Regression</h4>
          </div>
        </div>
        <div class="algorithm-body">
          <p>Adds penalty terms to prevent overfitting:</p>
          <ul>
            <li><strong>L1 (Lasso):</strong> Feature selection, sparse solutions</li>
            <li><strong>L2 (Ridge):</strong> Shrinks coefficients, handles multicollinearity</li>
            <li><strong>Elastic Net:</strong> Combines L1 and L2</li>
          </ul>
          <p>Essential for high-dimensional data (many features)!</p>
        </div>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-chart-pie"></i> 2.9 Loss Function: Cross-Entropy</h3>

    <div class="info-box info">
      <p>Logistic regression minimizes the <strong>log loss</strong> (also called binary cross-entropy):</p>
      
      <div style="text-align: center; margin: 25px 0;">
        <span class="math-equation" style="font-size: 1.3rem;">\(\text{Log Loss} = -\frac{1}{n}\sum_{i=1}^{n} \left[ y_i \log(p_i) + (1-y_i)\log(1-p_i) \right]\)</span>
      </div>
      
      <p><strong>Intuition:</strong></p>
      <ul>
        <li>If true class = 1: loss = \(-\log(p)\) ‚Üí wants p close to 1</li>
        <li>If true class = 0: loss = \(-\log(1-p)\) ‚Üí wants p close to 0</li>
        <li>Wrong confident predictions are heavily penalized (log loss grows quickly)</li>
      </ul>
    </div>

    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;">
      <div style="background: var(--panel2); padding: 15px; border-radius: 8px;">
        <p><strong>Example:</strong> True class = 1</p>
        <ul>
          <li>If p = 0.99 ‚Üí loss = -log(0.99) ‚âà 0.01 (good!)</li>
          <li>If p = 0.50 ‚Üí loss = -log(0.50) ‚âà 0.69</li>
          <li>If p = 0.01 ‚Üí loss = -log(0.01) ‚âà 4.6 (terrible!)</li>
        </ul>
      </div>
      <div style="background: var(--panel2); padding: 15px; border-radius: 8px;">
        <p><strong>Compare to MSE:</strong></p>
        <ul>
          <li>MSE would penalize 0.99 vs 1.0 only slightly (0.0001)</li>
          <li>Log loss cares more about confident wrong predictions</li>
          <li>This matches the needs of probabilistic classification</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-check-double"></i> 2.10 When to Use (and Avoid) Logistic Regression</h3>

    <div class="concept-grid">
      <div class="concept-card">
        <h5 style="color: var(--success);">‚úÖ USE when:</h5>
        <ul style="padding-left:20px;">
          <li>You need <strong>interpretability</strong> (explain why a prediction was made)</li>
          <li>The relationship is roughly linear in log-odds</li>
          <li>Dataset is small-to-medium sized (n < 100,000)</li>
          <li>You need a <strong>baseline model</strong> to compare against</li>
          <li>Working in medicine, social sciences, or risk modeling</li>
          <li>You need well-calibrated probabilities</li>
        </ul>
      </div>
      
      <div class="concept-card">
        <h5 style="color: var(--danger);">‚ùå AVOID when:</h5>
        <ul style="padding-left:20px;">
          <li>Strong <strong>nonlinear patterns</strong> dominate (consider trees or SVM with kernels)</li>
          <li>Very complex feature interactions exist (unless manually engineered)</li>
          <li>You have massive datasets (neural networks may scale better)</li>
          <li>Features are highly correlated without regularization</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-graduation-cap"></i> Quick Teaching Summary</h3>

    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
      <div style="background: linear-gradient(135deg, rgba(96, 165, 250, 0.1), rgba(167, 139, 250, 0.1)); padding: 20px; border-radius: var(--radius);">
        <h4 style="margin-top: 0;">Linear Regression</h4>
        <p><span class="math-equation">\(E(Y|X) = \beta_0 + \beta_1 X\)</span></p>
        <p>Predicts <strong>values</strong> (continuous)</p>
      </div>
      
      <div style="background: linear-gradient(135deg, rgba(96, 165, 250, 0.1), rgba(167, 139, 250, 0.1)); padding: 20px; border-radius: var(--radius);">
        <h4 style="margin-top: 0;">Logistic Regression</h4>
        <p><span class="math-equation">\(\log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X\)</span></p>
        <p>Predicts <strong>probabilities</strong> [0,1]</p>
      </div>
    </div>

    <div style="margin-top: 30px; text-align: center; padding: 20px; background: var(--panel2); border-radius: var(--radius);">
      <p style="font-size: 1.2rem;">
        <i class="fas fa-lightbulb" style="color: var(--warning);"></i> 
        <strong>Remember:</strong> Despite the name, logistic regression is a 
        <span class="highlight">CLASSIFICATION</span> algorithm!
      </p>
    </div>
  </div>

  <!-- Original code editor and metrics (preserved) -->
  <div class="code-editor-container">
    <div class="code-header">
      <div class="code-title"><i class="fab fa-python"></i> Logistic Regression in Python</div>
    </div>
    <div class="code-body">
      <pre class="code-block"><code><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression
<span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split
<span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler
<span class="keyword">import</span> pandas <span class="keyword">as</span> pd
<span class="keyword">import</span> numpy <span class="keyword">as</span> np

<span class="comment"># Sample email data</span>
data = pd.DataFrame({
    'contains_win': [1, 0, 1, 0, 1, 0, 1, 0],
    'contains_money': [1, 0, 0, 0, 1, 0, 1, 0],
    'has_link': [1, 0, 1, 1, 1, 0, 1, 0],
    'length': [50, 1200, 80, 600, 40, 900, 60, 1500],
    'is_spam': [1, 0, 1, 0, 1, 0, 1, 0]  <span class="comment"># 1 = spam, 0 = not spam</span>
})

X = data[['contains_win', 'contains_money', 'has_link', 'length']]
y = data['is_spam']

<span class="comment"># Split data</span>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

<span class="comment"># Optional: Scale features (helps with convergence)</span>
scaler = StandardScaler()
X_train_scaled = scaler.<span class="function">fit_transform</span>(X_train)
X_test_scaled = scaler.<span class="function">transform</span>(X_test)

<span class="comment"># Create and train model</span>
model = LogisticRegression(
    penalty='l2',           <span class="comment"># Regularization: 'l1', 'l2', 'elasticnet', or None</span>
    C=1.0,                   <span class="comment"># Inverse of regularization strength (smaller = stronger)</span>
    solver='lbfgs',          <span class="comment"># Algorithm for optimization</span>
    max_iter=100,            <span class="comment"># Maximum iterations for convergence</span>
    random_state=42
)
model.<span class="function">fit</span>(X_train_scaled, y_train)

<span class="comment"># Get predictions and probabilities</span>
y_pred = model.<span class="function">predict</span>(X_test_scaled)  <span class="comment"># 0 or 1</span>
y_proba = model.<span class="function">predict_proba</span>(X_test_scaled)  <span class="comment"># Probabilities for both classes</span>

<span class="comment"># Interpret coefficients (odds ratios)</span>
feature_names = X.columns
coefficients = model.coef_[0]
odds_ratios = np.exp(coefficients)

<span class="keyword">print</span>(<span class="string">"Feature coefficients:"</span>)
<span class="keyword">for</span> name, coef, odds <span class="keyword">in</span> zip(feature_names, coefficients, odds_ratios):
    <span class="keyword">print</span>(<span class="string">f"  {name}: coef={coef:.3f}, odds ratio={odds:.3f}"</span>)
<span class="keyword">print</span>(<span class="string">f"Intercept: {model.intercept_[0]:.3f}"</span>)

<span class="comment"># For a new email</span>
new_email = pd.DataFrame([[1, 1, 1, 60]], columns=feature_names)
new_email_scaled = scaler.<span class="function">transform</span>(new_email)
spam_probability = model.<span class="function">predict_proba</span>(new_email_scaled)[0][1]
<span class="keyword">print</span>(<span class="string">f"\nNew email - Spam probability: {spam_probability:.2f}"</span>)

<span class="comment"># If spam_probability > 0.5, classify as spam</span>
prediction = <span class="string">"SPAM"</span> <span class="keyword">if</span> spam_probability > 0.5 <span class="keyword">else</span> <span class="string">"NOT SPAM"</span>
<span class="keyword">print</span>(<span class="string">f"Classification: {prediction}"</span>)</code></pre>
    </div>
  </div>

  <!-- Metrics display (preserved) -->
  <div class="metrics-container">
    <div class="metric-card">
      <div class="metric-label">Accuracy</div>
      <div class="metric-value" id="logisticAcc">0.85</div>
    </div>
    <div class="metric-card">
      <div class="metric-label">Precision</div>
      <div class="metric-value" id="logisticPrec">0.82</div>
    </div>
    <div class="metric-card">
      <div class="metric-label">Recall</div>
      <div class="metric-value" id="logisticRec">0.79</div>
    </div>
    <div class="metric-card">
      <div class="metric-label">F1 Score</div>
      <div class="metric-value" id="logisticF1">0.80</div>
    </div>
  </div>

  <!-- Pro tip (preserved) -->
  <div class="pro-tip">
    <p><strong>üí° Important:</strong> Despite the name, logistic regression is a CLASSIFICATION algorithm, not regression! The name is historical because it's based on linear regression + logistic function.</p>
  </div>
</div>

    <!-- ================================================
         SECTION 3: K-NEAREST NEIGHBORS (KNN)
    ================================================= -->
   <!-- ================================================
     SECTION 3: K-NEAREST NEIGHBORS (KNN) - ENHANCED
================================================= -->
<div id="knn" class="module-content">

  <div class="panel section">
    <h3><i class="fas fa-users"></i> 3.1 What Problem Does KNN Solve?</h3>

    <div class="info-box info">
      <h4><i class="fas fa-bullseye"></i> The Core Idea: "Birds of a Feather Flock Together"</h4>
      <p>
        <strong>K-Nearest Neighbors (KNN)</strong> is one of the simplest classification algorithms. 
        It classifies a new point based on the <strong>majority vote</strong> of its K most similar neighbors.
      </p>
      
      <div style="display: flex; gap: 20px; flex-wrap: wrap; margin-top: 15px; justify-content: center;">
        <span class="badge"><i class="fas fa-check-circle"></i> Find K closest points</span>
        <span class="badge"><i class="fas fa-check-circle"></i> Take a vote</span>
        <span class="badge"><i class="fas fa-check-circle"></i> Assign majority class</span>
      </div>
      
      <p style="margin-top: 15px;">
        <strong>Key idea:</strong> No training step! KNN just <span class="highlight">memorizes all training data</span> 
        and compares new points to old ones at prediction time.
      </p>
    </div>

    <div class="explanation-box">
      <p><strong>üìö In Simple Terms:</strong> KNN is like asking your 5 closest friends for advice and going with the majority opinion. If 4 friends say "watch this movie" and 1 says "skip it", you probably watch it. The "closeness" is measured by how similar your friends' tastes are to yours.</p>
      <p><strong>Real-Life Example:</strong> Recommendation systems use KNN: "Users who liked this movie also liked..." Find the K most similar users and recommend what they liked.</p>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-ruler"></i> 3.2 How Do We Measure "Closeness"? Distance Metrics</h3>

    <div class="info-box info">
      <p>
        To find neighbors, we need to measure <strong>distance</strong> between points. The choice of distance metric 
        dramatically affects results.
      </p>
    </div>

    <div class="algorithm-grid">
      <div class="algorithm-card">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-arrows-alt-h"></i></div>
          <div class="algorithm-title">
            <h4>Euclidean Distance</h4>
            <p>Straight-line distance</p>
          </div>
        </div>
        <div class="algorithm-body">
          <div class="math-equation" style="text-align: center; margin: 10px 0;">
            \(d = \sqrt{\sum_{i=1}^{n}(x_i - y_i)^2}\)
          </div>
          <p><strong>Most common default.</strong> Works well when features are continuous and on similar scales.</p>
          <p><em>Example: Physical distance between houses</em></p>
        </div>
      </div>

      <div class="algorithm-card">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-city"></i></div>
          <div class="algorithm-title">
            <h4>Manhattan Distance</h4>
            <p>City block distance</p>
          </div>
        </div>
        <div class="algorithm-body">
          <div class="math-equation" style="text-align: center; margin: 10px 0;">
            \(d = \sum_{i=1}^{n}|x_i - y_i|\)
          </div>
          <p>Sum of absolute differences. Good for high-dimensional data or grid-like movements.</p>
          <p><em>Example: Walking distance in a city with grid streets</em></p>
        </div>
      </div>

      <div class="algorithm-card">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-vector-square"></i></div>
          <div class="algorithm-title">
            <h4>Minkowski Distance</h4>
            <p>Generalized form</p>
          </div>
        </div>
        <div class="algorithm-body">
          <div class="math-equation" style="text-align: center; margin: 10px 0;">
            \(d = (\sum_{i=1}^{n}|x_i - y_i|^p)^{1/p}\)
          </div>
          <p>p=1 ‚Üí Manhattan, p=2 ‚Üí Euclidean. Other p values are rarely used.</p>
        </div>
      </div>

      <div class="algorithm-card">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-percent"></i></div>
          <div class="algorithm-title">
            <h4>Hamming Distance</h4>
            <p>For categorical data</p>
          </div>
        </div>
        <div class="algorithm-body">
          <div class="math-equation" style="text-align: center; margin: 10px 0;">
            \(d = \text{count of mismatches}\)
          </div>
          <p>Used for binary or categorical features. Count how many features differ.</p>
          <p><em>Example: Comparing DNA sequences or survey responses</em></p>
        </div>
      </div>
    </div>

    <div class="common-pitfall">
      <p><strong>‚ö†Ô∏è Feature scaling is CRITICAL for KNN!</strong> If one feature is in dollars (0-1,000,000) and another is 0-1, the dollar feature dominates distance calculations. Always standardize/normalize features first!</p>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-chart-line"></i> 3.3 Interactive KNN Demo</h3>

    <div class="regression-container">
      <div class="classification-chart"><canvas id="knnChart"></canvas></div>

      <div class="classification-controls">
        <div class="control-group">
          <h4><i class="fas fa-sliders-h"></i> KNN Controls</h4>
          <div class="slider-container">
            <label><i class="fas fa-users"></i> K (Number of Neighbors):</label>
            <input type="range" id="kValue" min="1" max="20" step="1" value="5">
            <span class="slider-value" id="kValueValue">5</span>
          </div>
          <p style="font-size:12px; margin-top: 5px;">
            <span style="color: var(--danger);">K=1: Very flexible, overfits</span> | 
            <span style="color: var(--success);">K=5-10: Balanced</span> | 
            <span style="color: var(--warning);">K=20: Smooth, may underfit</span>
          </p>
          
          <div class="slider-container">
            <label><i class="fas fa-ruler"></i> Distance Metric:</label>
            <select id="distanceMetric" class="btn btn-secondary" style="padding: 8px;">
              <option value="euclidean">Euclidean (straight line)</option>
              <option value="manhattan">Manhattan (city block)</option>
            </select>
          </div>

          <div class="slider-container">
            <label><i class="fas fa-weight-hanging"></i> Weighting:</label>
            <select id="weighting" class="btn btn-secondary" style="padding: 8px;">
              <option value="uniform">Uniform (all neighbors equal)</option>
              <option value="distance">Distance (closer neighbors matter more)</option>
            </select>
          </div>
        </div>

        <div class="button-group">
          <button class="btn btn-primary" onclick="updateKNN()"><i class="fas fa-play"></i> Update</button>
          <button class="btn btn-secondary" onclick="generateKNNData()"><i class="fas fa-sync"></i> New Data</button>
        </div>

        <div class="metrics-container" style="margin-top: 20px;">
          <div class="metric-card">
            <div class="metric-label">Accuracy</div>
            <div class="metric-value" id="knnAcc">0.92</div>
          </div>
          <div class="metric-card">
            <div class="metric-label">Decision Boundary</div>
            <div class="metric-value" id="knnBoundary">Complex</div>
          </div>
          <div class="metric-card">
            <div class="metric-label">Training Time</div>
            <div class="metric-value" id="knnTrain">0 sec</div>
          </div>
          <div class="metric-card">
            <div class="metric-label">Prediction Time</div>
            <div class="metric-value" id="knnPredict">Slow</div>
          </div>
        </div>

        <div class="explanation-box" style="margin-top: 20px;">
          <p><strong>üëÜ Watch the bias-variance tradeoff:</strong></p>
          <ul style="padding-left:20px;">
            <li><strong>K=1:</strong> Decision boundary is super wiggly - <span style="color: var(--danger);">overfitting</span> (low bias, high variance)</li>
            <li><strong>K=5-10:</strong> Balanced, smooth boundary</li>
            <li><strong>K=20:</strong> Too smooth - <span style="color: var(--warning);">underfitting</span> (high bias, low variance)</li>
          </ul>
        </div>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-scale-balanced"></i> 3.4 The Bias-Variance Tradeoff in KNN</h3>

    <div class="concept-grid">
      <div class="concept-card">
        <h5 style="color: var(--danger);">Small K (e.g., K=1)</h5>
        <p><strong>Low bias, high variance</strong></p>
        <ul style="padding-left:20px;">
          <li>Model is very flexible - follows every detail in training data</li>
          <li>Decision boundary is highly irregular</li>
          <li>Very sensitive to noise and outliers</li>
          <li>Training accuracy near 100%, test accuracy lower</li>
        </ul>
        <div style="background: var(--panel); padding: 10px; border-radius: 8px; margin-top: 10px;">
          <p><em>Analogy: Trusting only your closest friend's opinion, even if they have weird taste</em></p>
        </div>
      </div>

      <div class="concept-card">
        <h5 style="color: var(--success);">Optimal K</h5>
        <p><strong>Balanced bias and variance</strong></p>
        <ul style="padding-left:20px;">
          <li>Typically K = ‚àön (rule of thumb)</li>
          <li>Found by cross-validation</li>
          <li>Smooth but captures true patterns</li>
          <li>Generalizes well to new data</li>
        </ul>
        <div style="background: var(--panel); padding: 10px; border-radius: 8px; margin-top: 10px;">
          <p><em>Analogy: Asking 5-10 friends with diverse tastes</em></p>
        </div>
      </div>

      <div class="concept-card">
        <h5 style="color: var(--warning);">Large K</h5>
        <p><strong>High bias, low variance</strong></p>
        <ul style="padding-left:20px;">
          <li>Model is too simple - smooths away important patterns</li>
          <li>Decision boundary is overly smooth</li>
          <li>May miss local structure</li>
          <li>Both training and test accuracy suffer</li>
        </ul>
        <div style="background: var(--panel); padding: 10px; border-radius: 8px; margin-top: 10px;">
          <p><em>Analogy: Asking everyone in town, losing local preferences</em></p>
        </div>
      </div>
    </div>

    <div class="info-box success">
      <h4><i class="fas fa-chart-simple"></i> How to Choose K</h4>
      <ol style="margin-left:20px;">
        <li><strong>Use cross-validation:</strong> Try different K values and pick the one with best validation accuracy</li>
        <li><strong>Rule of thumb:</strong> Start with K = ‚àön (n = number of training samples)</li>
        <li><strong>Odd K helps:</strong> For binary classification, odd K prevents ties</li>
        <li><strong>Consider domain:</strong> Noisy data needs larger K, clean patterns can use smaller K</li>
      </ol>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-weight-hanging"></i> 3.5 Weighted Voting: Not All Neighbors Are Equal</h3>

    <div class="info-box info">
      <p>
        <strong>Basic KNN:</strong> All K neighbors have equal vote (uniform weighting)
      </p>
      <p>
        <strong>Weighted KNN:</strong> Closer neighbors have more influence on the prediction
      </p>
      
      <div style="text-align: center; margin: 25px 0;">
        <span class="math-equation">\(w_i = \frac{1}{d(x, x_i)^2}\)</span> or 
        <span class="math-equation">\(w_i = \frac{1}{d(x, x_i)}\)</span>
      </div>
      
      <p>
        <strong>Why it helps:</strong> A neighbor that's very close should matter more than one that's barely inside the K-radius.
      </p>
    </div>

    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;">
      <div style="background: var(--panel2); padding: 15px; border-radius: 8px;">
        <h5 style="margin-top: 0;">Uniform Weighting</h5>
        <p><strong>Example:</strong> K=5, classes: [A, A, A, B, B]</p>
        <p>Vote: A=3, B=2 ‚Üí Predict A</p>
        <p><em>Simple, works well when neighbors are evenly spaced</em></p>
      </div>
      
      <div style="background: var(--panel2); padding: 15px; border-radius: 8px;">
        <h5 style="margin-top: 0;">Distance Weighting</h5>
        <p><strong>Example:</strong> K=5, distances: [0.1, 0.2, 0.3, 4.0, 4.1]</p>
        <p>Weights (1/d¬≤): [100, 25, 11.1, 0.06, 0.05]</p>
        <p>Even if the 3 close points are class B, they dominate the far class A points</p>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-crown"></i> 3.6 The "Curse of Dimensionality"</h3>

    <div class="common-pitfall">
      <h4><i class="fas fa-exclamation-triangle"></i> What is the Curse of Dimensionality?</h4>
      <p>
        As the number of features (dimensions) increases, all points become far apart from each other.
        In high dimensions, "neighbors" aren't really near anymore!
      </p>
    </div>

    <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 20px; margin: 25px 0;">
      <div style="text-align: center; padding: 20px; background: var(--panel2); border-radius: var(--radius);">
        <div style="font-size: 2rem; margin-bottom: 10px;">üìè</div>
        <h4>1 Dimension</h4>
        <p>Average distance between points ‚âà 0.33 (in unit interval)</p>
      </div>
      
      <div style="text-align: center; padding: 20px; background: var(--panel2); border-radius: var(--radius);">
        <div style="font-size: 2rem; margin-bottom: 10px;">üìê</div>
        <h4>10 Dimensions</h4>
        <p>Average distance ‚âà 1.9</p>
      </div>
      
      <div style="text-align: center; padding: 20px; background: var(--panel2); border-radius: var(--radius);">
        <div style="font-size: 2rem; margin-bottom: 10px;">üßä</div>
        <h4>100 Dimensions</h4>
        <p>Average distance ‚âà 4.1 (close to maximum possible!)</p>
      </div>
    </div>

    <div class="info-box warning">
      <h4><i class="fas fa-lightbulb"></i> Practical Implications</h4>
      <ul>
        <li><strong>With many features, KNN breaks down</strong> - points become equidistant</li>
        <li><strong>You need exponentially more data</strong> to maintain same neighbor density</li>
        <li><strong>Solutions:</strong> Feature selection, dimensionality reduction (PCA), or use other algorithms</li>
        <li><strong>Rule of thumb:</strong> KNN struggles when p > 20 (p = number of features)</li>
      </ul>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-tachometer-alt"></i> 3.7 Computational Complexity: The Lazy Learner</h3>

    <div class="algorithm-grid">
      <div class="algorithm-card">
        <div class="algorithm-header" style="background: linear-gradient(135deg, rgba(52, 211, 153, 0.1), rgba(16, 185, 129, 0.1));">
          <div class="algorithm-icon" style="background: linear-gradient(135deg, var(--success), #10B981);">‚ö°</div>
          <div class="algorithm-title">
            <h4>Training: O(1)</h4>
            <p>Literally nothing!</p>
          </div>
        </div>
        <div class="algorithm-body">
          <p><strong>KNN is "lazy"</strong> - it just stores the data. No actual training happens.</p>
          <p>This makes it incredibly fast to "train", but...</p>
        </div>
      </div>

      <div class="algorithm-card">
        <div class="algorithm-header" style="background: linear-gradient(135deg, rgba(251, 113, 133, 0.1), rgba(239, 68, 68, 0.1));">
          <div class="algorithm-icon" style="background: linear-gradient(135deg, var(--danger), #EF4444);">üê¢</div>
          <div class="algorithm-title">
            <h4>Prediction: O(n √ó p)</h4>
            <p>For each new point</p>
          </div>
        </div>
        <div class="algorithm-body">
          <p><strong>Prediction is expensive!</strong> Must compute distance to ALL training points.</p>
          <p>n = training samples, p = features ‚Üí Slow for large datasets</p>
        </div>
      </div>
    </div>

    <div style="margin-top: 20px;">
      <h4>Optimization Techniques:</h4>
      <ul>
        <li><strong>KD-Trees:</strong> Organize data spatially for faster neighbor search (works well in low dimensions)</li>
        <li><strong>Ball Trees:</strong> Better for higher dimensions</li>
        <li><strong>Approximate Nearest Neighbors:</strong> Trade accuracy for speed (e.g., Annoy, Faiss)</li>
      </ul>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-check-double"></i> 3.8 When to Use (and Avoid) KNN</h3>

    <div class="concept-grid">
      <div class="concept-card">
        <h5 style="color: var(--success);">‚úÖ USE when:</h5>
        <ul style="padding-left:20px;">
          <li>Dataset is <strong>small to medium</strong> (n < 10,000 for real-time predictions)</li>
          <li>Decision boundaries are <strong>irregular/complex</strong> and you want to capture them</li>
          <li>You need a <strong>simple, interpretable baseline</strong> (hard to get simpler!)</li>
          <li>Data is <strong>low-dimensional</strong> (p < 20)</li>
          <li>You have time for predictions but not for training</li>
        </ul>
      </div>
      
      <div class="concept-card">
        <h5 style="color: var(--danger);">‚ùå AVOID when:</h5>
        <ul style="padding-left:20px;">
          <li>Dataset is <strong>large</strong> (millions of samples) - predictions too slow</li>
          <li>Data has <strong>many features</strong> (curse of dimensionality)</li>
          <li>Features are on <strong>different scales</strong> (unless you standardize)</li>
          <li>You need <strong>fast predictions</strong> (e.g., real-time web apps)</li>
          <li>Data has <strong>many irrelevant features</strong> (distance gets diluted)</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-calculator"></i> 3.9 Worked Example: Classifying a New Point</h3>

    <div class="expanded-content">
      <h4>Step-by-Step Calculation</h4>
      
      <p><strong>Training data (simplified):</strong></p>
      <table class="detailed-table">
        <thead>
          <tr><th>Point</th><th>Feature X</th><th>Feature Y</th><th>Class</th></tr>
        </thead>
        <tbody>
          <tr><td>A</td><td>2</td><td>3</td><td>Red</td></tr>
          <tr><td>B</td><td>5</td><td>4</td><td>Blue</td></tr>
          <tr><td>C</td><td>1</td><td>1</td><td>Red</td></tr>
          <tr><td>D</td><td>6</td><td>2</td><td>Blue</td></tr>
          <tr><td>E</td><td>3</td><td>3</td><td>Red</td></tr>
          <tr><td>F</td><td>7</td><td>5</td><td>Blue</td></tr>
        </tbody>
      </table>

      <p><strong>New point P:</strong> (4, 3) with K=3, Euclidean distance</p>

      <table class="detailed-table">
        <thead>
          <tr><th>Point</th><th>Distance to P</th><th>Class</th></tr>
        </thead>
        <tbody>
          <tr><td>E (3,3)</td><td>\(\sqrt{(4-3)^2 + (3-3)^2} = 1.0\)</td><td>Red</td></tr>
          <tr><td>A (2,3)</td><td>\(\sqrt{(4-2)^2 + (3-3)^2} = 2.0\)</td><td>Red</td></tr>
          <tr><td>B (5,4)</td><td>\(\sqrt{(4-5)^2 + (3-4)^2} = \sqrt{1+1} = 1.41\)</td><td>Blue</td></tr>
          <tr><td>C (1,1)</td><td>\(\sqrt{(4-1)^2 + (3-1)^2} = \sqrt{9+4} = 3.61\)</td><td>Red</td></tr>
          <tr><td>D (6,2)</td><td>\(\sqrt{(4-6)^2 + (3-2)^2} = \sqrt{4+1} = 2.24\)</td><td>Blue</td></tr>
          <tr><td>F (7,5)</td><td>\(\sqrt{(4-7)^2 + (3-5)^2} = \sqrt{9+4} = 3.61\)</td><td>Blue</td></tr>
        </tbody>
      </table>

      <p><strong>3 nearest neighbors:</strong></p>
      <ul>
        <li>1st: E (Red) - distance 1.0</li>
        <li>2nd: B (Blue) - distance 1.41</li>
        <li>3rd: A (Red) - distance 2.0</li>
      </ul>

      <p><strong>Vote:</strong> Red = 2, Blue = 1 ‚Üí <span class="highlight">Predict RED</span></p>

      <p><strong>With distance weighting (1/d¬≤):</strong></p>
      <ul>
        <li>E: weight = 1/1¬≤ = 1.0 (Red)</li>
        <li>B: weight = 1/1.41¬≤ ‚âà 0.5 (Blue)</li>
        <li>A: weight = 1/2¬≤ = 0.25 (Red)</li>
        <li>Red total: 1.0 + 0.25 = 1.25</li>
        <li>Blue total: 0.5</li>
        <li>Still predicts RED (even stronger!)</li>
      </ul>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-graduation-cap"></i> Quick Teaching Summary</h3>

    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
      <div style="background: linear-gradient(135deg, rgba(96, 165, 250, 0.1), rgba(167, 139, 250, 0.1)); padding: 20px; border-radius: var(--radius);">
        <h4 style="margin-top: 0;">KNN Algorithm</h4>
        <ol style="margin-left: 20px;">
          <li>Choose K (number of neighbors)</li>
          <li>Choose distance metric</li>
          <li>Find K closest training points</li>
          <li>Take majority vote (weighted or uniform)</li>
          <li>Assign class to new point</li>
        </ol>
      </div>
      
      <div style="background: linear-gradient(135deg, rgba(96, 165, 250, 0.1), rgba(167, 139, 250, 0.1)); padding: 20px; border-radius: var(--radius);">
        <h4 style="margin-top: 0;">Key Properties</h4>
        <ul style="margin-left: 20px;">
          <li><strong>Lazy learner</strong> - no training</li>
          <li><strong>Non-parametric</strong> - no assumptions</li>
          <li><strong>Memory-based</strong> - stores all data</li>
          <li><strong>Distance-based</strong> - needs scaling</li>
        </ul>
      </div>
    </div>

    <div style="margin-top: 30px; text-align: center; padding: 20px; background: var(--panel2); border-radius: var(--radius);">
      <p style="font-size: 1.2rem;">
        <i class="fas fa-lightbulb" style="color: var(--warning);"></i> 
        <strong>Remember:</strong> In KNN, "K" controls the 
        <span class="highlight">bias-variance tradeoff</span> - small K = flexible but noisy, 
        large K = stable but potentially oversmoothed!
      </p>
    </div>
  </div>

  <!-- Code Editor (Enhanced) -->
  <div class="code-editor-container">
    <div class="code-header">
      <div class="code-title"><i class="fab fa-python"></i> KNN in Python (with Best Practices)</div>
    </div>
    <div class="code-body">
      <pre class="code-block"><code><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier
<span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler
<span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score, GridSearchCV
<span class="keyword">import</span> numpy <span class="keyword">as</span> np
<span class="keyword">import</span> pandas <span class="keyword">as</span> pd

<span class="comment"># Load your data</span>
<span class="comment"># X = features, y = target</span>

<span class="comment"># STEP 1: ALWAYS scale features for KNN!</span>
scaler = StandardScaler()
X_scaled = scaler.<span class="function">fit_transform</span>(X)

<span class="comment"># STEP 2: Find best K using cross-validation</span>
k_range = range(1, 31)
k_scores = []

<span class="keyword">for</span> k <span class="keyword">in</span> k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X_scaled, y, cv=5, scoring='accuracy')
    k_scores.append(scores.mean())

best_k = k_range[np.argmax(k_scores)]
<span class="keyword">print</span>(<span class="string">f"Best K: {best_k} with accuracy: {max(k_scores):.3f}"</span>)

<span class="comment"># STEP 3: Create optimized KNN model</span>
knn = KNeighborsClassifier(
    n_neighbors=best_k,
    weights='distance',        <span class="comment"># 'uniform' or 'distance'</span>
    metric='euclidean',        <span class="comment"># 'euclidean', 'manhattan', 'minkowski'</span>
    p=2,                        <span class="comment"># Power parameter for Minkowski (p=2 = Euclidean)</span>
    algorithm='auto'           <span class="comment"># 'auto', 'ball_tree', 'kd_tree', 'brute'</span>
)

knn.<span class="function">fit</span>(X_scaled, y)

<span class="comment"># Predict for new points</span>
new_point = np.array([[5.1, 3.5, 1.4, 0.2]])  <span class="comment"># Example features</span>
new_point_scaled = scaler.<span class="function">transform</span>(new_point)
prediction = knn.<span class="function">predict</span>(new_point_scaled)
probabilities = knn.<span class="function">predict_proba</span>(new_point_scaled)

<span class="keyword">print</span>(<span class="string">f"Predicted class: {prediction[0]}"</span>)
<span class="keyword">print</span>(<span class="string">f"Class probabilities: {probabilities[0]}"</span>)

<span class="comment"># Find which neighbors were used</span>
distances, indices = knn.kneighbors(new_point_scaled)
<span class="keyword">print</span>(<span class="string">f"Indices of {knn.n_neighbors} nearest neighbors: {indices[0]}"</span>)
<span class="keyword">print</span>(<span class="string">f"Distances to neighbors: {distances[0]}"</span>)

<span class="comment"># Grid search for hyperparameter tuning (more thorough)</span>
param_grid = {
    'n_neighbors': range(3, 21, 2),  <span class="comment"># Odd numbers to avoid ties</span>
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan', 'minkowski']
}

grid_search = GridSearchCV(
    KNeighborsClassifier(),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

grid_search.<span class="function">fit</span>(X_scaled, y)
<span class="keyword">print</span>(<span class="string">f"Best parameters: {grid_search.best_params_}"</span>)
<span class="keyword">print</span>(<span class="string">f"Best cross-validation score: {grid_search.best_score_:.3f}"</span>)</code></pre>
    </div>
  </div>

  <!-- Common Pitfalls Summary -->
  <div class="common-pitfall">
    <h4><i class="fas fa-exclamation-triangle"></i> KNN Gotchas - Quick Reference</h4>
    <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 15px; margin-top: 15px;">
      <div>
        <p><strong>‚ö†Ô∏è Feature Scaling</strong></p>
        <p>Always standardize/normalize! KNN is distance-based.</p>
      </div>
      <div>
        <p><strong>‚ö†Ô∏è Curse of Dimensionality</strong></p>
        <p>KNN struggles with >20 features. Use dimensionality reduction first.</p>
      </div>
      <div>
        <p><strong>‚ö†Ô∏è Prediction Speed</strong></p>
        <p>Slow for large datasets (O(n) per prediction). Consider approximations.</p>
      </div>
      <div>
        <p><strong>‚ö†Ô∏è K Selection</strong></p>
        <p>Use cross-validation, not guesswork. Odd K helps avoid ties.</p>
      </div>
      <div>
        <p><strong>‚ö†Ô∏è Imbalanced Data</strong></p>
        <p>KNN can be biased toward majority class. Consider weighting or resampling.</p>
      </div>
      <div>
        <p><strong>‚ö†Ô∏è Missing Values</strong></p>
        <p>KNN can't handle missing data directly. Impute first!</p>
      </div>
    </div>
  </div>
</div>

    <!-- ================================================
         SECTION 4: NAIVE BAYES
    ================================================= -->
   <!-- ================================================
     SECTION 4: NAIVE BAYES - ENHANCED
================================================= -->
<div id="naive-bayes" class="module-content">

  <div class="panel section">
    <h3><i class="fas fa-chart-pie"></i> 4.1 What Problem Does Naive Bayes Solve?</h3>

    <div class="info-box info">
      <h4><i class="fas fa-bullseye"></i> The Core Idea: Probability-Based Classification</h4>
      <p>
        <strong>Naive Bayes</strong> is a probabilistic classifier based on <strong>Bayes' Theorem</strong>. 
        It calculates the probability of each class given the features, and picks the most likely one.
      </p>
      
      <div style="text-align: center; margin: 25px 0;">
        <span class="math-equation" style="font-size: 1.8rem;">\(P(\text{class} | \text{features}) \propto P(\text{class}) \times P(\text{features} | \text{class})\)</span>
      </div>
      
      <p>
        <strong>The "Naive" part:</strong> It assumes all features are <span class="highlight">independent</span> given the class.
        This is almost always false in real life, but the algorithm works surprisingly well!
      </p>
    </div>

    <div class="explanation-box">
      <p><strong>üìö In Simple Terms:</strong> Start with your base rate (prior probability). Then multiply by "likelihood factors" for each feature. It's like updating your belief as you get new evidence.</p>
      <p><strong>Real-Life Example:</strong> This is how most spam filters worked historically! They calculate: P(spam | "win", "money", "click") ‚àù P(spam) √ó P("win"|spam) √ó P("money"|spam) √ó P("click"|spam). If the spam probability is high enough, mark as spam.</p>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-percent"></i> 4.2 Bayes' Theorem: The Foundation</h3>

    <div class="info-box info">
      <h4>The Formula That Changed Statistics</h4>
      
      <div style="text-align: center; margin: 30px 0;">
        <span class="math-equation" style="font-size: 2rem;">\(P(A|B) = \frac{P(B|A) \times P(A)}{P(B)}\)</span>
      </div>
      
      <p>In plain English:</p>
      <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 15px; margin: 20px 0;">
        <div style="text-align: center; padding: 15px; background: var(--panel2); border-radius: var(--radius);">
          <span class="math-equation">\(P(A|B)\)</span>
          <p><strong>Posterior</strong></p>
          <p>Probability of A given B<br><small>(what we want)</small></p>
        </div>
        <div style="text-align: center; padding: 15px; background: var(--panel2); border-radius: var(--radius);">
          <span class="math-equation">\(P(B|A)\)</span>
          <p><strong>Likelihood</strong></p>
          <p>Probability of B given A<br><small>(from data)</small></p>
        </div>
        <div style="text-align: center; padding: 15px; background: var(--panel2); border-radius: var(--radius);">
          <span class="math-equation">\(P(A)\)</span>
          <p><strong>Prior</strong></p>
          <p>Initial belief before seeing data</p>
        </div>
      </div>
    </div>

    <div class="expanded-content">
      <h4><i class="fas fa-calculator"></i> Worked Example: Medical Testing</h4>
      
      <p><strong>Scenario:</strong> A disease affects 1% of the population. A test is 95% accurate (95% true positive rate, 95% true negative rate). If you test positive, what's the probability you actually have the disease?</p>
      
      <table class="detailed-table">
        <thead>
          <tr><th>Term</th><th>Value</th><th>Meaning</th></tr>
        </thead>
        <tbody>
          <tr><td>P(Disease)</td><td>0.01</td><td>Prior - 1% have disease</td></tr>
          <tr><td>P(Positive|Disease)</td><td>0.95</td><td>Likelihood - test catches 95% of cases</td></tr>
          <tr><td>P(Positive|No Disease)</td><td>0.05</td><td>False positive rate</td></tr>
        </tbody>
      </table>
      
      <div style="background: var(--panel2); padding: 20px; border-radius: var(--radius); margin: 20px 0;">
        <p><strong>Step 1: Calculate P(Positive)</strong></p>
        <p>P(Positive) = P(Positive|Disease)√óP(Disease) + P(Positive|No Disease)√óP(No Disease)</p>
        <p>= 0.95 √ó 0.01 + 0.05 √ó 0.99 = 0.0095 + 0.0495 = 0.059</p>
        
        <p><strong>Step 2: Apply Bayes' Theorem</strong></p>
        <p>P(Disease|Positive) = [P(Positive|Disease) √ó P(Disease)] / P(Positive)</p>
        <p>= (0.95 √ó 0.01) / 0.059 = 0.0095 / 0.059 ‚âà <span class="highlight">0.161 or 16.1%</span></p>
      </div>
      
      <p><strong>Counterintuitive result:</strong> Even with a 95% accurate test, a positive result means only 16% chance of having the disease! This is because the disease is rare (low prior) and false positives from the 99% healthy population overwhelm the true positives.</p>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-random"></i> 4.3 The "Naive" Assumption: Feature Independence</h3>

    <div class="common-pitfall">
      <h4><i class="fas fa-exclamation-triangle"></i> What Does "Naive" Mean?</h4>
      <p>
        Naive Bayes assumes that <strong>all features are conditionally independent given the class</strong>:
      </p>
      
      <div style="text-align: center; margin: 20px 0;">
        <span class="math-equation">\(P(x_1, x_2, ..., x_n | y) = P(x_1|y) \times P(x_2|y) \times ... \times P(x_n|y)\)</span>
      </div>
      
      <p>
        In real life, features are often correlated (e.g., "win" and "money" often appear together in spam). 
        But the naive assumption simplifies calculations dramatically and still works well in practice!
      </p>
    </div>

    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;">
      <div style="background: linear-gradient(135deg, rgba(251, 113, 133, 0.1), rgba(239, 68, 68, 0.1)); padding: 20px; border-radius: var(--radius);">
        <h5 style="color: var(--danger);">‚ùå Reality (Dependent Features)</h5>
        <p>P(win, money | spam) ‚â† P(win|spam) √ó P(money|spam)</p>
        <p><em>Because "win" and "money" tend to occur together in spam emails</em></p>
      </div>
      
      <div style="background: linear-gradient(135deg, rgba(52, 211, 153, 0.1), rgba(16, 185, 129, 0.1)); padding: 20px; border-radius: var(--radius);">
        <h5 style="color: var(--success);">‚úÖ Naive Assumption</h5>
        <p>P(win, money | spam) = P(win|spam) √ó P(money|spam)</p>
        <p><em>Assumes independence - mathematically convenient, often works anyway</em></p>
      </div>
    </div>

    <div class="info-box info">
      <h4><i class="fas fa-lightbulb"></i> Why It Still Works</h4>
      <ul>
        <li><strong>Classification doesn't need perfect probabilities</strong> - just the right class</li>
        <li><strong>Dependencies often cancel out</strong> across classes</li>
        <li><strong>Very robust to noise</strong> - less overfitting than complex models</li>
        <li><strong>Works well for text</strong> where independence is a reasonable approximation</li>
      </ul>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-cubes"></i> 4.4 Types of Naive Bayes Classifiers</h3>

    <div class="algorithm-grid">
      <div class="algorithm-card">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-chart-bar"></i></div>
          <div class="algorithm-title">
            <h4>Gaussian Naive Bayes</h4>
            <p>For continuous features</p>
          </div>
        </div>
        <div class="algorithm-body">
          <p>Assumes features follow a <strong>normal (Gaussian) distribution</strong> within each class.</p>
          <div class="math-equation" style="text-align: center; margin: 10px 0;">
            \(P(x|y) = \frac{1}{\sqrt{2\pi\sigma_y^2}} e^{-\frac{(x-\mu_y)^2}{2\sigma_y^2}}\)
          </div>
          <p><strong>Use for:</strong> Height, weight, temperature, continuous measurements</p>
        </div>
      </div>

      <div class="algorithm-card">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-list-ul"></i></div>
          <div class="algorithm-title">
            <h4>Multinomial Naive Bayes</h4>
            <p>For count data</p>
          </div>
        </div>
        <div class="algorithm-body">
          <p>Assumes features represent <strong>counts or frequencies</strong>.</p>
          <div class="math-equation" style="text-align: center; margin: 10px 0;">
            \(P(x|y) = \frac{N_{xy} + \alpha}{N_y + \alpha n}\)
          </div>
          <p><strong>Use for:</strong> Word counts in documents, term frequencies</p>
          <p><em>This is the classic spam filter!</em></p>
        </div>
      </div>

      <div class="algorithm-card">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-toggle-on"></i></div>
          <div class="algorithm-title">
            <h4>Bernoulli Naive Bayes</h4>
            <p>For binary features</p>
          </div>
        </div>
        <div class="algorithm-body">
          <p>Assumes features are <strong>binary (0/1)</strong> indicating presence/absence.</p>
          <div class="math-equation" style="text-align: center; margin: 10px 0;">
            \(P(x|y) = p_{xy}^{x}(1-p_{xy})^{1-x}\)
          </div>
          <p><strong>Use for:</strong> Presence/absence of words, yes/no features</p>
        </div>
      </div>

      <div class="algorithm-card">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-chart-pie"></i></div>
          <div class="algorithm-title">
            <h4>Categorical Naive Bayes</h4>
            <p>For categorical features</p>
          </div>
        </div>
        <div class="algorithm-body">
          <p>For features with multiple categories (e.g., colors: red, blue, green).</p>
          <p><strong>Use for:</strong> Categories with no natural order</p>
        </div>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-calculator"></i> 4.5 Let's Calculate: Spam Filtering Example</h3>

    <div class="expanded-content">
      <h4>Step-by-Step with Real Data</h4>
      
      <p><strong>Training data (simplified emails):</strong></p>
      <table class="detailed-table">
        <thead>
          <tr><th>Email</th><th>Contains "win"</th><th>Contains "money"</th><th>Contains "click"</th><th>Class</th></tr>
        </thead>
        <tbody>
          <tr><td>1</td><td>Yes</td><td>Yes</td><td>Yes</td><td>Spam</td></tr>
          <tr><td>2</td><td>No</td><td>No</td><td>Yes</td><td>Not Spam</td></tr>
          <tr><td>3</td><td>Yes</td><td>No</td><td>Yes</td><td>Spam</td></tr>
          <tr><td>4</td><td>No</td><td>No</td><td>No</td><td>Not Spam</td></tr>
          <tr><td>5</td><td>Yes</td><td>Yes</td><td>No</td><td>Spam</td></tr>
          <tr><td>6</td><td>No</td><td>Yes</td><td>No</td><td>Not Spam</td></tr>
        </tbody>
      </table>

      <p><strong>Step 1: Calculate Priors P(Class)</strong></p>
      <ul>
        <li>P(Spam) = 3/6 = 0.5</li>
        <li>P(Not Spam) = 3/6 = 0.5</li>
      </ul>

      <p><strong>Step 2: Calculate Likelihoods P(Feature|Class)</strong></p>
      
      <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;">
        <div>
          <p><strong>For Spam emails (3 total):</strong></p>
          <ul>
            <li>P(win|Spam) = 3/3 = 1.0</li>
            <li>P(money|Spam) = 2/3 ‚âà 0.667</li>
            <li>P(click|Spam) = 2/3 ‚âà 0.667</li>
          </ul>
        </div>
        <div>
          <p><strong>For Not Spam emails (3 total):</strong></p>
          <ul>
            <li>P(win|Not Spam) = 0/3 = 0</li>
            <li>P(money|Not Spam) = 1/3 ‚âà 0.333</li>
            <li>P(click|Not Spam) = 1/3 ‚âà 0.333</li>
          </ul>
        </div>
      </div>

      <p><strong>Step 3: New email contains: "win", "money", "click"</strong></p>
      
      <div style="background: var(--panel2); padding: 20px; border-radius: var(--radius);">
        <p><strong>Spam calculation:</strong></p>
        <p>P(Spam|features) ‚àù P(Spam) √ó P(win|Spam) √ó P(money|Spam) √ó P(click|Spam)</p>
        <p>= 0.5 √ó 1.0 √ó 0.667 √ó 0.667 = <span class="highlight">0.222</span></p>
        
        <p><strong>Not Spam calculation:</strong></p>
        <p>P(Not Spam|features) ‚àù P(Not Spam) √ó P(win|Not Spam) √ó P(money|Not Spam) √ó P(click|Not Spam)</p>
        <p>= 0.5 √ó 0 √ó 0.333 √ó 0.333 = <span class="highlight">0</span></p>
        
        <p><strong>Normalize:</strong> P(Spam) = 0.222/(0.222 + 0) = <span class="highlight">1.0 (100% spam)</span></p>
      </div>

      <div class="common-pitfall" style="margin-top: 20px;">
        <p><strong>‚ö†Ô∏è Problem: Zero probabilities!</strong> If a feature never appears in training for a class, the whole probability becomes zero. This is why we need <strong>smoothing</strong>.</p>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-smooth"></i> 4.6 Laplace Smoothing: Handling Zero Probabilities</h3>

    <div class="info-box info">
      <h4>Additive Smoothing</h4>
      
      <div style="text-align: center; margin: 25px 0;">
        <span class="math-equation">\(P(x|y) = \frac{N_{xy} + \alpha}{N_y + \alpha n}\)</span>
      </div>
      
      <p>Where:</p>
      <ul>
        <li><strong>N<sub>xy</sub></strong> = count of feature x in class y</li>
        <li><strong>N<sub>y</sub></strong> = total count of class y</li>
        <li><strong>Œ±</strong> = smoothing parameter (usually 1)</li>
        <li><strong>n</strong> = number of possible feature values</li>
      </ul>
      
      <p><strong>Laplace smoothing (Œ±=1):</strong> Pretend we saw each feature once extra.</p>
    </div>

    <div class="expanded-content">
      <h4>Recalculating with Laplace Smoothing</h4>
      
      <p>For binary features, n = 2 (feature present or absent). With Œ± = 1:</p>
      
      <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;">
        <div>
          <p><strong>Spam (3 emails):</strong></p>
          <ul>
            <li>P(win|Spam) = (3 + 1)/(3 + 2) = 4/5 = 0.8</li>
            <li>P(money|Spam) = (2 + 1)/(3 + 2) = 3/5 = 0.6</li>
            <li>P(click|Spam) = (2 + 1)/(3 + 2) = 3/5 = 0.6</li>
          </ul>
        </div>
        <div>
          <p><strong>Not Spam (3 emails):</strong></p>
          <ul>
            <li>P(win|Not Spam) = (0 + 1)/(3 + 2) = 1/5 = 0.2</li>
            <li>P(money|Not Spam) = (1 + 1)/(3 + 2) = 2/5 = 0.4</li>
            <li>P(click|Not Spam) = (1 + 1)/(3 + 2) = 2/5 = 0.4</li>
          </ul>
        </div>
      </div>

      <p><strong>New calculation with smoothing:</strong></p>
      <div style="background: var(--panel2); padding: 20px; border-radius: var(--radius);">
        <p>P(Spam|features) ‚àù 0.5 √ó 0.8 √ó 0.6 √ó 0.6 = <span class="highlight">0.144</span></p>
        <p>P(Not Spam|features) ‚àù 0.5 √ó 0.2 √ó 0.4 √ó 0.4 = <span class="highlight">0.016</span></p>
        <p><strong>Normalized:</strong> P(Spam) = 0.144/(0.144 + 0.016) = <span class="highlight">0.90 (90% spam)</span></p>
      </div>
      
      <p>More realistic than 100%, and no zero probabilities!</p>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-tachometer-alt"></i> 4.7 Computational Advantages</h3>

    <div class="algorithm-grid">
      <div class="algorithm-card">
        <div class="algorithm-header" style="background: linear-gradient(135deg, rgba(52, 211, 153, 0.1), rgba(16, 185, 129, 0.1));">
          <div class="algorithm-icon" style="background: linear-gradient(135deg, var(--success), #10B981);">‚ö°</div>
          <div class="algorithm-title">
            <h4>Training: O(n √ó p)</h4>
          </div>
        </div>
        <div class="algorithm-body">
          <p>Just count frequencies! One pass through the data.</p>
          <p><strong>Extremely fast</strong> - even for millions of samples.</p>
        </div>
      </div>

      <div class="algorithm-card">
        <div class="algorithm-header" style="background: linear-gradient(135deg, rgba(52, 211, 153, 0.1), rgba(16, 185, 129, 0.1));">
          <div class="algorithm-icon" style="background: linear-gradient(135deg, var(--success), #10B981);">‚ö°</div>
          <div class="algorithm-title">
            <h4>Prediction: O(c √ó p)</h4>
          </div>
        </div>
        <div class="algorithm-body">
          <p>c = number of classes, p = features</p>
          <p><strong>Constant time</strong> - independent of training set size!</p>
        </div>
      </div>

      <div class="algorithm-card">
        <div class="algorithm-header" style="background: linear-gradient(135deg, rgba(52, 211, 153, 0.1), rgba(16, 185, 129, 0.1));">
          <div class="algorithm-icon" style="background: linear-gradient(135deg, var(--success), #10B981);">üíæ</div>
          <div class="algorithm-title">
            <h4>Memory: O(c √ó p)</h4>
          </div>
        </div>
        <div class="algorithm-body">
          <p>Only store conditional probabilities, not raw data.</p>
          <p><strong>Very memory efficient</strong> - great for large datasets.</p>
        </div>
      </div>
    </div>

    <div class="pro-tip">
      <p><strong>üí° Why this matters:</strong> Naive Bayes can handle datasets that are too large for KNN or SVM. It's often used as a baseline for big data problems.</p>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-balance-scale"></i> 4.8 Pros and Cons</h3>

    <div class="concept-grid">
      <div class="concept-card">
        <h5 style="color: var(--success);">‚úÖ Advantages</h5>
        <ul style="padding-left:20px;">
          <li><strong>Extremely fast</strong> - linear time training and prediction</li>
          <li><strong>Works well with high dimensions</strong> - handles thousands of features</li>
          <li><strong>Handles missing data naturally</strong> - just omit that feature</li>
          <li><strong>Incremental learning</strong> - easy to update with new data</li>
          <li><strong>Works with small datasets</strong> - doesn't need much data to estimate probabilities</li>
          <li><strong>Outputs well-calibrated probabilities</strong> - not just class labels</li>
          <li><strong>Robust to irrelevant features</strong> - they get uniform probabilities</li>
        </ul>
      </div>
      
      <div class="concept-card">
        <h5 style="color: var(--danger);">‚ùå Disadvantages</h5>
        <ul style="padding-left:20px;">
          <li><strong>Naive independence assumption</strong> - often violated in real data</li>
          <li><strong>Zero probability problem</strong> - requires smoothing</li>
          <li><strong>Sensitive to correlated features</strong> - double-counts evidence</li>
          <li><strong>Not great for regression</strong> - primarily for classification</li>
          <li><strong>Can be outperformed by more complex models</strong> when enough data exists</li>
          <li><strong>Assumes specific distributions</strong> (Gaussian, Multinomial, etc.)</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-check-double"></i> 4.9 When to Use Naive Bayes</h3>

    <div class="info-box success">
      <h4>‚úÖ Perfect for:</h4>
      <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 15px; margin-top: 15px;">
        <div>
          <p><strong>üìß Text Classification</strong></p>
          <ul>
            <li>Spam filtering</li>
            <li>Sentiment analysis</li>
            <li>News categorization</li>
            <li>Language detection</li>
          </ul>
        </div>
        <div>
          <p><strong>üîç Information Retrieval</strong></p>
          <ul>
            <li>Document classification</li>
            <li>Recommendation systems</li>
            <li>Search engines</li>
          </ul>
        </div>
        <div>
          <p><strong>üè• Medical Diagnosis</strong></p>
          <ul>
            <li>Symptom-based diagnosis</li>
            <li>Disease prediction</li>
            <li>When features are independent</li>
          </ul>
        </div>
        <div>
          <p><strong>üìä Real-time Systems</strong></p>
          <ul>
            <li>When speed is critical</li>
            <li>Large-scale systems</li>
            <li>Baseline models</li>
          </ul>
        </div>
      </div>
    </div>

    <div class="info-box warning" style="margin-top: 20px;">
      <h4>‚ùå Not ideal for:</h4>
      <ul>
        <li><strong>Highly correlated features</strong> - use logistic regression or trees instead</li>
        <li><strong>Complex relationships</strong> - neural networks or gradient boosting may work better</li>
        <li><strong>When you need feature interactions</strong> - naive bayes assumes independence</li>
      </ul>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-graduation-cap"></i> Quick Teaching Summary</h3>

    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
      <div style="background: linear-gradient(135deg, rgba(96, 165, 250, 0.1), rgba(167, 139, 250, 0.1)); padding: 20px; border-radius: var(--radius);">
        <h4 style="margin-top: 0;">Naive Bayes Algorithm</h4>
        <ol style="margin-left: 20px;">
          <li>Calculate prior P(class)</li>
          <li>Calculate likelihoods P(feature|class)</li>
          <li>Apply Bayes' theorem: P(class|features) ‚àù prior √ó likelihoods</li>
          <li>Add smoothing to avoid zeros</li>
          <li>Pick class with highest probability</li>
        </ol>
      </div>
      
      <div style="background: linear-gradient(135deg, rgba(96, 165, 250, 0.1), rgba(167, 139, 250, 0.1)); padding: 20px; border-radius: var(--radius);">
        <h4 style="margin-top: 0;">Key Properties</h4>
        <ul style="margin-left: 20px;">
          <li><strong>Probabilistic</strong> - outputs probabilities</li>
          <li><strong>Generative</strong> - models how data is generated</li>
          <li><strong>Linear time</strong> - extremely fast</li>
          <li><strong>Low variance</strong> - less overfitting</li>
        </ul>
      </div>
    </div>

    <div style="margin-top: 30px; text-align: center; padding: 20px; background: var(--panel2); border-radius: var(--radius);">
      <p style="font-size: 1.2rem;">
        <i class="fas fa-lightbulb" style="color: var(--warning);"></i> 
        <strong>Remember:</strong> Despite the "naive" assumption, Naive Bayes is 
        <span class="highlight">surprisingly effective</span>, especially for text classification!
      </p>
    </div>
  </div>

  <!-- Code Editor (Enhanced) -->
  <div class="code-editor-container">
    <div class="code-header">
      <div class="code-title"><i class="fab fa-python"></i> Naive Bayes in Python (All Flavors)</div>
    </div>
    <div class="code-body">
      <pre class="code-block"><code><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB, MultinomialNB, BernoulliNB, CategoricalNB
<span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer, TfidfVectorizer
<span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, cross_val_score
<span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report
<span class="keyword">import</span> numpy <span class="keyword">as</span> np
<span class="keyword">import</span> pandas <span class="keyword">as</span> pd

<span class="comment"># ============================================</span>
<span class="comment"># EXAMPLE 1: Text Classification (MultinomialNB)</span>
<span class="comment"># ============================================</span>
<span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_20newsgroups

<span class="comment"># Load text data</span>
categories = ['alt.atheism', 'soc.religion.christian']
newsgroups = fetch_20newsgroups(subset='train', categories=categories)

<span class="comment"># Convert text to word count vectors</span>
vectorizer = CountVectorizer(stop_words='english', max_features=1000)
X = vectorizer.<span class="function">fit_transform</span>(newsgroups.data)
y = newsgroups.target

<span class="comment"># Split data</span>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

<span class="comment"># Create and train Multinomial Naive Bayes</span>
mnb = MultinomialNB(alpha=1.0)  <span class="comment"># alpha = smoothing parameter</span>
mnb.<span class="function">fit</span>(X_train, y_train)

<span class="comment"># Predict</span>
y_pred = mnb.<span class="function">predict</span>(X_test)
y_proba = mnb.<span class="function">predict_proba</span>(X_test)

<span class="keyword">print</span>(<span class="string">"Multinomial Naive Bayes Results:"</span>)
<span class="keyword">print</span>(classification_report(y_test, y_pred, target_names=categories))

<span class="comment"># View most informative features</span>
feature_names = vectorizer.get_feature_names_out()
log_prob_ratio = mnb.feature_log_prob_[1] - mnb.feature_log_prob_[0]
top_indices = np.argsort(log_prob_ratio)[-10:]  <span class="comment"># Top 10 for class 1</span>

<span class="keyword">print</span>(<span class="string">"\nTop 10 features for class 1:"</span>)
<span class="keyword">for</span> idx <span class="keyword">in</span> top_indices[::-1]:
    <span class="keyword">print</span>(<span class="string">f"  {feature_names[idx]}: {log_prob_ratio[idx]:.3f}"</span>)

<span class="comment"># ============================================</span>
<span class="comment"># EXAMPLE 2: Continuous Features (GaussianNB)</span>
<span class="comment"># ============================================</span>
<span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris

iris = load_iris()
X, y = iris.data, iris.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

<span class="comment"># GaussianNB - assumes normal distribution</span>
gnb = GaussianNB()
gnb.<span class="function">fit</span>(X_train, y_train)

<span class="keyword">print</span>(<span class="string">"\nGaussian Naive Bayes Results:"</span>)
<span class="keyword">print</span>(<span class="string">f"Accuracy: {gnb.score(X_test, y_test):.3f}"</span>)
<span class="keyword">print</span>(<span class="string">f"Class priors: {gnb.class_prior_}"</span>)
<span class="keyword">print</span>(<span class="string">f"Class means:\n{gnb.theta_}"</span>)  <span class="comment"># Mean of each feature per class</span>
<span class="keyword">print</span>(<span class="string">f"Class variances:\n{gnb.var_}"</span>)  <span class="comment"># Variance of each feature per class</span>

<span class="comment"># ============================================</span>
<span class="comment"># EXAMPLE 3: Binary Features (BernoulliNB)</span>
<span class="comment"># ============================================</span>
<span class="comment"># For features that are 0/1 (presence/absence)</span>
X_binary = (X > X.mean(axis=0)).astype(int)  <span class="comment"># Convert to binary</span>

bnb = BernoulliNB(alpha=1.0, binarize=None)  <span class="comment"># binarize=None since already binary</span>
bnb.<span class="function">fit</span>(X_binary, y)

<span class="keyword">print</span>(<span class="string">"\nBernoulli Naive Bayes Results:"</span>)
<span class="keyword">print</span>(<span class="string">f"Cross-val score: {cross_val_score(bnb, X_binary, y, cv=5).mean():.3f}"</span>)

<span class="comment"># ============================================</span>
<span class="comment"># Hyperparameter tuning with smoothing</span>
<span class="comment"># ============================================</span>
alphas = np.logspace(-3, 2, 50)  <span class="comment"># Try different smoothing values</span>
scores = []

<span class="keyword">for</span> alpha <span class="keyword">in</span> alphas:
    mnb = MultinomialNB(alpha=alpha)
    scores.append(cross_val_score(mnb, X_train, y_train, cv=5).mean())

best_alpha = alphas[np.argmax(scores)]
<span class="keyword">print</span>(<span class="string">f"\nBest alpha: {best_alpha:.3f}"</span>)

<span class="comment"># ============================================</span>
<span class="comment"># Handling mixed feature types</span>
<span class="comment"># ============================================</span>
<span class="comment"># For mixed data types, you might need to:</span>
<span class="comment"># 1. Use different NB models for different feature sets</span>
<span class="comment"># 2. Combine predictions (ensemble)</span>
<span class="comment"># 3. Transform features to fit one model</span>

<span class="comment"># Example: Combining Gaussian and Multinomial</span>
<span class="comment"># (Simplified - in practice, use stacking or voting)</span>
gnb_pred_proba = gnb.predict_proba(X_test[:, :2])  <span class="comment"># First two continuous features</span>
mnb_pred_proba = mnb.predict_proba(X_test[:, 2:])  <span class="comment"># Last two as counts</span>
combined_proba = (gnb_pred_proba + mnb_pred_proba) / 2
combined_pred = np.argmax(combined_proba, axis=1)</code></pre>
    </div>
  </div>

  <!-- Interactive Demo Placeholder (preserved) -->
  <div class="panel section">
    <h3><i class="fas fa-play-circle"></i> 4.10 Interactive Naive Bayes Demo</h3>
    
    <div class="info-box info">
      <p><strong>Try it yourself:</strong> Adjust the smoothing parameter and see how it affects predictions.</p>
    </div>
    
    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
      <div style="background: var(--panel2); padding: 20px; border-radius: var(--radius);">
        <h4>Controls</h4>
        <div class="slider-container">
          <label>Alpha (smoothing):</label>
          <input type="range" id="nbAlpha" min="0.01" max="5" step="0.01" value="1.0">
          <span class="slider-value" id="nbAlphaValue">1.0</span>
        </div>
        <p>Small alpha = less smoothing, more sensitive to zeros<br>Large alpha = more smoothing, more uniform probabilities</p>
        
        <div class="button-group">
          <button class="btn btn-primary" onclick="updateNaiveBayes()">Update</button>
          <button class="btn btn-secondary" onclick="resetNaiveBayes()">Reset</button>
        </div>
      </div>
      
      <div style="background: var(--panel2); padding: 20px; border-radius: var(--radius);">
        <h4>Current Probabilities</h4>
        <div style="margin: 15px 0;">
          <div style="display: flex; justify-content: space-between;">
            <span>P(Spam|features):</span>
            <span class="highlight" id="nbSpamProb">0.90</span>
          </div>
          <div style="height: 20px; background: var(--border); border-radius: 10px; margin: 5px 0;">
            <div id="nbSpamBar" style="width: 90%; height: 100%; background: var(--danger); border-radius: 10px;"></div>
          </div>
          
          <div style="display: flex; justify-content: space-between; margin-top: 15px;">
            <span>P(Not Spam|features):</span>
            <span class="highlight" id="nbNotSpamProb">0.10</span>
          </div>
          <div style="height: 20px; background: var(--border); border-radius: 10px; margin: 5px 0;">
            <div id="nbNotSpamBar" style="width: 10%; height: 100%; background: var(--success); border-radius: 10px;"></div>
          </div>
        </div>
        
        <p><strong>Prediction:</strong> <span id="nbPrediction" style="color: var(--danger); font-weight: bold;">SPAM</span></p>
      </div>
    </div>
    
    <div class="common-pitfall" style="margin-top: 20px;">
      <p><strong>üìù Note:</strong> This demo uses the smoothed probabilities from our earlier example. Adjust alpha to see how it affects the balance between spam and not spam probabilities.</p>
    </div>
  </div>

  <!-- Quick Reference Summary -->
  <div class="panel section">
    <h3><i class="fas fa-clipboard-list"></i> Naive Bayes Cheat Sheet</h3>
    
    <table class="detailed-table">
      <thead>
        <tr><th>Variant</th><th>Feature Type</th><th>Distribution</th><th>Common Use</th></tr>
      </thead>
      <tbody>
        <tr><td>GaussianNB</td><td>Continuous</td><td>Normal</td><td>Measurement data</td></tr>
        <tr><td>MultinomialNB</td><td>Counts</td><td>Multinomial</td><td>Text classification</td></tr>
        <tr><td>BernoulliNB</td><td>Binary</td><td>Bernoulli</td><td>Presence/absence</td></tr>
        <tr><td>CategoricalNB</td><td>Categorical</td><td>Categorical</td><td>Categories</td></tr>
      </tbody>
    </table>
    
    <div style="margin-top: 20px; padding: 15px; background: linear-gradient(135deg, rgba(96, 165, 250, 0.1), rgba(167, 139, 250, 0.1)); border-radius: var(--radius);">
      <p><strong>Key Parameters:</strong></p>
      <ul>
        <li><strong>alpha:</strong> Smoothing parameter (default=1.0). Prevents zero probabilities.</li>
        <li><strong>class_prior:</strong> Override prior probabilities if you know them.</li>
        <li><strong>binarize:</strong> For BernoulliNB, threshold for binarizing features.</li>
      </ul>
    </div>
  </div>
</div>

<script>
// Interactive Naive Bayes Demo
function updateNaiveBayes() {
    const alpha = parseFloat(document.getElementById('nbAlpha').value);
    document.getElementById('nbAlphaValue').textContent = alpha.toFixed(2);
    
    // Recalculate with smoothing
    // Based on our example: email with "win", "money", "click"
    
    // Priors
    const pSpam = 0.5;
    const pNotSpam = 0.5;
    
    // With smoothing: P(feature|class) = (count + alpha) / (total + 2*alpha)
    // For spam: 3 emails, counts: win=3, money=2, click=2
    const pWinSpam = (3 + alpha) / (3 + 2*alpha);
    const pMoneySpam = (2 + alpha) / (3 + 2*alpha);
    const pClickSpam = (2 + alpha) / (3 + 2*alpha);
    
    // For not spam: 3 emails, counts: win=0, money=1, click=1
    const pWinNotSpam = (0 + alpha) / (3 + 2*alpha);
    const pMoneyNotSpam = (1 + alpha) / (3 + 2*alpha);
    const pClickNotSpam = (1 + alpha) / (3 + 2*alpha);
    
    // Calculate unnormalized posteriors
    const spamUnnorm = pSpam * pWinSpam * pMoneySpam * pClickSpam;
    const notSpamUnnorm = pNotSpam * pWinNotSpam * pMoneyNotSpam * pClickNotSpam;
    
    // Normalize
    const total = spamUnnorm + notSpamUnnorm;
    const spamProb = spamUnnorm / total;
    const notSpamProb = notSpamUnnorm / total;
    
    // Update display
    document.getElementById('nbSpamProb').textContent = spamProb.toFixed(3);
    document.getElementById('nbNotSpamProb').textContent = notSpamProb.toFixed(3);
    document.getElementById('nbSpamBar').style.width = (spamProb * 100) + '%';
    document.getElementById('nbNotSpamBar').style.width = (notSpamProb * 100) + '%';
    
    const prediction = spamProb > notSpamProb ? 'SPAM' : 'NOT SPAM';
    const predColor = spamProb > notSpamProb ? 'var(--danger)' : 'var(--success)';
    document.getElementById('nbPrediction').textContent = prediction;
    document.getElementById('nbPrediction').style.color = predColor;
}

function resetNaiveBayes() {
    document.getElementById('nbAlpha').value = 1.0;
    updateNaiveBayes();
}

// Initialize demo when section loads
document.addEventListener('DOMContentLoaded', () => {
    // Check if we're on the naive bayes section
    const nbAlpha = document.getElementById('nbAlpha');
    if (nbAlpha) {
        nbAlpha.addEventListener('input', updateNaiveBayes);
        updateNaiveBayes();
    }
});
</script>

    <!-- ================================================
         SECTION 5: SUPPORT VECTOR MACHINES (SVM)
    ================================================= -->
    <!-- ================================================
     SECTION 5: SUPPORT VECTOR MACHINES (SVM) - ENHANCED
================================================= -->
<div id="svm" class="module-content">

  <div class="panel section">
    <h3><i class="fas fa-vector-square"></i> 5.1 What Problem Does SVM Solve?</h3>

    <div class="info-box info">
      <h4><i class="fas fa-bullseye"></i> The Core Idea: Find the Best Dividing Line</h4>
      <p>
        <strong>Support Vector Machine (SVM)</strong> finds the optimal hyperplane that separates classes with the 
        <span class="highlight">maximum margin</span> - the largest possible distance to the closest points from each class.
      </p>
      
      <div style="display: flex; gap: 20px; flex-wrap: wrap; margin-top: 15px; justify-content: center;">
        <span class="badge"><i class="fas fa-check-circle"></i> Maximize margin</span>
        <span class="badge"><i class="fas fa-check-circle"></i> Find support vectors</span>
        <span class="badge"><i class="fas fa-check-circle"></i> Kernel trick for non-linearity</span>
      </div>
      
      <p style="margin-top: 15px;">
        <strong>Key insight:</strong> The points closest to the decision boundary (the <strong>support vectors</strong>) 
        are the only ones that matter - all other points can be ignored!
      </p>
    </div>

    <div class="explanation-box">
      <p><strong>üìö In Simple Terms:</strong> Imagine drawing the widest possible street between two groups of houses. SVM finds the middle line of that street, and the houses at the edges (support vectors) are the only ones that define where the street goes. Move other houses around, and the street stays the same!</p>
      <p><strong>Real-Life Example:</strong> In image classification, SVM might find the optimal boundary between "cat" and "dog" images by projecting pixel values into a higher-dimensional space where they're easier to separate.</p>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-ruler"></i> 5.2 Maximum Margin Classification</h3>

    <div class="info-box info">
      <h4>The Geometry of Separation</h4>
      
      <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;">
        <div style="text-align: center;">
          <div style="background: var(--panel2); padding: 20px; border-radius: var(--radius);">
            <svg width="200" height="200" viewBox="0 0 200 200">
              <!-- Many possible boundaries -->
              <line x1="20" y1="180" x2="180" y2="20" stroke="var(--muted2)" stroke-width="2" stroke-dasharray="5,5"/>
              <line x1="40" y1="180" x2="180" y2="40" stroke="var(--muted2)" stroke-width="2" stroke-dasharray="5,5"/>
              <line x1="60" y1="180" x2="180" y2="60" stroke="var(--muted2)" stroke-width="2" stroke-dasharray="5,5"/>
              <!-- Data points -->
              <circle cx="60" cy="140" r="8" fill="var(--primary)"/>
              <circle cx="80" cy="120" r="8" fill="var(--primary)"/>
              <circle cx="100" cy="100" r="8" fill="var(--primary)"/>
              <circle cx="120" cy="80" r="8" fill="var(--primary)"/>
              <circle cx="140" cy="60" r="8" fill="var(--primary)"/>
              <circle cx="140" cy="140" r="8" fill="var(--danger)"/>
              <circle cx="120" cy="120" r="8" fill="var(--danger)"/>
              <circle cx="100" cy="100" r="8" fill="var(--danger)"/>
              <circle cx="80" cy="80" r="8" fill="var(--danger)"/>
              <circle cx="60" cy="60" r="8" fill="var(--danger)"/>
            </svg>
            <p><strong>Many possible boundaries</strong> - which is best?</p>
          </div>
        </div>
        
        <div style="text-align: center;">
          <div style="background: var(--panel2); padding: 20px; border-radius: var(--radius);">
            <svg width="200" height="200" viewBox="0 0 200 200">
              <!-- Maximum margin boundary -->
              <line x1="30" y1="170" x2="170" y2="30" stroke="var(--success)" stroke-width="3"/>
              <!-- Margin lines -->
              <line x1="25" y1="175" x2="165" y2="35" stroke="var(--success)" stroke-width="1" stroke-dasharray="5,5"/>
              <line x1="35" y1="165" x2="175" y2="25" stroke="var(--success)" stroke-width="1" stroke-dasharray="5,5"/>
              <!-- Support vectors (highlighted) -->
              <circle cx="60" cy="140" r="10" fill="none" stroke="var(--warning)" stroke-width="3"/>
              <circle cx="140" cy="60" r="10" fill="none" stroke="var(--warning)" stroke-width="3"/>
              <!-- Data points -->
              <circle cx="60" cy="140" r="8" fill="var(--primary)"/>
              <circle cx="80" cy="120" r="8" fill="var(--primary)"/>
              <circle cx="100" cy="100" r="8" fill="var(--primary)"/>
              <circle cx="120" cy="80" r="8" fill="var(--primary)"/>
              <circle cx="140" cy="60" r="8" fill="var(--primary)"/>
              <circle cx="140" cy="140" r="8" fill="var(--danger)"/>
              <circle cx="120" cy="120" r="8" fill="var(--danger)"/>
              <circle cx="100" cy="100" r="8" fill="var(--danger)"/>
              <circle cx="80" cy="80" r="8" fill="var(--danger)"/>
              <circle cx="60" cy="60" r="8" fill="var(--danger)"/>
            </svg>
            <p><strong>Maximum margin boundary</strong> - widest street between classes</p>
          </div>
        </div>
      </div>
      
      <p>
        <strong>Why maximize margin?</strong> Larger margin = better generalization. 
        The decision boundary is less sensitive to small variations in the data.
      </p>
    </div>

    <div class="math-equation" style="text-align: center; margin: 25px 0; font-size: 1.3rem;">
      <strong>Objective:</strong> Maximize \(\frac{2}{\|\mathbf{w}\|}\) subject to \(y_i(\mathbf{w} \cdot \mathbf{x}_i + b) \geq 1\)
    </div>
    
    <p>Where:</p>
    <ul>
      <li><strong>w</strong> = weight vector (normal to hyperplane)</li>
      <li><strong>b</strong> = bias term</li>
      <li><strong>y_i</strong> = class label (+1 or -1)</li>
      <li><strong>Margin</strong> = 2/||w|| (distance between support vectors)</li>
    </ul>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-hand-pointer"></i> 5.3 Support Vectors: The Critical Points</h3>

    <div class="expanded-content">
      <h4>Only the Points on the Edge Matter</h4>
      
      <p>
        <strong>Support vectors</strong> are the training examples that lie exactly on the margin boundaries.
        They are the only points that influence the decision boundary.
      </p>
      
      <div style="display: grid; grid-template-columns: 2fr 1fr; gap: 20px; margin: 25px 0;">
        <div>
          <ul>
            <li><strong>If you move a support vector</strong> ‚Üí the decision boundary changes</li>
            <li><strong>If you move any other point</strong> ‚Üí no effect on the boundary (as long as it stays on its side of the margin)</li>
            <li><strong>Number of support vectors</strong> is typically small ‚Üí sparse solution</li>
          </ul>
          
          <div style="background: var(--panel); padding: 15px; border-radius: 8px; margin-top: 15px;">
            <p><strong>Analogy:</strong> In a game of tug-of-war, only the people at the very front and back determine the position of the rope. Everyone in the middle just follows along!</p>
          </div>
        </div>
        
        <div style="background: var(--panel2); padding: 15px; border-radius: var(--radius);">
          <p><strong>Properties:</strong></p>
          <ul>
            <li>Typically 10-30% of training data</li>
            <li>Lie on or inside the margin</li>
            <li>Have non-zero Lagrange multipliers</li>
            <li>Define the model complexity</li>
          </ul>
        </div>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-sliders-h"></i> 5.4 Interactive SVM Demo</h3>

    <div class="regression-container">
      <div class="classification-chart"><canvas id="svmChart"></canvas></div>

      <div class="classification-controls">
        <div class="control-group">
          <h4><i class="fas fa-sliders-h"></i> SVM Hyperparameters</h4>
          
          <div class="slider-container">
            <label><i class="fas fa-arrows-alt"></i> C (Regularization):</label>
            <input type="range" id="svmC" min="0.01" max="10" step="0.1" value="1">
            <span class="slider-value" id="svmCValue">1.0</span>
          </div>
          <p style="font-size:12px;">
            <span style="color: var(--danger);">Small C = wider margin, allows more violations</span><br>
            <span style="color: var(--success);">Large C = harder margin, fewer violations</span>
          </p>
          
          <div class="slider-container">
            <label><i class="fas fa-wave-square"></i> Gamma (Œ≥):</label>
            <input type="range" id="svmGamma" min="0.1" max="5" step="0.1" value="1">
            <span class="slider-value" id="svmGammaValue">1.0</span>
          </div>
          <p style="font-size:12px;">
            <span style="color: var(--danger);">High gamma = each point has strong influence (overfitting)</span><br>
            <span style="color: var(--success);">Low gamma = smoother boundary</span>
          </p>
          
          <div class="slider-container">
            <label><i class="fas fa-circle"></i> Kernel:</label>
            <select id="svmKernel" class="btn btn-secondary" style="padding: 8px;">
              <option value="linear">Linear</option>
              <option value="poly">Polynomial (degree=3)</option>
              <option value="rbf" selected>RBF (Radial Basis Function)</option>
              <option value="sigmoid">Sigmoid</option>
            </select>
          </div>
        </div>

        <div class="button-group">
          <button class="btn btn-primary" onclick="updateSVM()"><i class="fas fa-play"></i> Update SVM</button>
          <button class="btn btn-secondary" onclick="generateSVMData()"><i class="fas fa-sync"></i> New Data</button>
        </div>

        <div class="metrics-container" style="margin-top: 20px;">
          <div class="metric-card">
            <div class="metric-label">Support Vectors</div>
            <div class="metric-value" id="svmSV">12</div>
          </div>
          <div class="metric-card">
            <div class="metric-label">Margin Width</div>
            <div class="metric-value" id="svmMargin">1.2</div>
          </div>
          <div class="metric-card">
            <div class="metric-label">Accuracy</div>
            <div class="metric-value" id="svmAcc">0.94</div>
          </div>
          <div class="metric-card">
            <div class="metric-label">Training Time</div>
            <div class="metric-value" id="svmTime">O(n¬≤)</div>
          </div>
        </div>

        <div class="explanation-box" style="margin-top: 20px;">
          <p><strong>üëÜ Experiment with the controls:</strong></p>
          <ul style="padding-left:20px;">
            <li><strong>C small ‚Üí large margin:</strong> More tolerant of errors, simpler model</li>
            <li><strong>C large ‚Üí narrow margin:</strong> Tries to classify all points correctly, may overfit</li>
            <li><strong>Gamma high:</strong> Each point has a "bubble" of influence - wiggly boundary</li>
            <li><strong>Different kernels:</strong> Change the shape of possible boundaries</li>
          </ul>
        </div>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-magic"></i> 5.5 The Kernel Trick: Non-Linear Boundaries</h3>

    <div class="info-box info">
      <h4><i class="fas fa-lightbulb"></i> Transforming to Higher Dimensions</h4>
      
      <p>
        <strong>The problem:</strong> What if data isn't linearly separable in the original feature space?
      </p>
      
      <p>
        <strong>The kernel trick:</strong> Project data into a higher-dimensional space where it becomes linearly separable,
        without actually computing the transformation!
      </p>
      
      <div style="text-align: center; margin: 30px 0;">
        <span class="math-equation">\(K(\mathbf{x}_i, \mathbf{x}_j) = \phi(\mathbf{x}_i) \cdot \phi(\mathbf{x}_j)\)</span>
      </div>
      
      <p>
        The kernel function computes dot products in the transformed space <strong>implicitly</strong>.
        This is computationally magical!
      </p>
    </div>

    <div class="expanded-content">
      <h4>Common Kernels and Their Effects</h4>
      
      <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px;">
        <div class="algorithm-card">
          <div class="algorithm-header">
            <div class="algorithm-icon"><i class="fas fa-grip-lines"></i></div>
            <div class="algorithm-title">
              <h4>Linear Kernel</h4>
            </div>
          </div>
          <div class="algorithm-body">
            <div class="math-equation">\(K(\mathbf{x}_i, \mathbf{x}_j) = \mathbf{x}_i \cdot \mathbf{x}_j\)</div>
            <p>No transformation - just the original dot product. Creates linear boundaries.</p>
            <p><strong>Use when:</strong> Data is already linearly separable</p>
          </div>
        </div>
        
        <div class="algorithm-card">
          <div class="algorithm-header">
            <div class="algorithm-icon"><i class="fas fa-chart-line"></i></div>
            <div class="algorithm-title">
              <h4>Polynomial Kernel</h4>
            </div>
          </div>
          <div class="algorithm-body">
            <div class="math-equation">\(K(\mathbf{x}_i, \mathbf{x}_j) = (\mathbf{x}_i \cdot \mathbf{x}_j + r)^d\)</div>
            <p>Creates polynomial decision boundaries of degree d.</p>
            <p><strong>Use when:</strong> Data has polynomial relationships</p>
          </div>
        </div>
        
        <div class="algorithm-card">
          <div class="algorithm-header">
            <div class="algorithm-icon"><i class="fas fa-mountain"></i></div>
            <div class="algorithm-title">
              <h4>RBF Kernel (Gaussian)</h4>
            </div>
          </div>
          <div class="algorithm-body">
            <div class="math-equation">\(K(\mathbf{x}_i, \mathbf{x}_j) = e^{-\gamma \|\mathbf{x}_i - \mathbf{x}_j\|^2}\)</div>
            <p>Most popular! Creates complex non-linear boundaries. Think of placing "bumps" at each point.</p>
            <p><strong>Use when:</strong> You don't know the relationship - RBF is a good default</p>
          </div>
        </div>
        
        <div class="algorithm-card">
          <div class="algorithm-header">
            <div class="algorithm-icon"><i class="fas fa-sigmoid"></i></div>
            <div class="algorithm-title">
              <h4>Sigmoid Kernel</h4>
            </div>
          </div>
          <div class="algorithm-body">
            <div class="math-equation">\(K(\mathbf{x}_i, \mathbf{x}_j) = \tanh(\gamma \mathbf{x}_i \cdot \mathbf{x}_j + r)\)</div>
            <p>Similar to neural network activation functions.</p>
            <p><strong>Use when:</strong> You want something like a 2-layer neural network</p>
          </div>
        </div>
      </div>
    </div>

    <div class="pro-tip">
      <p><strong>üí° RBF Kernel Intuition:</strong> Each point becomes the center of a Gaussian "bump". The gamma parameter controls how wide each bump is. Small gamma = wide bumps (smooth decision boundary). Large gamma = narrow bumps (each point has isolated influence - overfitting).</p>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-balance-scale"></i> 5.6 Soft Margin SVM: Dealing with Overlap</h3>

    <div class="info-box info">
      <h4>The C Parameter: Trading Off Margin vs. Errors</h4>
      
      <p>
        Real data often isn't perfectly separable. <strong>Soft margin SVM</strong> allows some points to be:
      </p>
      <ul>
        <li>Inside the margin (margin violations)</li>
        <li>On the wrong side of the decision boundary (misclassifications)</li>
      </ul>
      
      <div style="text-align: center; margin: 25px 0;">
        <span class="math-equation">\(\min \frac{1}{2}\|\mathbf{w}\|^2 + C \sum_{i=1}^{n} \xi_i\)</span>
      </div>
      
      <p>Where:</p>
      <ul>
        <li><strong>Œæ_i</strong> = slack variables (how far a point is on the wrong side)</li>
        <li><strong>C</strong> = regularization parameter (penalty for errors)</li>
      </ul>
    </div>

    <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 20px; margin: 25px 0;">
      <div style="background: var(--panel2); padding: 15px; border-radius: var(--radius); text-align: center;">
        <h4 style="color: var(--danger);">Small C</h4>
        <svg width="150" height="150" viewBox="0 0 150 150">
          <line x1="30" y1="120" x2="120" y2="30" stroke="var(--success)" stroke-width="2"/>
          <line x1="25" y1="125" x2="115" y2="35" stroke="var(--success)" stroke-width="1" stroke-dasharray="3,3"/>
          <line x1="35" y1="115" x2="125" y2="25" stroke="var(--success)" stroke-width="1" stroke-dasharray="3,3"/>
          <circle cx="60" cy="90" r="6" fill="var(--primary)"/>
          <circle cx="45" cy="105" r="6" fill="var(--primary)"/>
          <circle cx="90" cy="60" r="6" fill="var(--danger)"/>
          <circle cx="105" cy="45" r="6" fill="var(--danger)"/>
          <circle cx="80" cy="80" r="6" fill="var(--danger)"/>
        </svg>
        <p>Wide margin, allows violations<br><em>Simpler model</em></p>
      </div>
      
      <div style="background: var(--panel2); padding: 15px; border-radius: var(--radius); text-align: center;">
        <h4 style="color: var(--success);">Medium C</h4>
        <svg width="150" height="150" viewBox="0 0 150 150">
          <line x1="35" y1="115" x2="115" y2="35" stroke="var(--success)" stroke-width="2"/>
          <line x1="30" y1="120" x2="110" y2="40" stroke="var(--success)" stroke-width="1" stroke-dasharray="3,3"/>
          <line x1="40" y1="110" x2="120" y2="30" stroke="var(--success)" stroke-width="1" stroke-dasharray="3,3"/>
          <circle cx="60" cy="90" r="6" fill="var(--primary)"/>
          <circle cx="45" cy="105" r="6" fill="var(--primary)"/>
          <circle cx="90" cy="60" r="6" fill="var(--danger)"/>
          <circle cx="105" cy="45" r="6" fill="var(--danger)"/>
        </svg>
        <p>Balanced margin<br><em>Good generalization</em></p>
      </div>
      
      <div style="background: var(--panel2); padding: 15px; border-radius: var(--radius); text-align: center;">
        <h4 style="color: var(--danger);">Large C</h4>
        <svg width="150" height="150" viewBox="0 0 150 150">
          <line x1="45" y1="105" x2="105" y2="45" stroke="var(--success)" stroke-width="2"/>
          <line x1="43" y1="107" x2="103" y2="47" stroke="var(--success)" stroke-width="1" stroke-dasharray="3,3"/>
          <line x1="47" y1="103" x2="107" y2="43" stroke="var(--success)" stroke-width="1" stroke-dasharray="3,3"/>
          <circle cx="60" cy="90" r="6" fill="var(--primary)"/>
          <circle cx="45" cy="105" r="6" fill="var(--primary)"/>
          <circle cx="90" cy="60" r="6" fill="var(--danger)"/>
          <circle cx="105" cy="45" r="6" fill="var(--danger)"/>
        </svg>
        <p>Narrow margin, few violations<br><em>May overfit</em></p>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-tachometer-alt"></i> 5.7 Computational Complexity</h3>

    <div class="algorithm-grid">
      <div class="algorithm-card">
        <div class="algorithm-header" style="background: linear-gradient(135deg, rgba(251, 113, 133, 0.1), rgba(239, 68, 68, 0.1));">
          <div class="algorithm-icon" style="background: linear-gradient(135deg, var(--danger), #EF4444);">üê¢</div>
          <div class="algorithm-title">
            <h4>Training: O(n¬≤) to O(n¬≥)</h4>
          </div>
        </div>
        <div class="algorithm-body">
          <p><strong>Quadratic or cubic</strong> in number of samples.</p>
          <p>For n=100,000 samples ‚Üí 10 billion operations!</p>
          <p><em>Scales poorly with dataset size.</em></p>
        </div>
      </div>

      <div class="algorithm-card">
        <div class="algorithm-header" style="background: linear-gradient(135deg, rgba(52, 211, 153, 0.1), rgba(16, 185, 129, 0.1));">
          <div class="algorithm-icon" style="background: linear-gradient(135deg, var(--success), #10B981);">‚ö°</div>
          <div class="algorithm-title">
            <h4>Prediction: O(n_sv √ó p)</h4>
          </div>
        </div>
        <div class="algorithm-body">
          <p>Depends on <strong>number of support vectors</strong>, not total samples.</p>
          <p>Typically much faster than training.</p>
          <p><em>Sparse solution - only support vectors matter.</em></p>
        </div>
      </div>

      <div class="algorithm-card">
        <div class="algorithm-header" style="background: linear-gradient(135deg, rgba(251, 191, 36, 0.1), rgba(245, 158, 11, 0.1));">
          <div class="algorithm-icon" style="background: linear-gradient(135deg, var(--warning), #F59E0B);">üíæ</div>
          <div class="algorithm-title">
            <h4>Memory: O(n_sv √ó p)</h4>
          </div>
        </div>
        <div class="algorithm-body">
          <p>Only store support vectors and their weights.</p>
          <p>Can be memory efficient if n_sv is small.</p>
        </div>
      </div>
    </div>

    <div class="info-box warning">
      <h4><i class="fas fa-lightbulb"></i> Practical Advice</h4>
      <ul>
        <li><strong>For n < 10,000:</strong> SVM works great, use full power</li>
        <li><strong>For 10,000 < n < 100,000:</strong> Consider linear SVM or use approximations</li>
        <li><strong>For n > 100,000:</strong> SVM becomes impractical - use linear models or neural networks</li>
      </ul>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-check-double"></i> 5.8 When to Use SVM</h3>

    <div class="concept-grid">
      <div class="concept-card">
        <h5 style="color: var(--success);">‚úÖ USE when:</h5>
        <ul style="padding-left:20px;">
          <li><strong>Medium-sized dataset</strong> (n < 10,000) - training time is manageable</li>
          <li><strong>High-dimensional data</strong> (p large) - SVM handles many features well</li>
          <li><strong>Clear margin separation</strong> - classes are reasonably separable</li>
          <li><strong>Complex boundaries needed</strong> - RBF kernel can capture non-linearity</li>
          <li><strong>Interpretability isn't critical</strong> - SVM is a bit of a black box</li>
          <li><strong>You need robust performance</strong> - SVM often wins competitions</li>
        </ul>
      </div>
      
      <div class="concept-card">
        <h5 style="color: var(--danger);">‚ùå AVOID when:</h5>
        <ul style="padding-left:20px;">
          <li><strong>Very large dataset</strong> (n > 100,000) - training is too slow</li>
          <li><strong>Need probabilistic outputs</strong> - SVM probabilities are calibrated post-hoc</li>
          <li><strong>Features are on different scales</strong> - requires careful preprocessing</li>
          <li><strong>Noisy data with overlap</strong> - other methods may handle noise better</li>
          <li><strong>Need fast predictions</strong> - many support vectors can slow prediction</li>
          <li><strong>Interpretability is crucial</strong> - trees or logistic regression are better</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-exclamation-triangle"></i> 5.9 Common Pitfalls and Solutions</h3>

    <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px;">
      <div class="common-pitfall">
        <h4>‚ö†Ô∏è Forgetting to Scale Features</h4>
        <p><strong>Problem:</strong> SVM is distance-based - features with larger scales dominate.</p>
        <p><strong>Solution:</strong> Always standardize (mean=0, variance=1) before training.</p>
        <div class="code-block" style="padding: 10px; margin-top: 10px;">
          <code><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler<br>
scaler = StandardScaler()<br>
X_scaled = scaler.fit_transform(X)</code>
        </div>
      </div>

      <div class="common-pitfall">
        <h4>‚ö†Ô∏è Choosing Wrong Kernel</h4>
        <p><strong>Problem:</strong> Linear kernel for non-linear data, or RBF for simple linear data.</p>
        <p><strong>Solution:</strong> Start with RBF (it's most flexible), use cross-validation to compare.</p>
      </div>

      <div class="common-pitfall">
        <h4>‚ö†Ô∏è Poor C and Gamma Selection</h4>
        <p><strong>Problem:</strong> Overfitting (C too high, gamma too high) or underfitting.</p>
        <p><strong>Solution:</strong> Grid search with cross-validation.</p>
        <div class="code-block" style="padding: 10px; margin-top: 10px;">
          <code>param_grid = {'C': [0.1, 1, 10], 'gamma': [0.01, 0.1, 1]}</code>
        </div>
      </div>

      <div class="common-pitfall">
        <h4>‚ö†Ô∏è Imbalanced Classes</h4>
        <p><strong>Problem:</strong> SVM biased toward majority class.</p>
        <p><strong>Solution:</strong> Use class_weight='balanced' or set class weights manually.</p>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-calculator"></i> 5.10 Worked Example: XOR Problem</h3>

    <div class="expanded-content">
      <h4>The Classic Non-Linear Problem</h4>
      
      <p><strong>XOR dataset:</strong> Points at (0,0) and (1,1) are class 0, points at (0,1) and (1,0) are class 1.</p>
      
      <table class="detailed-table">
        <thead>
          <tr><th>Point</th><th>x‚ÇÅ</th><th>x‚ÇÇ</th><th>Class</th></tr>
        </thead>
        <tbody>
          <tr><td>A</td><td>0</td><td>0</td><td>0</td></tr>
          <tr><td>B</td><td>0</td><td>1</td><td>1</td></tr>
          <tr><td>C</td><td>1</td><td>0</td><td>1</td></tr>
          <tr><td>D</td><td>1</td><td>1</td><td>0</td></tr>
        </tbody>
      </table>

      <p><strong>Linear SVM fails:</strong> No straight line can separate these points.</p>
      
      <p><strong>RBF Kernel to the rescue:</strong> With RBF kernel, SVM can solve this by creating "bumps".</p>
      
      <div style="background: var(--panel2); padding: 20px; border-radius: var(--radius); margin: 20px 0;">
        <p><strong>How RBF solves XOR:</strong></p>
        <ul>
          <li>Place Gaussian bumps at each support vector</li>
          <li>Points near (0,0) and (1,1) have low activation from opposite class bumps</li>
          <li>Points near (0,1) and (1,0) have high activation from their class bumps</li>
          <li>The decision boundary becomes non-linear and can separate the classes</li>
        </ul>
      </div>
      
      <div style="text-align: center;">
        <svg width="300" height="200" viewBox="0 0 300 200">
          <!-- XOR pattern visualization -->
          <circle cx="75" cy="75" r="15" fill="var(--primary)" stroke="var(--text)" stroke-width="2"/>
          <text x="70" y="80" fill="var(--text)">0</text>
          <circle cx="225" cy="125" r="15" fill="var(--primary)" stroke="var(--text)" stroke-width="2"/>
          <text x="220" y="130" fill="var(--text)">0</text>
          <circle cx="75" cy="125" r="15" fill="var(--danger)" stroke="var(--text)" stroke-width="2"/>
          <text x="70" y="130" fill="var(--text)">1</text>
          <circle cx="225" cy="75" r="15" fill="var(--danger)" stroke="var(--text)" stroke-width="2"/>
          <text x="220" y="80" fill="var(--text)">1</text>
          
          <!-- Decision boundary (curved line) -->
          <path d="M 120 50 Q 150 100, 180 150" stroke="var(--success)" stroke-width="3" fill="none" stroke-dasharray="5,5"/>
        </svg>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-graduation-cap"></i> Quick Teaching Summary</h3>

    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
      <div style="background: linear-gradient(135deg, rgba(96, 165, 250, 0.1), rgba(167, 139, 250, 0.1)); padding: 20px; border-radius: var(--radius);">
        <h4 style="margin-top: 0;">SVM Algorithm</h4>
        <ol style="margin-left: 20px;">
          <li>Find hyperplane that maximizes margin</li>
          <li>Identify support vectors (critical points)</li>
          <li>Apply kernel trick for non-linearity</li>
          <li>Use soft margin (C) for overlapping data</li>
          <li>Predict based on distance to hyperplane</li>
        </ol>
      </div>
      
      <div style="background: linear-gradient(135deg, rgba(96, 165, 250, 0.1), rgba(167, 139, 250, 0.1)); padding: 20px; border-radius: var(--radius);">
        <h4 style="margin-top: 0;">Key Parameters</h4>
        <ul style="margin-left: 20px;">
          <li><strong>C:</strong> Regularization (small = softer margin)</li>
          <li><strong>Kernel:</strong> Linear, RBF, Polynomial</li>
          <li><strong>Gamma:</strong> RBF spread (small = smoother)</li>
          <li><strong>Degree:</strong> For polynomial kernel</li>
        </ul>
      </div>
    </div>

    <div style="margin-top: 30px; text-align: center; padding: 20px; background: var(--panel2); border-radius: var(--radius);">
      <p style="font-size: 1.2rem;">
        <i class="fas fa-lightbulb" style="color: var(--warning);"></i> 
        <strong>Remember:</strong> SVM finds the <span class="highlight">widest possible street</span> between classes.
        The only points that matter are the <span class="highlight">support vectors</span> at the edges!
      </p>
    </div>
  </div>

  <!-- Code Editor (Enhanced) -->
  <div class="code-editor-container">
    <div class="code-header">
      <div class="code-title"><i class="fab fa-python"></i> SVM in Python (with Best Practices)</div>
    </div>
    <div class="code-body">
      <pre class="code-block"><code><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC, LinearSVC
<span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler
<span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, GridSearchCV, cross_val_score
<span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report, confusion_matrix
<span class="keyword">import</span> numpy <span class="keyword">as</span> np
<span class="keyword">import</span> pandas <span class="keyword">as</span> pd

<span class="comment"># ============================================</span>
<span class="comment"># STEP 1: ALWAYS scale features for SVM!</span>
<span class="comment"># ============================================</span>
scaler = StandardScaler()
X_scaled = scaler.<span class="function">fit_transform</span>(X)

<span class="comment"># Split data</span>
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.3, random_state=42, stratify=y
)

<span class="comment"># ============================================</span>
<span class="comment"># STEP 2: Try different kernels</span>
<span class="comment"># ============================================</span>

<span class="comment"># Linear SVM (fast, good for high-dimensional data)</span>
linear_svm = SVC(kernel='linear', C=1.0, random_state=42)
linear_svm.<span class="function">fit</span>(X_train, y_train)
<span class="keyword">print</span>(<span class="string">f"Linear SVM accuracy: {linear_svm.score(X_test, y_test):.3f}"</span>)

<span class="comment"># RBF SVM (most common, handles non-linearity)</span>
rbf_svm = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)
rbf_svm.<span class="function">fit</span>(X_train, y_train)
<span class="keyword">print</span>(<span class="string">f"RBF SVM accuracy: {rbf_svm.score(X_test, y_test):.3f}"</span>)

<span class="comment"># Polynomial SVM</span>
poly_svm = SVC(kernel='poly', degree=3, C=1.0, gamma='scale', random_state=42)
poly_svm.<span class="function">fit</span>(X_train, y_train)
<span class="keyword">print</span>(<span class="string">f"Polynomial SVM accuracy: {poly_svm.score(X_test, y_test):.3f}"</span>)

<span class="comment"># ============================================</span>
<span class="comment"># STEP 3: Hyperparameter tuning with Grid Search</span>
<span class="comment"># ============================================</span>

<span class="comment"># Define parameter grid</span>
param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': [0.001, 0.01, 0.1, 1, 'scale', 'auto'],
    'kernel': ['rbf']  <span class="comment"># Start with RBF, then try others</span>
}

<span class="comment"># Create grid search</span>
grid_search = GridSearchCV(
    SVC(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

grid_search.<span class="function">fit</span>(X_train, y_train)

<span class="keyword">print</span>(<span class="string">"\nBest parameters:"</span>, grid_search.best_params_)
<span class="keyword">print</span>(<span class="string">f"Best cross-validation score: {grid_search.best_score_:.3f}"</span>)
<span class="keyword">print</span>(<span class="string">f"Test accuracy: {grid_search.score(X_test, y_test):.3f}"</span>)

<span class="comment"># ============================================</span>
<span class="comment"># STEP 4: Analyze the best model</span>
<span class="comment"># ============================================</span>
best_svm = grid_search.best_estimator_

<span class="comment"># Support vectors</span>
<span class="keyword">print</span>(<span class="string">f"\nNumber of support vectors: {len(best_svm.support_vectors_)}"</span>)
<span class="keyword">print</span>(<span class="string">f"Support vector indices: {best_svm.support_}"</span>)
<span class="keyword">print</span>(<span class="string">f"Support vector classes: {best_svm.n_support_}"</span>)

<span class="comment"># For linear kernel, we can get coefficients</span>
<span class="keyword">if</span> best_svm.kernel == 'linear':
    <span class="keyword">print</span>(<span class="string">f"Weights: {best_svm.coef_[0]}"</span>)
    <span class="keyword">print</span>(<span class="string">f"Intercept: {best_svm.intercept_[0]}"</span>)

<span class="comment"># Predictions and probabilities (if probability=True)</span>
y_pred = best_svm.<span class="function">predict</span>(X_test)
<span class="keyword">if</span> hasattr(best_svm, 'predict_proba'):
    y_proba = best_svm.<span class="function">predict_proba</span>(X_test)
    <span class="keyword">print</span>(<span class="string">f"\nSample probabilities:\n{y_proba[:5]}"</span>)

<span class="keyword">print</span>(<span class="string">"\nClassification Report:"</span>)
<span class="keyword">print</span>(classification_report(y_test, y_pred))

<span class="comment"># ============================================</span>
<span class="comment"># STEP 5: LinearSVC for large datasets</span>
<span class="comment"># ============================================</span>
<span class="comment"># LinearSVC is faster for linear problems with many samples</span>
<span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC

linear_svc = LinearSVC(
    C=1.0,
    penalty='l2',
    loss='squared_hinge',
    dual=False,  <span class="comment"># Set to False for n_samples > n_features</span>
    random_state=42,
    max_iter=10000
)

linear_svc.<span class="function">fit</span>(X_train, y_train)
<span class="keyword">print</span>(<span class="string">f"\nLinearSVC accuracy: {linear_svc.score(X_test, y_test):.3f}"</span>)

<span class="comment"># ============================================</span>
<span class="comment"># STEP 6: Visualizing SVM boundaries (for 2D data)</span>
<span class="comment"># ============================================</span>
<span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt
<span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap

<span class="keyword">def</span> plot_svm_boundary(X, y, model, title):
    <span class="comment"># Create mesh</span>
    h = 0.02
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    <span class="comment"># Predict on mesh</span>
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    <span class="comment"># Plot</span>
    plt.contourf(xx, yy, Z, alpha=0.3, cmap=ListedColormap(['#FF6B6B', '#4ECDC4']))
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=ListedColormap(['#FF6B6B', '#4ECDC4']), 
                edgecolors='black', linewidth=1)
    
    <span class="comment"># Highlight support vectors</span>
    plt.scatter(model.support_vectors_[:, 0], 
                model.support_vectors_[:, 1],
                s=100, facecolors='none', edgecolors='yellow', 
                linewidths=2, label='Support Vectors')
    
    plt.title(title)
    plt.legend()
    plt.show()

<span class="comment"># Usage (for 2D data):</span>
<span class="comment"># plot_svm_boundary(X_train[:, :2], y_train, best_svm, 'SVM Decision Boundary')</span></code></pre>
    </div>
  </div>

  <!-- Quick Reference Card -->
  <div class="panel section">
    <h3><i class="fas fa-clipboard-list"></i> SVM Cheat Sheet</h3>
    
    <table class="detailed-table">
      <thead>
        <tr><th>Parameter</th><th>What it does</th><th>Typical range</th><th>Effect</th></tr>
      </thead>
      <tbody>
        <tr><td><strong>C</strong></td><td>Regularization</td><td>0.1 - 100</td><td>Small C = softer margin, better generalization</td></tr>
        <tr><td><strong>gamma</strong></td><td>RBF spread</td><td>0.001 - 1</td><td>Small gamma = smoother boundary</td></tr>
        <tr><td><strong>kernel</strong></td><td>Transformation</td><td>linear, rbf, poly</td><td>RBF is good default</td></tr>
        <tr><td><strong>degree</strong></td><td>Polynomial degree</td><td>2-5</td><td>Higher = more complex</td></tr>
        <tr><td><strong>class_weight</strong></td><td>Handle imbalance</td><td>'balanced'</td><td>Adjusts for class frequencies</td></tr>
      </tbody>
    </table>
    
    <div style="margin-top: 20px; padding: 15px; background: linear-gradient(135deg, rgba(96, 165, 250, 0.1), rgba(167, 139, 250, 0.1)); border-radius: var(--radius);">
      <p><strong>Rule of thumb for hyperparameter search:</strong></p>
      <ul>
        <li>Start with RBF kernel</li>
        <li>Try C = [0.1, 1, 10, 100]</li>
        <li>Try gamma = [0.001, 0.01, 0.1, 1] or use 'scale'</li>
        <li>Use GridSearchCV with 5-fold CV</li>
      </ul>
    </div>
  </div>

  <!-- Interactive Demo Controls (preserved from original) -->
  <script>
  function updateSVM() {
      const C = parseFloat(document.getElementById('svmC').value);
      const gamma = parseFloat(document.getElementById('svmGamma').value);
      const kernel = document.getElementById('svmKernel').value;
      
      document.getElementById('svmCValue').textContent = C.toFixed(1);
      document.getElementById('svmGammaValue').textContent = gamma.toFixed(1);
      
      // Update metrics based on parameters
      const nSV = Math.floor(8 + 4 * gamma + 2 / C);
      document.getElementById('svmSV').textContent = nSV;
      
      const margin = (1.5 / C).toFixed(1);
      document.getElementById('svmMargin').textContent = margin;
      
      const accuracy = (0.85 + 0.1 * Math.exp(-gamma/2) - 0.05 * C/10).toFixed(2);
      document.getElementById('svmAcc').textContent = accuracy;
      
      document.getElementById('svmTime').textContent = kernel === 'linear' ? 'O(n¬≤)' : 'O(n¬≥)';
      
      // Regenerate data with current parameters
      generateSVMData();
  }

  function generateSVMData() {
      const C = parseFloat(document.getElementById('svmC').value);
      const gamma = parseFloat(document.getElementById('svmGamma').value);
      const kernel = document.getElementById('svmKernel').value;
      
      // Generate synthetic data
      const class0 = [];
      const class1 = [];
      const supportVectors = [];
      
      // Generate two interleaving crescents (non-linear pattern)
      for (let i = 0; i < 30; i++) {
          const angle = Math.random() * Math.PI;
          const radius = 2 + Math.random() * 2;
          class0.push({
              x: Math.cos(angle) * radius,
              y: Math.sin(angle) * radius
          });
      }
      
      for (let i = 0; i < 30; i++) {
          const angle = Math.random() * Math.PI + Math.PI;
          const radius = 2 + Math.random() * 2;
          class1.push({
              x: Math.cos(angle) * radius,
              y: Math.sin(angle) * radius
          });
      }
      
      // Add some noise based on C (small C = more noise)
      const noiseLevel = 1 / C;
      if (noiseLevel > 0.2) {
          for (let i = 0; i < 5; i++) {
              class0.push({
                  x: (Math.random() - 0.5) * 6,
                  y: (Math.random() - 0.5) * 6
              });
              class1.push({
                  x: (Math.random() - 0.5) * 6,
                  y: (Math.random() - 0.5) * 6
              });
          }
      }
      
      // Generate support vectors along the boundary
      for (let i = 0; i < Math.floor(8 + 4 * gamma); i++) {
          const angle = Math.random() * 2 * Math.PI;
          const radius = 2.5 + (Math.random() - 0.5) * 1.5;
          supportVectors.push({
              x: Math.cos(angle) * radius,
              y: Math.sin(angle) * radius
          });
      }
      
      if (svmChart) {
          svmChart.data.datasets[0].data = class0;
          svmChart.data.datasets[1].data = class1;
          svmChart.data.datasets[2].data = supportVectors;
          svmChart.update();
      }
  }
  </script>
</div>
    <!-- ================================================
         SECTION 6: DECISION TREES FOR CLASSIFICATION
    ================================================= -->
    <!-- ================================================
     SECTION 6: DECISION TREES FOR CLASSIFICATION - ENHANCED
================================================= -->
<div id="tree-classification" class="module-content">

  <div class="panel section">
    <h3><i class="fas fa-tree"></i> 6.1 What Problem Do Decision Trees Solve?</h3>

    <div class="info-box info">
      <h4><i class="fas fa-bullseye"></i> The Core Idea: If-This-Then-That Rules</h4>
      <p>
        <strong>Decision trees</strong> classify by asking a series of yes/no questions about the features.
        Each question splits the data into purer groups until we reach a final prediction.
      </p>
      
      <div style="display: flex; gap: 20px; flex-wrap: wrap; margin-top: 15px; justify-content: center;">
        <span class="badge"><i class="fas fa-check-circle"></i> Ask questions</span>
        <span class="badge"><i class="fas fa-check-circle"></i> Split data</span>
        <span class="badge"><i class="fas fa-check-circle"></i> Maximize purity</span>
        <span class="badge"><i class="fas fa-check-circle"></i> Predict majority class</span>
      </div>
      
      <p style="margin-top: 15px;">
        <strong>Key insight:</strong> The result is a set of interpretable rules that anyone can understand -
        no black box!
      </p>
    </div>

    <div class="explanation-box">
      <p><strong>üìö In Simple Terms:</strong> Decision trees are like playing "20 Questions" - you ask yes/no questions to narrow down possibilities until you're confident enough to guess. "Is it an animal? Does it have fur? Does it bark? Then it's probably a dog!"</p>
      <p><strong>Real-Life Example:</strong> Banks use decision trees for loan approval: "Income > $50k? ‚Üí Yes ‚Üí Credit score > 700? ‚Üí Yes ‚Üí Approved!" Each path through the tree is a clear rule.</p>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-sitemap"></i> 6.2 Anatomy of a Decision Tree</h3>

    <div style="display: grid; grid-template-columns: 2fr 1fr; gap: 20px; margin: 20px 0;">
      <div style="background: var(--panel2); padding: 20px; border-radius: var(--radius);">
        <svg width="100%" height="250" viewBox="0 0 400 250">
          <!-- Root node -->
          <circle cx="200" cy="40" r="25" fill="var(--primary)" opacity="0.8"/>
          <text x="200" y="45" text-anchor="middle" fill="white" font-size="12">Contains</text>
          <text x="200" y="60" text-anchor="middle" fill="white" font-size="12">"win"?</text>
          
          <!-- Branches -->
          <line x1="200" y1="65" x2="120" y2="110" stroke="var(--text)" stroke-width="2"/>
          <line x1="200" y1="65" x2="280" y2="110" stroke="var(--text)" stroke-width="2"/>
          
          <!-- Left child (Yes) -->
          <circle cx="120" cy="135" r="20" fill="var(--warning)" opacity="0.8"/>
          <text x="120" y="135" text-anchor="middle" fill="white" font-size="11">Contains</text>
          <text x="120" y="150" text-anchor="middle" fill="white" font-size="11">"money"?</text>
          
          <!-- Right child (No) -->
          <circle cx="280" cy="135" r="20" fill="var(--success)" opacity="0.8"/>
          <text x="280" y="140" text-anchor="middle" fill="white" font-size="11">Leaf: NOT SPAM</text>
          
          <!-- Left-left leaf -->
          <line x1="120" y1="155" x2="70" y2="200" stroke="var(--text)" stroke-width="2"/>
          <circle cx="70" cy="220" r="18" fill="var(--danger)" opacity="0.8"/>
          <text x="70" y="225" text-anchor="middle" fill="white" font-size="10">SPAM</text>
          
          <!-- Left-right leaf -->
          <line x1="120" y1="155" x2="170" y2="200" stroke="var(--text)" stroke-width="2"/>
          <circle cx="170" cy="220" r="18" fill="var(--success)" opacity="0.8"/>
          <text x="170" y="225" text-anchor="middle" fill="white" font-size="10">NOT SPAM</text>
          
          <!-- Labels -->
          <text x="80" y="90" fill="var(--text)" font-size="10">Yes</text>
          <text x="250" y="90" fill="var(--text)" font-size="10">No</text>
        </svg>
      </div>
      
      <div>
        <h4>Tree Components:</h4>
        <ul>
          <li><strong style="color: var(--primary);">Root node:</strong> First question</li>
          <li><strong style="color: var(--warning);">Internal nodes:</strong> Questions that split data</li>
          <li><strong style="color: var(--success);">Leaf nodes:</strong> Final predictions</li>
          <li><strong>Branches:</strong> Answers (Yes/No)</li>
          <li><strong>Depth:</strong> Longest path from root to leaf</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-chart-bar"></i> 6.3 Measuring Purity: How to Choose Splits</h3>

    <div class="info-box info">
      <h4>The Goal: Create Pure Children</h4>
      <p>
        When we split a node, we want the resulting groups to be as <strong>pure</strong> as possible -
        containing mostly one class. Two common measures:
      </p>
    </div>

    <div class="concept-grid">
      <div class="concept-card">
        <h5>Gini Impurity</h5>
        <div class="math-equation" style="text-align: center; margin: 15px 0;">
          \(Gini = 1 - \sum_{i=1}^{c} p_i^2\)
        </div>
        <p><strong>Interpretation:</strong> Probability of misclassifying a randomly chosen element if you randomly label it according to class distribution.</p>
        <div style="background: var(--panel); padding: 15px; border-radius: 8px; margin-top: 15px;">
          <p><strong>Example:</strong> Node with 80% spam, 20% not spam</p>
          <p>Gini = 1 - (0.8¬≤ + 0.2¬≤) = 1 - (0.64 + 0.04) = <span class="highlight">0.32</span></p>
          <p>Perfectly pure: Gini = 0<br>Completely mixed (50/50): Gini = 0.5</p>
        </div>
      </div>

      <div class="concept-card">
        <h5>Entropy</h5>
        <div class="math-equation" style="text-align: center; margin: 15px 0;">
          \(Entropy = -\sum_{i=1}^{c} p_i \log_2(p_i)\)
        </div>
        <p><strong>Interpretation:</strong> Amount of uncertainty/information (from information theory).</p>
        <div style="background: var(--panel); padding: 15px; border-radius: 8px; margin-top: 15px;">
          <p><strong>Example:</strong> Same node (80% spam, 20% not)</p>
          <p>Entropy = -(0.8√ólog‚ÇÇ0.8 + 0.2√ólog‚ÇÇ0.2) ‚âà <span class="highlight">0.72</span></p>
          <p>Perfectly pure: Entropy = 0<br>Completely mixed: Entropy = 1</p>
        </div>
      </div>
    </div>

    <div class="info-box success" style="margin-top: 20px;">
      <h4><i class="fas fa-arrow-right"></i> Information Gain</h4>
      <p>The reduction in impurity after a split:</p>
      <div class="math-equation" style="text-align: center; margin: 15px 0;">
        \(IG = Impurity_{parent} - \sum \frac{n_{child}}{n_{parent}} \times Impurity_{child}\)
      </div>
      <p>We choose the split that gives the <strong>highest information gain</strong>.</p>
      
      <div style="background: var(--panel2); padding: 15px; border-radius: 8px; margin-top: 15px;">
        <p><strong>Worked Example:</strong></p>
        <p>Parent node: 10 samples (5 spam, 5 not) ‚Üí Gini = 0.5</p>
        <p>Split on "contains win?":</p>
        <ul>
          <li>Left child (Yes): 6 samples (5 spam, 1 not) ‚Üí Gini = 1 - [(5/6)¬≤ + (1/6)¬≤] = 0.278</li>
          <li>Right child (No): 4 samples (0 spam, 4 not) ‚Üí Gini = 0 (pure!)</li>
        </ul>
        <p>Weighted child impurity = (6/10)√ó0.278 + (4/10)√ó0 = 0.167</p>
        <p><strong>Information Gain = 0.5 - 0.167 = 0.333</strong></p>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-sliders-h"></i> 6.4 Interactive Decision Tree Demo</h3>

    <div class="regression-container">
      <div class="classification-chart"><canvas id="treeClassChart"></canvas></div>

      <div class="classification-controls">
        <div class="control-group">
          <h4><i class="fas fa-sliders-h"></i> Tree Controls</h4>
          
          <div class="slider-container">
            <label><i class="fas fa-layer-group"></i> Max Depth:</label>
            <input type="range" id="treeClassDepth" min="1" max="10" step="1" value="3">
            <span class="slider-value" id="treeClassDepthValue">3</span>
          </div>
          <p style="font-size:12px;">
            <span style="color: var(--danger);">Shallow = underfitting</span> | 
            <span style="color: var(--success);">Deep = overfitting</span>
          </p>
          
          <div class="slider-container">
            <label><i class="fas fa-users"></i> Min Samples Leaf:</label>
            <input type="range" id="treeClassMinLeaf" min="1" max="20" step="1" value="5">
            <span class="slider-value" id="treeClassMinLeafValue">5</span>
          </div>
          <p style="font-size:12px;">
            Higher = smoother, less overfitting
          </p>
          
          <div class="slider-container">
            <label><i class="fas fa-chart-bar"></i> Criterion:</label>
            <select id="treeCriterion" class="btn btn-secondary">
              <option value="gini">Gini Impurity</option>
              <option value="entropy">Entropy (Information Gain)</option>
            </select>
          </div>
          
          <div class="slider-container">
            <label><i class="fas fa-balance-scale"></i> Min Samples Split:</label>
            <input type="range" id="treeMinSplit" min="2" max="20" step="1" value="2">
            <span class="slider-value" id="treeMinSplitValue">2</span>
          </div>
        </div>

        <div class="button-group">
          <button class="btn btn-primary" onclick="updateTreeClass()">Update Tree</button>
          <button class="btn btn-secondary" onclick="generateTreeClassData()">New Data</button>
        </div>

        <div class="metrics-container">
          <div class="metric-card">
            <div class="metric-label">Train Accuracy</div>
            <div class="metric-value" id="treeClassTrain">0.92</div>
          </div>
          <div class="metric-card">
            <div class="metric-label">Test Accuracy</div>
            <div class="metric-value" id="treeClassTest">0.88</div>
          </div>
          <div class="metric-card">
            <div class="metric-label">Leaves</div>
            <div class="metric-value" id="treeClassLeaves">8</div>
          </div>
          <div class="metric-card">
            <div class="metric-label">Depth</div>
            <div class="metric-value" id="treeClassActualDepth">3</div>
          </div>
        </div>

        <div class="explanation-box" style="margin-top: 20px;">
          <p><strong>üëÜ Watch the overfitting:</strong></p>
          <ul style="padding-left:20px;">
            <li><strong>Depth=1:</strong> Stump - very simple, underfits</li>
            <li><strong>Depth=3-5:</strong> Balanced - good complexity</li>
            <li><strong>Depth=10:</strong> Deep tree - overfits, wiggly boundary</li>
            <li><strong>Min samples leaf:</strong> Higher values = smoother boundaries</li>
          </ul>
        </div>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-scale-balanced"></i> 6.5 The Bias-Variance Tradeoff in Trees</h3>

    <div class="concept-grid">
      <div class="concept-card">
        <h5 style="color: var(--danger);">Shallow Trees (High Bias)</h5>
        <ul style="padding-left:20px;">
          <li>Only a few splits</li>
          <li>Too simple - misses patterns</li>
          <li>Low training accuracy</li>
          <li>Low test accuracy</li>
          <li><strong>Underfitting</strong></li>
        </ul>
        <div style="background: var(--panel); padding: 10px; border-radius: 8px; margin-top: 10px;">
          <p><em>Analogy: Asking only "Is it big?" to identify all animals</em></p>
        </div>
      </div>

      <div class="concept-card">
        <h5 style="color: var(--success);">Optimal Depth</h5>
        <ul style="padding-left:20px;">
          <li>Captures true patterns</li>
          <li>Good balance</li>
          <li>High training accuracy</li>
          <li>High test accuracy</li>
          <li><strong>Good generalization</strong></li>
        </ul>
        <div style="background: var(--panel); padding: 10px; border-radius: 8px; margin-top: 10px;">
          <p><em>Analogy: Asking relevant questions about size, habitat, diet</em></p>
        </div>
      </div>

      <div class="concept-card">
        <h5 style="color: var(--danger);">Deep Trees (High Variance)</h5>
        <ul style="padding-left:20px;">
          <li>Many splits, deep branches</li>
          <li>Too complex - fits noise</li>
          <li>Near 100% training accuracy</li>
          <li>Poor test accuracy</li>
          <li><strong>Overfitting</strong></li>
        </ul>
        <div style="background: var(--panel); padding: 10px; border-radius: 8px; margin-top: 10px;">
          <p><em>Analogy: Memorizing individual animals instead of learning species</em></p>
        </div>
      </div>
    </div>

    <div class="pro-tip">
      <p><strong>üí° Finding the sweet spot:</strong> Use cross-validation to find optimal depth. Plot training vs validation error - choose depth where validation error is minimized before it starts increasing again.</p>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-cut"></i> 6.6 Pruning: Fighting Overfitting</h3>

    <div class="info-box info">
      <h4>Two Types of Pruning</h4>
      
      <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-top: 20px;">
        <div style="background: var(--panel2); padding: 15px; border-radius: var(--radius);">
          <h5>Pre-pruning (Early Stopping)</h5>
          <p>Stop growing the tree before it becomes too complex:</p>
          <ul>
            <li><strong>max_depth:</strong> Limit tree depth</li>
            <li><strong>min_samples_split:</strong> Minimum samples to split a node</li>
            <li><strong>min_samples_leaf:</strong> Minimum samples in a leaf</li>
            <li><strong>max_features:</strong> Limit features considered for split</li>
          </ul>
        </div>
        
        <div style="background: var(--panel2); padding: 15px; border-radius: var(--radius);">
          <h5>Post-pruning (Cost Complexity)</h5>
          <p>Grow full tree, then cut back branches:</p>
          <ul>
            <li>Calculate impurity reduction for each split</li>
            <li>Prune branches that don't improve accuracy enough</li>
            <li>Use <strong>ccp_alpha</strong> parameter in sklearn</li>
            <li>Trade-off: complexity vs accuracy</li>
          </ul>
        </div>
      </div>
    </div>

    <div class="expanded-content">
      <h4>Cost Complexity Pruning (CCP) Example</h4>
      
      <table class="detailed-table">
        <thead>
          <tr><th>ccp_alpha</th><th>Tree Size</th><th>Train Accuracy</th><th>Test Accuracy</th><th>Interpretation</th></tr>
        </thead>
        <tbody>
          <tr><td>0.0</td><td>Full tree (15 leaves)</td><td>1.00</td><td>0.82</td><td>Overfitting</td></tr>
          <tr><td>0.01</td><td>Pruned (8 leaves)</td><td>0.95</td><td>0.88</td><td>Good balance</td></tr>
          <tr><td>0.05</td><td>Small (4 leaves)</td><td>0.88</td><td>0.85</td><td>Underfitting</td></tr>
          <tr><td>0.1</td><td>Stump (1 leaf)</td><td>0.75</td><td>0.74</td><td>Too simple</td></tr>
        </tbody>
      </table>
      
      <p>Choose ccp_alpha that maximizes validation accuracy!</p>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-star"></i> 6.7 Feature Importance</h3>

    <div class="info-box info">
      <h4>Which Features Matter Most?</h4>
      <p>
        Decision trees provide natural <strong>feature importance</strong> scores:
      </p>
      
      <div style="text-align: center; margin: 25px 0;">
        <span class="math-equation">\(Importance(f) = \sum_{nodes splitting on f} \frac{n_{node}}{n_{total}} \times \text{impurity reduction}\)</span>
      </div>
      
      <p>
        Features used near the top of the tree and in many splits get higher importance.
      </p>
    </div>

    <div style="background: var(--panel2); padding: 20px; border-radius: var(--radius); margin: 20px 0;">
      <h4>Example: Email Classification</h4>
      
      <div style="display: flex; align-items: center; gap: 20px;">
        <div style="flex: 1;">
          <div style="margin: 10px 0;">
            <div style="display: flex; justify-content: space-between;">
              <span>Contains "win"</span>
              <span>0.45</span>
            </div>
            <div style="height: 20px; background: var(--border); border-radius: 10px;">
              <div style="width: 45%; height: 100%; background: var(--primary); border-radius: 10px;"></div>
            </div>
          </div>
          
          <div style="margin: 10px 0;">
            <div style="display: flex; justify-content: space-between;">
              <span>Contains "money"</span>
              <span>0.30</span>
            </div>
            <div style="height: 20px; background: var(--border); border-radius: 10px;">
              <div style="width: 30%; height: 100%; background: var(--primary); border-radius: 10px;"></div>
            </div>
          </div>
          
          <div style="margin: 10px 0;">
            <div style="display: flex; justify-content: space-between;">
              <span>Has link</span>
              <span>0.15</span>
            </div>
            <div style="height: 20px; background: var(--border); border-radius: 10px;">
              <div style="width: 15%; height: 100%; background: var(--primary); border-radius: 10px;"></div>
            </div>
          </div>
          
          <div style="margin: 10px 0;">
            <div style="display: flex; justify-content: space-between;">
              <span>Length</span>
              <span>0.10</span>
            </div>
            <div style="height: 20px; background: var(--border); border-radius: 10px;">
              <div style="width: 10%; height: 100%; background: var(--primary); border-radius: 10px;"></div>
            </div>
          </div>
        </div>
        
        <div style="flex: 1;">
          <p><strong>Interpretation:</strong> "win" is the most predictive feature (45% of importance), followed by "money". Length barely matters for this classifier.</p>
        </div>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-check-double"></i> 6.8 Advantages and Disadvantages</h3>

    <div class="concept-grid">
      <div class="concept-card">
        <h5 style="color: var(--success);">‚úÖ Advantages</h5>
        <ul style="padding-left:20px;">
          <li><strong>Highly interpretable</strong> - can be visualized and explained</li>
          <li><strong>No feature scaling needed</strong> - handles mixed data types</li>
          <li><strong>Handles non-linearity</strong> - naturally captures interactions</li>
          <li><strong>Feature selection built-in</strong> - ignores irrelevant features</li>
          <li><strong>Handles missing values</strong> - can use surrogate splits</li>
          <li><strong>Robust to outliers</strong> - splits based on order, not values</li>
          <li><strong>Mirrors human decision-making</strong> - easy to explain to stakeholders</li>
        </ul>
      </div>
      
      <div class="concept-card">
        <h5 style="color: var(--danger);">‚ùå Disadvantages</h5>
        <ul style="padding-left:20px;">
          <li><strong>High variance</strong> - small data changes can create very different trees</li>
          <li><strong>Overfitting prone</strong> - can grow until 100% training accuracy</li>
          <li><strong>Greedy algorithm</strong> - may not find optimal tree (NP-hard problem)</li>
          <li><strong>Instability</strong> - different training/test splits give different trees</li>
          <li><strong>Axis-aligned splits</strong> - can't create diagonal boundaries easily</li>
          <li><strong>Bias toward features with many levels</strong> - categorical with many categories</li>
        </ul>
      </div>
    </div>

    <div class="common-pitfall">
      <p><strong>‚ö†Ô∏è The instability problem:</strong> Change a few data points and the entire tree structure can change! This is why ensemble methods (Random Forest) are often preferred.</p>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-calculator"></i> 6.9 Worked Example: Building a Tree Step by Step</h3>

    <div class="expanded-content">
      <h4>Tiny Dataset: Should We Play Tennis?</h4>
      
      <table class="detailed-table">
        <thead>
          <tr><th>Day</th><th>Outlook</th><th>Temperature</th><th>Humidity</th><th>Wind</th><th>Play?</th></tr>
        </thead>
        <tbody>
          <tr><td>1</td><td>Sunny</td><td>Hot</td><td>High</td><td>Weak</td><td>No</td></tr>
          <tr><td>2</td><td>Sunny</td><td>Hot</td><td>High</td><td>Strong</td><td>No</td></tr>
          <tr><td>3</td><td>Overcast</td><td>Hot</td><td>High</td><td>Weak</td><td>Yes</td></tr>
          <tr><td>4</td><td>Rainy</td><td>Mild</td><td>High</td><td>Weak</td><td>Yes</td></tr>
          <tr><td>5</td><td>Rainy</td><td>Cool</td><td>Normal</td><td>Weak</td><td>Yes</td></tr>
          <tr><td>6</td><td>Rainy</td><td>Cool</td><td>Normal</td><td>Strong</td><td>No</td></tr>
          <tr><td>7</td><td>Overcast</td><td>Cool</td><td>Normal</td><td>Strong</td><td>Yes</td></tr>
          <tr><td>8</td><td>Sunny</td><td>Mild</td><td>High</td><td>Weak</td><td>No</td></tr>
          <tr><td>9</td><td>Sunny</td><td>Cool</td><td>Normal</td><td>Weak</td><td>Yes</td></tr>
          <tr><td>10</td><td>Rainy</td><td>Mild</td><td>Normal</td><td>Weak</td><td>Yes</td></tr>
          <tr><td>11</td><td>Sunny</td><td>Mild</td><td>Normal</td><td>Strong</td><td>Yes</td></tr>
          <tr><td>12</td><td>Overcast</td><td>Mild</td><td>High</td><td>Strong</td><td>Yes</td></tr>
          <tr><td>13</td><td>Overcast</td><td>Hot</td><td>Normal</td><td>Weak</td><td>Yes</td></tr>
          <tr><td>14</td><td>Rainy</td><td>Mild</td><td>High</td><td>Strong</td><td>No</td></tr>
        </tbody>
      </table>

      <p><strong>Step 1: Calculate parent impurity</strong></p>
      <p>9 Yes, 5 No ‚Üí Gini = 1 - [(9/14)¬≤ + (5/14)¬≤] = 1 - (0.413 + 0.128) = 0.459</p>

      <p><strong>Step 2: Try splits on each feature</strong></p>
      
      <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 15px; margin: 20px 0;">
        <div style="background: var(--panel2); padding: 15px; border-radius: 8px;">
          <p><strong>Split on Outlook:</strong></p>
          <ul>
            <li>Sunny: 2 Yes, 3 No ‚Üí Gini = 0.48</li>
            <li>Overcast: 4 Yes, 0 No ‚Üí Gini = 0</li>
            <li>Rainy: 3 Yes, 2 No ‚Üí Gini = 0.48</li>
          </ul>
          <p>Weighted Gini = (5/14)√ó0.48 + (4/14)√ó0 + (5/14)√ó0.48 = 0.343</p>
          <p><strong>Info Gain = 0.459 - 0.343 = 0.116</strong></p>
        </div>
        
        <div style="background: var(--panel2); padding: 15px; border-radius: 8px;">
          <p><strong>Split on Humidity:</strong></p>
          <ul>
            <li>High: 3 Yes, 4 No ‚Üí Gini = 0.49</li>
            <li>Normal: 6 Yes, 1 No ‚Üí Gini = 0.24</li>
          </ul>
          <p>Weighted Gini = (7/14)√ó0.49 + (7/14)√ó0.24 = 0.365</p>
          <p><strong>Info Gain = 0.459 - 0.365 = 0.094</strong></p>
        </div>
      </div>

      <p><strong>Step 3: Choose best split (Outlook has highest gain)</strong></p>
      
      <div style="background: var(--panel2); padding: 20px; border-radius: var(--radius); margin: 20px 0;">
        <p><strong>Resulting tree:</strong></p>
        <pre style="font-family: monospace; color: var(--text);">
Outlook?
‚îú‚îÄ‚îÄ Sunny ‚Üí Check Humidity
‚îÇ   ‚îú‚îÄ‚îÄ High ‚Üí No (3/0)
‚îÇ   ‚îî‚îÄ‚îÄ Normal ‚Üí Yes (1/2)
‚îú‚îÄ‚îÄ Overcast ‚Üí Yes (4/0)
‚îî‚îÄ‚îÄ Rainy ‚Üí Check Wind
    ‚îú‚îÄ‚îÄ Weak ‚Üí Yes (3/0)
    ‚îî‚îÄ‚îÄ Strong ‚Üí No (0/2)
        </pre>
      </div>

      <p>This tree perfectly classifies the training data! But would it generalize to new days?</p>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-graduation-cap"></i> Quick Teaching Summary</h3>

    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
      <div style="background: linear-gradient(135deg, rgba(96, 165, 250, 0.1), rgba(167, 139, 250, 0.1)); padding: 20px; border-radius: var(--radius);">
        <h4 style="margin-top: 0;">Decision Tree Algorithm</h4>
        <ol style="margin-left: 20px;">
          <li>Start with all data at root</li>
          <li>Find best split (maximize information gain)</li>
          <li>Split data into child nodes</li>
          <li>Repeat recursively for each child</li>
          <li>Stop when: pure, max_depth, or min_samples</li>
          <li>Prune if needed to avoid overfitting</li>
        </ol>
      </div>
      
      <div style="background: linear-gradient(135deg, rgba(96, 165, 250, 0.1), rgba(167, 139, 250, 0.1)); padding: 20px; border-radius: var(--radius);">
        <h4 style="margin-top: 0;">Key Parameters</h4>
        <ul style="margin-left: 20px;">
          <li><strong>max_depth:</strong> Tree depth limit</li>
          <li><strong>min_samples_split:</strong> Min to split node</li>
          <li><strong>min_samples_leaf:</strong> Min in leaf</li>
          <li><strong>criterion:</strong> gini or entropy</li>
          <li><strong>ccp_alpha:</strong> Pruning parameter</li>
        </ul>
      </div>
    </div>

    <div style="margin-top: 30px; text-align: center; padding: 20px; background: var(--panel2); border-radius: var(--radius);">
      <p style="font-size: 1.2rem;">
        <i class="fas fa-lightbulb" style="color: var(--warning);"></i> 
        <strong>Remember:</strong> Decision trees are <span class="highlight">interpretable but unstable</span>. 
        Use pruning to prevent overfitting, and consider Random Forest for better stability!
      </p>
    </div>
  </div>

  <!-- Code Editor (Enhanced) -->
  <div class="code-editor-container">
    <div class="code-header">
      <div class="code-title"><i class="fab fa-python"></i> Decision Trees in Python (with Best Practices)</div>
    </div>
    <div class="code-body">
      <pre class="code-block"><code><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier, plot_tree
<span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, GridSearchCV, cross_val_score
<span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report, confusion_matrix
<span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder
<span class="keyword">import</span> numpy <span class="keyword">as</span> np
<span class="keyword">import</span> pandas <span class="keyword">as</span> pd
<span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt

<span class="comment"># ============================================</span>
<span class="comment"># STEP 1: Prepare data (trees handle mixed types well!)</span>
<span class="comment"># ============================================</span>
<span class="comment"># Note: For categorical features, either encode them or use pandas category dtype</span>
<span class="comment"># sklearn's tree can handle numeric features only, so encode categories</span>

<span class="comment"># Example with the tennis dataset</span>
tennis_data = pd.DataFrame({
    'outlook': ['sunny', 'sunny', 'overcast', 'rainy', 'rainy', 'rainy', 'overcast',
                'sunny', 'sunny', 'rainy', 'sunny', 'overcast', 'overcast', 'rainy'],
    'temp': ['hot', 'hot', 'hot', 'mild', 'cool', 'cool', 'cool',
             'mild', 'cool', 'mild', 'mild', 'mild', 'hot', 'mild'],
    'humidity': ['high', 'high', 'high', 'high', 'normal', 'normal', 'normal',
                 'high', 'normal', 'normal', 'normal', 'high', 'normal', 'high'],
    'wind': ['weak', 'strong', 'weak', 'weak', 'weak', 'strong', 'strong',
             'weak', 'weak', 'weak', 'strong', 'strong', 'weak', 'strong'],
    'play': ['no', 'no', 'yes', 'yes', 'yes', 'no', 'yes',
             'no', 'yes', 'yes', 'yes', 'yes', 'yes', 'no']
})

<span class="comment"># Encode categorical features</span>
le_dict = {}
X_encoded = pd.DataFrame()
<span class="keyword">for</span> col <span class="keyword">in</span> ['outlook', 'temp', 'humidity', 'wind']:
    le = LabelEncoder()
    X_encoded[col] = le.fit_transform(tennis_data[col])
    le_dict[col] = le

y = (tennis_data['play'] == 'yes').astype(int)

X_train, X_test, y_train, y_test = train_test_split(
    X_encoded, y, test_size=0.3, random_state=42, stratify=y
)

<span class="comment"># ============================================</span>
<span class="comment"># STEP 2: Basic Decision Tree</span>
<span class="comment"># ============================================</span>
dt = DecisionTreeClassifier(
    criterion='gini',
    max_depth=3,
    min_samples_split=2,
    min_samples_leaf=1,
    random_state=42
)

dt.<span class="function">fit</span>(X_train, y_train)

<span class="keyword">print</span>(<span class="string">"Basic Decision Tree:"</span>)
<span class="keyword">print</span>(<span class="string">f"Train accuracy: {dt.score(X_train, y_train):.3f}"</span>)
<span class="keyword">print</span>(<span class="string">f"Test accuracy: {dt.score(X_test, y_test):.3f}"</span>)
<span class="keyword">print</span>(<span class="string">f"Number of leaves: {dt.get_n_leaves()}"</span>)
<span class="keyword">print</span>(<span class="string">f"Tree depth: {dt.get_depth()}"</span>)

<span class="comment"># ============================================</span>
<span class="comment"># STEP 3: Visualize the tree</span>
<span class="comment"># ============================================</span>
plt.figure(figsize=(20,10))
plot_tree(dt, 
          feature_names=['outlook', 'temp', 'humidity', 'wind'],
          class_names=['No', 'Yes'],
          filled=True, 
          rounded=True,
          fontsize=10)
plt.title("Decision Tree for Tennis Classification")
plt.show()

<span class="comment"># ============================================</span>
<span class="comment"># STEP 4: Feature Importance</span>
<span class="comment"># ============================================</span>
feature_importance = pd.DataFrame({
    'feature': ['outlook', 'temp', 'humidity', 'wind'],
    'importance': dt.feature_importances_
}).sort_values('importance', ascending=False)

<span class="keyword">print</span>(<span class="string">"\nFeature Importance:"</span>)
<span class="keyword">print</span>(feature_importance)

<span class="comment"># ============================================</span>
<span class="comment"># STEP 5: Hyperparameter Tuning with GridSearch</span>
<span class="comment"># ============================================</span>
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [2, 3, 4, 5, 6, 8, 10, None],
    'min_samples_split': [2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 5, 10],
    'max_features': [None, 'sqrt', 'log2']
}

grid_search = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

grid_search.<span class="function">fit</span>(X_train, y_train)

<span class="keyword">print</span>(<span class="string">"\nBest parameters:"</span>, grid_search.best_params_)
<span class="keyword">print</span>(<span class="string">f"Best cross-validation score: {grid_search.best_score_:.3f}"</span>)
<span class="keyword">print</span>(<span class="string">f"Test accuracy: {grid_search.score(X_test, y_test):.3f}"</span>)

<span class="comment"># ============================================</span>
<span class="comment"># STEP 6: Cost Complexity Pruning</span>
<span class="comment"># ============================================</span>
<span class="comment"># Get cost complexity path</span>
path = dt.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas = path.ccp_alphas

<span class="comment"># Train trees with different alphas</span>
trees = []
<span class="keyword">for</span> ccp_alpha <span class="keyword">in</span> ccp_alphas:
    dt_pruned = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)
    dt_pruned.<span class="function">fit</span>(X_train, y_train)
    trees.append(dt_pruned)

<span class="comment"># Plot training vs test accuracy</span>
train_scores = [tree.score(X_train, y_train) <span class="keyword">for</span> tree <span class="keyword">in</span> trees]
test_scores = [tree.score(X_test, y_test) <span class="keyword">for</span> tree <span class="keyword">in</span> trees]

plt.figure(figsize=(10,6))
plt.plot(ccp_alphas, train_scores, marker='o', label='train', drawstyle="steps-post")
plt.plot(ccp_alphas, test_scores, marker='o', label='test', drawstyle="steps-post")
plt.xlabel('ccp_alpha')
plt.ylabel('accuracy')
plt.legend()
plt.title('Accuracy vs ccp_alpha (pruning parameter)')
plt.grid(True)
plt.show()

<span class="comment"># ============================================</span>
<span class="comment"># STEP 7: Export Rules for Interpretation</span>
<span class="comment"># ============================================</span>
<span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> export_text

tree_rules = export_text(dt, feature_names=['outlook', 'temp', 'humidity', 'wind'])
<span class="keyword">print</span>(<span class="string">"\nDecision Rules:"</span>)
<span class="keyword">print</span>(tree_rules)

<span class="comment"># ============================================</span>
<span class="comment"># STEP 8: Predict probabilities</span>
<span class="comment"># ============================================</span>
y_pred = dt.<span class="function">predict</span>(X_test)
y_proba = dt.<span class="function">predict_proba</span>(X_test)

<span class="keyword">print</span>(<span class="string">"\nSample predictions (first 5):"</span>)
<span class="keyword">for</span> i <span class="keyword">in</span> range(min(5, len(y_pred))):
    <span class="keyword">print</span>(<span class="string">f"  True: {y_test.iloc[i]}, Pred: {y_pred[i]}, Prob: {y_proba[i]}"</span>)

<span class="keyword">print</span>(<span class="string">"\nClassification Report:"</span>)
<span class="keyword">print</span>(classification_report(y_test, y_pred, target_names=['No', 'Yes']))

<span class="comment"># ============================================</span>
<span class="comment"># STEP 9: Handling Imbalanced Data</span>
<span class="comment"># ============================================</span>
dt_balanced = DecisionTreeClassifier(
    class_weight='balanced',  <span class="comment"># Adjust weights inversely proportional to class frequencies</span>
    random_state=42
)

dt_balanced.<span class="function">fit</span>(X_train, y_train)
<span class="keyword">print</span>(<span class="string">f"\nBalanced tree test accuracy: {dt_balanced.score(X_test, y_test):.3f}"</span>)</code></pre>
    </div>
  </div>

  <!-- Quick Reference Card -->
  <div class="panel section">
    <h3><i class="fas fa-clipboard-list"></i> Decision Trees Cheat Sheet</h3>
    
    <table class="detailed-table">
      <thead>
        <tr><th>Parameter</th><th>What it does</th><th>Typical range</th><th>Effect</th></tr>
      </thead>
      <tbody>
        <tr><td><strong>criterion</strong></td><td>Split quality measure</td><td>'gini', 'entropy'</td><td>Similar results, gini faster</td></tr>
        <tr><td><strong>max_depth</strong></td><td>Maximum tree depth</td><td>3-10</td><td>Higher = more complex</td></tr>
        <tr><td><strong>min_samples_split</strong></td><td>Min samples to split</td><td>2-20</td><td>Higher = smoother</td></tr>
        <tr><td><strong>min_samples_leaf</strong></td><td>Min samples in leaf</td><td>1-10</td><td>Higher = less overfitting</td></tr>
        <tr><td><strong>max_features</strong></td><td>Features per split</td><td>'sqrt', 'log2'</td><td>Random subspace</td></tr>
        <tr><td><strong>ccp_alpha</strong></td><td>Pruning parameter</td><td>0-0.1</td><td>Higher = more pruning</td></tr>
        <tr><td><strong>class_weight</strong></td><td>Handle imbalance</td><td>'balanced'</td><td>Adjusts for class frequencies</td></tr>
      </tbody>
    </table>
    
    <div style="margin-top: 20px; padding: 15px; background: linear-gradient(135deg, rgba(96, 165, 250, 0.1), rgba(167, 139, 250, 0.1)); border-radius: var(--radius);">
      <p><strong>Rule of thumb for tree tuning:</strong></p>
      <ul>
        <li>Start with default parameters</li>
        <li>Use cross-validation to find optimal max_depth (usually 3-8)</li>
        <li>Increase min_samples_leaf if overfitting</li>
        <li>Try both gini and entropy (usually similar)</li>
        <li>Use pruning (ccp_alpha) for final model</li>
      </ul>
    </div>
  </div>

  <script>
  // Interactive Tree Demo Functions
  function updateTreeClass() {
      const depth = parseInt(document.getElementById('treeClassDepth').value);
      const minLeaf = parseInt(document.getElementById('treeClassMinLeaf').value);
      const minSplit = parseInt(document.getElementById('treeMinSplit').value);
      const criterion = document.getElementById('treeCriterion').value;
      
      document.getElementById('treeClassDepthValue').textContent = depth;
      document.getElementById('treeClassMinLeafValue').textContent = minLeaf;
      document.getElementById('treeMinSplitValue').textContent = minSplit;
      
      // Calculate metrics based on parameters
      const leaves = Math.min(Math.pow(2, depth), 20);
      document.getElementById('treeClassLeaves').textContent = leaves;
      document.getElementById('treeClassActualDepth').textContent = depth;
      
      // Train accuracy increases with depth, test accuracy peaks then drops
      const trainAcc = (0.7 + 0.03 * depth - 0.001 * minLeaf).toFixed(2);
      const testAcc = (0.7 + 0.04 * depth - 0.002 * Math.pow(depth, 2) - 0.001 * minLeaf).toFixed(2);
      
      document.getElementById('treeClassTrain').textContent = trainAcc;
      document.getElementById('treeClassTest').textContent = testAcc;
      
      // Regenerate data with current parameters
      generateTreeClassData();
  }

  function generateTreeClassData() {
      const depth = parseInt(document.getElementById('treeClassDepth').value);
      const minLeaf = parseInt(document.getElementById('treeClassMinLeaf').value);
      
      // Generate synthetic data with pattern
      const class0 = [];
      const class1 = [];
      
      for (let i = 0; i < 50; i++) {
          const x = Math.random() * 10;
          const y = Math.random() * 10;
          
          // Create a pattern: alternating squares
          const pattern = (Math.floor(x * depth/5) % 2) ^ (Math.floor(y * depth/5) % 2);
          
          if (pattern === 0) {
              class0.push({ x, y });
          } else {
              class1.push({ x, y });
          }
      }
      
      // Add some noise based on minLeaf (more noise = harder to split)
      if (minLeaf < 5) {
          // Add some noise points
          for (let i = 0; i < 5; i++) {
              class0.push({
                  x: Math.random() * 10,
                  y: Math.random() * 10
              });
              class1.push({
                  x: Math.random() * 10,
                  y: Math.random() * 10
              });
          }
      }
      
      if (treeClassChart) {
          treeClassChart.data.datasets[0].data = class0;
          treeClassChart.data.datasets[1].data = class1;
          treeClassChart.update();
      }
  }
  </script>
</div>

    <!-- ================================================
         SECTION 7: ENSEMBLE METHODS FOR CLASSIFICATION
    ================================================= -->
    <!-- ================================================
     SECTION 7: ENSEMBLE METHODS FOR CLASSIFICATION - ENHANCED
================================================= -->
<div id="ensemble-class" class="module-content">

  <div class="panel section">
    <h3><i class="fas fa-layer-group"></i> 7.1 What Problem Do Ensembles Solve?</h3>

    <div class="info-box info">
      <h4><i class="fas fa-bullseye"></i> The Core Idea: Wisdom of the Crowd</h4>
      <p>
        <strong>Ensemble methods</strong> combine multiple models to create one powerful model.
        Just as a group of people often makes better decisions than any individual, a group of models 
        often outperforms any single model.
      </p>
      
      <div style="display: flex; gap: 20px; flex-wrap: wrap; margin-top: 15px; justify-content: center;">
        <span class="badge"><i class="fas fa-check-circle"></i> Combine weak learners</span>
        <span class="badge"><i class="fas fa-check-circle"></i> Reduce variance</span>
        <span class="badge"><i class="fas fa-check-circle"></i> Reduce bias</span>
        <span class="badge"><i class="fas fa-check-circle"></i> Improve stability</span>
      </div>
      
      <p style="margin-top: 15px;">
        <strong>Key insight:</strong> Individual models make different mistakes. When combined, 
        their errors cancel out!
      </p>
    </div>

    <div class="explanation-box">
      <p><strong>üìö In Simple Terms:</strong> Instead of asking one expert, ask 100 experts and take a vote. Even if some experts are wrong, the majority is usually right. This is why ensembles are more accurate and stable.</p>
      <p><strong>Real-Life Example:</strong> Netflix Prize winning solution combined over 100 different models. Kaggle competition winners almost always use ensembles of multiple algorithms.</p>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-sitemap"></i> 7.2 Types of Ensemble Methods</h3>

    <div class="algorithm-grid">
      <div class="algorithm-card">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-random"></i></div>
          <div class="algorithm-title">
            <h4>Bagging (Bootstrap Aggregating)</h4>
            <p>Parallel training</p>
          </div>
        </div>
        <div class="algorithm-body">
          <ul>
            <li>Train models independently on different bootstrap samples</li>
            <li>Combine by averaging (regression) or voting (classification)</li>
            <li><strong>Reduces variance</strong> - prevents overfitting</li>
            <li>Example: Random Forest</li>
          </ul>
          <div style="background: var(--panel); padding: 10px; border-radius: 8px; margin-top: 10px;">
            <p><em>"Many trees planted randomly in parallel"</em></p>
          </div>
        </div>
      </div>

      <div class="algorithm-card">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-arrow-up"></i></div>
          <div class="algorithm-title">
            <h4>Boosting</h4>
            <p>Sequential training</p>
          </div>
        </div>
        <div class="algorithm-body">
          <ul>
            <li>Train models sequentially, each focusing on previous mistakes</li>
            <li>Weighted combination of models</li>
            <li><strong>Reduces bias</strong> - improves accuracy</li>
            <li>Examples: AdaBoost, Gradient Boosting, XGBoost</li>
          </ul>
          <div style="background: var(--panel); padding: 10px; border-radius: 8px; margin-top: 10px;">
            <p><em>"A team learning from its mistakes, getting better each round"</em></p>
          </div>
        </div>
      </div>

      <div class="algorithm-card">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-cubes"></i></div>
          <div class="algorithm-title">
            <h4>Stacking</h4>
            <p>Meta-learning</p>
          </div>
        </div>
        <div class="algorithm-body">
          <ul>
            <li>Train diverse models (base learners)</li>
            <li>Train a meta-model on their predictions</li>
            <li>Combines strengths of different algorithms</li>
            <li>Often wins competitions</li>
          </ul>
          <div style="background: var(--panel); padding: 10px; border-radius: 8px; margin-top: 10px;">
            <p><em>"A manager learning when to trust each expert"</em></p>
          </div>
        </div>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-tree"></i> 7.3 Random Forest: Bagging + Random Features</h3>

    <div class="info-box info">
      <h4>How Random Forest Works</h4>
      
      <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;">
        <div>
          <ol style="margin-left:20px;">
            <li><strong>Bootstrap sampling:</strong> Create many datasets by sampling with replacement</li>
            <li><strong>Random feature selection:</strong> At each split, consider only a random subset of features</li>
            <li><strong>Grow deep trees:</strong> No pruning (high variance individually)</li>
            <li><strong>Average/Vote:</strong> Combine all trees' predictions</li>
          </ol>
        </div>
        <div style="background: var(--panel2); padding: 15px; border-radius: var(--radius);">
          <p><strong>Why random features?</strong></p>
          <p>If one feature is very strong, all trees would use it and become correlated. Random features force trees to be different, reducing correlation and improving the ensemble.</p>
        </div>
      </div>
      
      <div style="text-align: center; margin: 20px 0;">
        <svg width="100%" height="200" viewBox="0 0 600 200">
          <!-- Original data -->
          <rect x="50" y="30" width="80" height="80" fill="var(--primary)" opacity="0.3" stroke="var(--text)"/>
          <text x="90" y="70" text-anchor="middle" fill="var(--text)">Data</text>
          
          <!-- Bootstrap samples -->
          <rect x="180" y="10" width="60" height="50" fill="var(--success)" opacity="0.5" stroke="var(--text)"/>
          <rect x="180" y="70" width="60" height="50" fill="var(--success)" opacity="0.5" stroke="var(--text)"/>
          <rect x="180" y="130" width="60" height="50" fill="var(--success)" opacity="0.5" stroke="var(--text)"/>
          <text x="210" y="175" fill="var(--text)">Bootstrap</text>
          
          <!-- Trees -->
          <circle cx="300" cy="35" r="20" fill="var(--warning)" opacity="0.7"/>
          <circle cx="300" cy="100" r="20" fill="var(--warning)" opacity="0.7"/>
          <circle cx="300" cy="165" r="20" fill="var(--warning)" opacity="0.7"/>
          <text x="300" y="190" fill="var(--text)">Trees</text>
          
          <!-- Predictions -->
          <text x="400" y="40" fill="var(--text)">Class A</text>
          <text x="400" y="95" fill="var(--text)">Class A</text>
          <text x="400" y="160" fill="var(--text)">Class B</text>
          
          <!-- Vote -->
          <rect x="480" y="60" width="80" height="80" fill="var(--danger)" opacity="0.3" stroke="var(--text)"/>
          <text x="520" y="110" text-anchor="middle" fill="var(--text)">Vote: A</text>
        </svg>
      </div>
    </div>

    <div class="expanded-content">
      <h4>Key Parameters in Random Forest</h4>
      
      <table class="detailed-table">
        <thead>
          <tr><th>Parameter</th><th>What it does</th><th>Typical range</th><th>Effect</th></tr>
        </thead>
        <tbody>
          <tr><td><strong>n_estimators</strong></td><td>Number of trees</td><td>100-1000</td><td>More is better (diminishing returns)</td></tr>
          <tr><td><strong>max_depth</strong></td><td>Tree depth</td><td>None or 10-50</td><td>Deep trees OK (bagging handles variance)</td></tr>
          <tr><td><strong>max_features</strong></td><td>Features per split</td><td>'sqrt', 'log2'</td><td>Smaller = more diverse trees</td></tr>
          <tr><td><strong>min_samples_leaf</strong></td><td>Min samples in leaf</td><td>1-5</td><td>Small is fine for random forest</td></tr>
          <tr><td><strong>bootstrap</strong></td><td>Use bootstrap?</td><td>True/False</td><td>True = bagging, False = random subspace</td></tr>
        </tbody>
      </table>
      
      <div class="pro-tip" style="margin-top: 15px;">
        <p><strong>üí° Random Forest Magic:</strong> Unlike single trees, random forest rarely overfits with more trees! You can add hundreds of trees and test error keeps decreasing or stabilizes.</p>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-chart-line"></i> 7.4 Boosting: Learning from Mistakes</h3>

    <div class="info-box info">
      <h4>How Boosting Works</h4>
      
      <div style="display: grid; grid-template-columns: 2fr 1fr; gap: 20px;">
        <div>
          <ol style="margin-left:20px;">
            <li><strong>Start:</strong> Train first model on original data</li>
            <li><strong>Weight mistakes:</strong> Increase weight of misclassified samples</li>
            <li><strong>Next model:</strong> Train on weighted data, focusing on hard cases</li>
            <li><strong>Repeat:</strong> Continue for M rounds</li>
            <li><strong>Combine:</strong> Weighted vote based on each model's accuracy</li>
          </ol>
        </div>
        <div style="background: var(--panel2); padding: 15px; border-radius: var(--radius);">
          <p><strong>Analogy:</strong> Students taking an exam. Those who fail get extra tutoring and retake a harder version. Eventually, everyone passes!</p>
        </div>
      </div>
    </div>

    <div class="algorithm-grid">
      <div class="algorithm-card">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-microchip"></i></div>
          <div class="algorithm-title">
            <h4>AdaBoost</h4>
            <p>Adaptive Boosting</p>
          </div>
        </div>
        <div class="algorithm-body">
          <ul>
            <li>First successful boosting algorithm</li>
            <li>Adjusts sample weights after each round</li>
            <li>Models weighted by accuracy</li>
            <li>Sensitive to noisy data</li>
          </ul>
        </div>
      </div>

      <div class="algorithm-card">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-chart-bar"></i></div>
          <div class="algorithm-title">
            <h4>Gradient Boosting</h4>
            <p>Generalized boosting</p>
          </div>
        </div>
        <div class="algorithm-body">
          <ul>
            <li>Each new model fits residuals of previous ensemble</li>
            <li>Gradient descent in function space</li>
            <li>Very flexible, many loss functions</li>
            <li>Base for XGBoost, LightGBM, CatBoost</li>
          </ul>
        </div>
      </div>

      <div class="algorithm-card">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-bolt"></i></div>
          <div class="algorithm-title">
            <h4>XGBoost</h4>
            <p>Extreme Gradient Boosting</p>
          </div>
        </div>
        <div class="algorithm-body">
          <ul>
            <li>Optimized gradient boosting implementation</li>
            <li>Regularization to prevent overfitting</li>
            <li>Handles missing values automatically</li>
            <li>Parallel processing, cache optimization</li>
            <li>Dominates Kaggle competitions</li>
          </ul>
        </div>
      </div>
    </div>

    <div class="expanded-content">
      <h4>XGBoost Key Parameters</h4>
      
      <table class="detailed-table">
        <thead>
          <tr><th>Parameter</th><th>What it does</th><th>Typical range</th><th>Effect</th></tr>
        </thead>
        <tbody>
          <tr><td><strong>learning_rate (eta)</strong></td><td>Shrinkage factor</td><td>0.01-0.3</td><td>Smaller = more robust, needs more trees</td></tr>
          <tr><td><strong>n_estimators</strong></td><td>Number of boosting rounds</td><td>100-1000</td><td>More trees with small learning rate</td></tr>
          <tr><td><strong>max_depth</strong></td><td>Tree depth</td><td>3-10</td><td>Shallow trees prevent overfitting</td></tr>
          <tr><td><strong>subsample</strong></td><td>Row sampling</td><td>0.5-1.0</td><td>Stochastic gradient boosting</td></tr>
          <tr><td><strong>colsample_bytree</strong></td><td>Column sampling</td><td>0.5-1.0</td><td>Like random forest feature selection</td></tr>
          <tr><td><strong>lambda/alpha</strong></td><td>Regularization</td><td>0-10</td><td>L2/L1 regularization on weights</td></tr>
        </tbody>
      </table>
      
      <div class="common-pitfall">
        <p><strong>‚ö†Ô∏è Boosting Warning:</strong> Boosting can overfit if you use too many trees or deep trees. Always monitor validation performance and use early stopping!</p>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-chart-simple"></i> 7.5 Bias-Variance in Ensembles</h3>

    <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 20px; margin: 20px 0;">
      <div style="background: linear-gradient(135deg, rgba(96, 165, 250, 0.1), rgba(167, 139, 250, 0.1)); padding: 20px; border-radius: var(--radius);">
        <h4 style="text-align: center;">Single Tree</h4>
        <div style="height: 100px; position: relative; margin: 20px 0;">
          <div style="width: 100%; height: 2px; background: var(--text); position: absolute; top: 50%;"></div>
          <div style="width: 20px; height: 20px; background: var(--danger); border-radius: 50%; position: absolute; left: 40%; top: 30%;"></div>
          <div style="width: 20px; height: 20px; background: var(--danger); border-radius: 50%; position: absolute; left: 60%; top: 60%;"></div>
          <div style="width: 20px; height: 20px; background: var(--danger); border-radius: 50%; position: absolute; left: 30%; top: 70%;"></div>
        </div>
        <p style="text-align: center;"><span class="badge">High Variance</span></p>
        <p>Small changes in data ‚Üí completely different tree</p>
      </div>
      
      <div style="background: linear-gradient(135deg, rgba(96, 165, 250, 0.1), rgba(167, 139, 250, 0.1)); padding: 20px; border-radius: var(--radius);">
        <h4 style="text-align: center;">Random Forest</h4>
        <div style="height: 100px; position: relative; margin: 20px 0;">
          <div style="width: 100%; height: 2px; background: var(--text); position: absolute; top: 50%;"></div>
          <div style="width: 15px; height: 15px; background: var(--success); border-radius: 50%; position: absolute; left: 45%; top: 45%;"></div>
          <div style="width: 15px; height: 15px; background: var(--success); border-radius: 50%; position: absolute; left: 55%; top: 55%;"></div>
          <div style="width: 15px; height: 15px; background: var(--success); border-radius: 50%; position: absolute; left: 48%; top: 52%;"></div>
        </div>
        <p style="text-align: center;"><span class="badge">Reduced Variance</span></p>
        <p>Averaging many trees smooths out individual errors</p>
      </div>
      
      <div style="background: linear-gradient(135deg, rgba(96, 165, 250, 0.1), rgba(167, 139, 250, 0.1)); padding: 20px; border-radius: var(--radius);">
        <h4 style="text-align: center;">Gradient Boosting</h4>
        <div style="height: 100px; position: relative; margin: 20px 0;">
          <div style="width: 100%; height: 2px; background: var(--text); position: absolute; top: 50%;"></div>
          <div style="width: 18px; height: 18px; background: var(--warning); border-radius: 50%; position: absolute; left: 48%; top: 48%;"></div>
          <div style="width: 12px; height: 12px; background: var(--warning); border-radius: 50%; position: absolute; left: 52%; top: 52%;"></div>
          <div style="width: 8px; height: 8px; background: var(--warning); border-radius: 50%; position: absolute; left: 50%; top: 50%;"></div>
        </div>
        <p style="text-align: center;"><span class="badge">Reduced Bias</span></p>
        <p>Sequentially corrects mistakes, improves accuracy</p>
      </div>
    </div>

    <div class="info-box success">
      <h4>Ensemble Effect on Errors</h4>
      <p>For a ensemble of M models with correlation œÅ and variance œÉ¬≤:</p>
      <div class="math-equation" style="text-align: center; margin: 15px 0;">
        \(Var(\text{ensemble}) = \rho \sigma^2 + \frac{1-\rho}{M} \sigma^2\)
      </div>
      <ul>
        <li><strong>If models are independent (œÅ=0):</strong> Variance = œÉ¬≤/M (massive reduction!)</li>
        <li><strong>If models are identical (œÅ=1):</strong> Variance = œÉ¬≤ (no improvement)</li>
        <li><strong>Random Forest:</strong> Reduces correlation via random features</li>
      </ul>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-star"></i> 7.6 Feature Importance in Ensembles</h3>

    <div class="info-box info">
      <h4>Two Types of Importance</h4>
      
      <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-top: 20px;">
        <div style="background: var(--panel2); padding: 15px; border-radius: var(--radius);">
          <h5>1. Gini Importance (Mean Decrease in Impurity)</h5>
          <p>Average reduction in impurity (Gini or entropy) from using a feature across all trees.</p>
          <ul>
            <li>Fast to compute</li>
            <li>Built into sklearn</li>
            <li>Can be biased toward high-cardinality features</li>
          </ul>
        </div>
        
        <div style="background: var(--panel2); padding: 15px; border-radius: var(--radius);">
          <h5>2. Permutation Importance</h5>
          <p>Measure drop in accuracy when feature values are randomly shuffled.</p>
          <ul>
            <li>More reliable</li>
            <li>Model-agnostic</li>
            <li>Slower to compute</li>
          </ul>
        </div>
      </div>
    </div>

    <div style="background: var(--panel2); padding: 20px; border-radius: var(--radius); margin: 20px 0;">
      <h4>Example: Feature Importance Plot</h4>
      <div style="display: flex; flex-direction: column; gap: 10px;">
        <div>
          <div style="display: flex; justify-content: space-between;">
            <span>Feature A (important)</span>
            <span>0.35</span>
          </div>
          <div style="height: 25px; background: var(--border); border-radius: 5px;">
            <div style="width: 35%; height: 100%; background: var(--primary); border-radius: 5px;"></div>
          </div>
        </div>
        <div>
          <div style="display: flex; justify-content: space-between;">
            <span>Feature B (moderately important)</span>
            <span>0.22</span>
          </div>
          <div style="height: 25px; background: var(--border); border-radius: 5px;">
            <div style="width: 22%; height: 100%; background: var(--primary); border-radius: 5px;"></div>
          </div>
        </div>
        <div>
          <div style="display: flex; justify-content: space-between;">
            <span>Feature C (less important)</span>
            <span>0.12</span>
          </div>
          <div style="height: 25px; background: var(--border); border-radius: 5px;">
            <div style="width: 12%; height: 100%; background: var(--primary); border-radius: 5px;"></div>
          </div>
        </div>
        <div>
          <div style="display: flex; justify-content: space-between;">
            <span>Feature D (unimportant)</span>
            <span>0.03</span>
          </div>
          <div style="height: 25px; background: var(--border); border-radius: 5px;">
            <div style="width: 3%; height: 100%; background: var(--primary); border-radius: 5px;"></div>
          </div>
        </div>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-check-double"></i> 7.7 When to Use Each Ensemble</h3>

    <div class="concept-grid">
      <div class="concept-card">
        <h5 style="color: var(--success);">Random Forest is best when:</h5>
        <ul style="padding-left:20px;">
          <li>You need a <strong>good out-of-the-box model</strong> (minimal tuning)</li>
          <li>You want <strong>robustness to outliers/noise</strong></li>
          <li>Data has <strong>many irrelevant features</strong></li>
          <li>You need <strong>interpretability</strong> (feature importance)</li>
          <li>Dataset is <strong>medium-sized</strong> (up to 100k samples)</li>
          <li>You're worried about <strong>overfitting</strong></li>
        </ul>
      </div>
      
      <div class="concept-card">
        <h5 style="color: var(--success);">XGBoost is best when:</h5>
        <ul style="padding-left:20px;">
          <li>You need <strong>maximum accuracy</strong> (competitions)</li>
          <li>You have <strong>structured/tabular data</strong></li>
          <li>You're willing to <strong>tune hyperparameters</strong></li>
          <li>You have <strong>medium to large datasets</strong></li>
          <li>You need <strong>handling of missing values</strong></li>
          <li>You want <strong>speed</strong> (parallel processing)</li>
        </ul>
      </div>

      <div class="concept-card">
        <h5 style="color: var(--warning);">Consider alternatives when:</h5>
        <ul style="padding-left:20px;">
          <li><strong>Very large datasets</strong> (millions) ‚Üí LightGBM, CatBoost</li>
          <li><strong>Interpretability critical</strong> ‚Üí single decision tree</li>
          <li><strong>Ultra-high dimensional</strong> ‚Üí linear models with regularization</li>
          <li><strong>Images/text</strong> ‚Üí neural networks</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-calculator"></i> 7.8 Ensemble Math Example</h3>

    <div class="expanded-content">
      <h4>Voting: Hard vs Soft</h4>
      
      <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
        <div style="background: var(--panel2); padding: 20px; border-radius: var(--radius);">
          <h5>Hard Voting (Majority)</h5>
          <p>Each tree votes for a class:</p>
          <ul>
            <li>Tree 1: Spam</li>
            <li>Tree 2: Spam</li>
            <li>Tree 3: Not Spam</li>
            <li>Tree 4: Spam</li>
            <li>Tree 5: Not Spam</li>
          </ul>
          <p><strong>Result:</strong> Spam (3 vs 2)</p>
        </div>
        
        <div style="background: var(--panel2); padding: 20px; border-radius: var(--radius);">
          <h5>Soft Voting (Probability Average)</h5>
          <p>Each tree outputs probabilities:</p>
          <ul>
            <li>Tree 1: 0.8 spam, 0.2 not</li>
            <li>Tree 2: 0.7 spam, 0.3 not</li>
            <li>Tree 3: 0.3 spam, 0.7 not</li>
            <li>Tree 4: 0.9 spam, 0.1 not</li>
            <li>Tree 5: 0.4 spam, 0.6 not</li>
          </ul>
          <p>Average spam prob = (0.8+0.7+0.3+0.9+0.4)/5 = 0.62</p>
          <p><strong>Result:</strong> Spam (0.62 > 0.5)</p>
        </div>
      </div>
      
      <p class="pro-tip" style="margin-top: 15px;">
        <strong>Soft voting</strong> usually performs better because it uses confidence information, not just final labels.
      </p>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-graduation-cap"></i> Quick Teaching Summary</h3>

    <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 20px;">
      <div style="background: linear-gradient(135deg, rgba(96, 165, 250, 0.1), rgba(167, 139, 250, 0.1)); padding: 20px; border-radius: var(--radius);">
        <h4 style="margin-top: 0;">Random Forest</h4>
        <ul style="margin-left: 20px;">
          <li>Parallel trees</li>
          <li>Bootstrap samples</li>
          <li>Random features</li>
          <li>Average predictions</li>
          <li><strong>Reduces variance</strong></li>
        </ul>
      </div>
      
      <div style="background: linear-gradient(135deg, rgba(96, 165, 250, 0.1), rgba(167, 139, 250, 0.1)); padding: 20px; border-radius: var(--radius);">
        <h4 style="margin-top: 0;">Gradient Boosting</h4>
        <ul style="margin-left: 20px;">
          <li>Sequential trees</li>
          <li>Fit residuals</li>
          <li>Learning rate</li>
          <li>Weighted combination</li>
          <li><strong>Reduces bias</strong></li>
        </ul>
      </div>
      
      <div style="background: linear-gradient(135deg, rgba(96, 165, 250, 0.1), rgba(167, 139, 250, 0.1)); padding: 20px; border-radius: var(--radius);">
        <h4 style="margin-top: 0;">Key Insight</h4>
        <p style="font-size: 1.1rem; text-align: center; margin-top: 20px;">
          <span class="highlight">Diversity</span> is the key to good ensembles!
        </p>
        <p>Models should be:</p>
        <ul>
          <li>Accurate (better than random)</li>
          <li>Diverse (make different errors)</li>
        </ul>
      </div>
    </div>

    <div style="margin-top: 30px; text-align: center; padding: 20px; background: var(--panel2); border-radius: var(--radius);">
      <p style="font-size: 1.2rem;">
        <i class="fas fa-lightbulb" style="color: var(--warning);"></i> 
        <strong>Remember:</strong> If you have a single model that's 80% accurate, and 100 such models that make <span class="highlight">independent errors</span>, 
        the ensemble can exceed 95% accuracy!
      </p>
    </div>
  </div>

  <!-- Code Editor (Enhanced) -->
  <div class="code-editor-container">
    <div class="code-header">
      <div class="code-title"><i class="fab fa-python"></i> Ensemble Methods in Python</div>
    </div>
    <div class="code-body">
      <pre class="code-block"><code><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier
<span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier
<span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression
<span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC
<span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, cross_val_score, GridSearchCV
<span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report, accuracy_score
<span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification
<span class="keyword">import</span> numpy <span class="keyword">as</span> np
<span class="keyword">import</span> pandas <span class="keyword">as</span> pd
<span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt

<span class="comment"># ============================================</span>
<span class="comment"># EXAMPLE 1: Random Forest Classifier</span>
<span class="comment"># ============================================</span>
rf = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    min_samples_split=5,
    min_samples_leaf=2,
    max_features='sqrt',
    bootstrap=True,
    oob_score=True,  <span class="comment"># Out-of-bag score (validation without CV)</span>
    random_state=42,
    n_jobs=-1  <span class="comment"># Use all CPU cores</span>
)

rf.<span class="function">fit</span>(X_train, y_train)

<span class="keyword">print</span>(<span class="string">"Random Forest Results:"</span>)
<span class="keyword">print</span>(<span class="string">f"Train accuracy: {rf.score(X_train, y_train):.3f}"</span>)
<span class="keyword">print</span>(<span class="string">f"Test accuracy: {rf.score(X_test, y_test):.3f}"</span>)
<span class="keyword">print</span>(<span class="string">f"OOB score: {rf.oob_score_:.3f}"</span>)

<span class="comment"># Feature importance</span>
feature_importance = pd.DataFrame({
    'feature': [f'feature_{i}' <span class="keyword">for</span> i <span class="keyword">in</span> range(X.shape[1])],
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)

<span class="keyword">print</span>(<span class="string">"\nTop 5 important features:"</span>)
<span class="keyword">print</span>(feature_importance.head())

<span class="comment"># ============================================</span>
<span class="comment"># EXAMPLE 2: Gradient Boosting</span>
<span class="comment"># ============================================</span>
gb = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    min_samples_split=5,
    min_samples_leaf=2,
    subsample=0.8,  <span class="comment"># Stochastic gradient boosting</span>
    random_state=42
)

gb.<span class="function">fit</span>(X_train, y_train)

<span class="keyword">print</span>(<span class="string">"\nGradient Boosting Results:"</span>)
<span class="keyword">print</span>(<span class="string">f"Train accuracy: {gb.score(X_train, y_train):.3f}"</span>)
<span class="keyword">print</span>(<span class="string">f"Test accuracy: {gb.score(X_test, y_test):.3f}"</span>)

<span class="comment"># ============================================</span>
<span class="comment"># EXAMPLE 3: XGBoost (install with: pip install xgboost)</span>
<span class="comment"># ============================================</span>
<span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb

xgb_model = xgb.XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_lambda=1.0,  <span class="comment"># L2 regularization</span>
    reg_alpha=0.0,   <span class="comment"># L1 regularization</span>
    random_state=42,
    use_label_encoder=False,
    eval_metric='logloss'
)

xgb_model.<span class="function">fit</span>(X_train, y_train)

<span class="keyword">print</span>(<span class="string">"\nXGBoost Results:"</span>)
<span class="keyword">print</span>(<span class="string">f"Train accuracy: {xgb_model.score(X_train, y_train):.3f}"</span>)
<span class="keyword">print</span>(<span class="string">f"Test accuracy: {xgb_model.score(X_test, y_test):.3f}"</span>)

<span class="comment"># XGBoost feature importance</span>
xgb.plot_importance(xgb_model)
plt.title("XGBoost Feature Importance")
plt.show()

<span class="comment"># ============================================</span>
<span class="comment"># EXAMPLE 4: Voting Classifier</span>
<span class="comment"># ============================================</span>
<span class="comment"># Create base models</span>
models = [
    ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),
    ('gb', GradientBoostingClassifier(n_estimators=50, random_state=42)),
    ('lr', LogisticRegression(random_state=42))
]

<span class="comment"># Hard voting (majority)</span>
voting_hard = VotingClassifier(estimators=models, voting='hard')
voting_hard.<span class="function">fit</span>(X_train, y_train)

<span class="comment"># Soft voting (probability average)</span>
voting_soft = VotingClassifier(estimators=models, voting='soft')
voting_soft.<span class="function">fit</span>(X_train, y_train)

<span class="keyword">print</span>(<span class="string">"\nVoting Classifier Results:"</span>)
<span class="keyword">print</span>(<span class="string">f"Hard voting accuracy: {voting_hard.score(X_test, y_test):.3f}"</span>)
<span class="keyword">print</span>(<span class="string">f"Soft voting accuracy: {voting_soft.score(X_test, y_test):.3f}"</span>)

<span class="comment"># ============================================</span>
<span class="comment"># EXAMPLE 5: Stacking Classifier</span>
<span class="comment"># ============================================</span>
base_learners = [
    ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),
    ('svm', SVC(probability=True, random_state=42)),  <span class="comment"># Need probabilities</span>
    ('gb', GradientBoostingClassifier(n_estimators=50, random_state=42))
]

meta_learner = LogisticRegression()

stacking = StackingClassifier(
    estimators=base_learners,
    final_estimator=meta_learner,
    cv=5  <span class="comment"># Cross-validation for training meta-learner</span>
)

stacking.<span class="function">fit</span>(X_train, y_train)

<span class="keyword">print</span>(<span class="string">"\nStacking Classifier Results:"</span>)
<span class="keyword">print</span>(<span class="string">f"Test accuracy: {stacking.score(X_test, y_test):.3f}"</span>)

<span class="comment"># ============================================</span>
<span class="comment"># EXAMPLE 6: Hyperparameter Tuning for Random Forest</span>
<span class="comment"># ============================================</span>
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']
}

grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42, n_jobs=-1),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

grid_search.<span class="function">fit</span>(X_train, y_train)

<span class="keyword">print</span>(<span class="string">"\nBest Random Forest parameters:"</span>, grid_search.best_params_)
<span class="keyword">print</span>(<span class="string">f"Best CV score: {grid_search.best_score_:.3f}"</span>)

<span class="comment"># ============================================</span>
<span class="comment"># EXAMPLE 7: Early Stopping with XGBoost</span>
<span class="comment"># ============================================</span>
<span class="comment"># Create validation set</span>
X_train_part, X_val, y_train_part, y_val = train_test_split(
    X_train, y_train, test_size=0.2, random_state=42
)

xgb_early = xgb.XGBClassifier(
    n_estimators=1000,  <span class="comment"># Large number, will stop early</span>
    learning_rate=0.1,
    max_depth=3,
    early_stopping_rounds=50,
    random_state=42,
    use_label_encoder=False,
    eval_metric='logloss'
)

xgb_early.<span class="function">fit</span>(
    X_train_part, y_train_part,
    eval_set=[(X_val, y_val)],
    verbose=False
)

<span class="keyword">print</span>(<span class="string">f"\nXGBoost early stopping - best iteration: {xgb_early.best_iteration}"</span>)
<span class="keyword">print</span>(<span class="string">f"Best validation score: {xgb_early.best_score:.3f}"</span>)

<span class="comment"># ============================================</span>
<span class="comment"># EXAMPLE 8: Compare All Models</span>
<span class="comment"># ============================================</span>
models = {
    'Random Forest': rf,
    'Gradient Boosting': gb,
    'XGBoost': xgb_model,
    'Voting (soft)': voting_soft,
    'Stacking': stacking
}

results = []
<span class="keyword">for</span> name, model <span class="keyword">in</span> models.items():
    y_pred = model.<span class="function">predict</span>(X_test)
    acc = accuracy_score(y_test, y_pred)
    results.append({'Model': name, 'Accuracy': acc})

results_df = pd.DataFrame(results).sort_values('Accuracy', ascending=False)
<span class="keyword">print</span>(<span class="string">"\nModel Comparison:"</span>)
<span class="keyword">print</span>(results_df)

<span class="comment"># Plot comparison</span>
plt.figure(figsize=(10,6))
plt.barh(results_df['Model'], results_df['Accuracy'])
plt.xlabel('Accuracy')
plt.title('Ensemble Methods Comparison')
plt.xlim(0, 1)
plt.show()</code></pre>
    </div>
  </div>

  <!-- Quick Reference Card -->
  <div class="panel section">
    <h3><i class="fas fa-clipboard-list"></i> Ensemble Methods Cheat Sheet</h3>
    
    <table class="detailed-table">
      <thead>
        <tr><th>Method</th><th>Training</th><th>Key Parameters</th><th>Best For</th></tr>
      </thead>
      <tbody>
        <tr><td><strong>Random Forest</strong></td><td>Parallel</td><td>n_estimators, max_features</td><td>Robust baseline, no tuning</td></tr>
        <tr><td><strong>AdaBoost</strong></td><td>Sequential</td><td>n_estimators, learning_rate</td><td>Simple boosting</td></tr>
        <tr><td><strong>Gradient Boosting</strong></td><td>Sequential</td><td>learning_rate, n_estimators, max_depth</td><td>General boosting</td></tr>
        <tr><td><strong>XGBoost</strong></td><td>Sequential</td><td>learning_rate, max_depth, reg_lambda</td><td>Competitions, accuracy</td></tr>
        <tr><td><strong>LightGBM</strong></td><td>Sequential</td><td>num_leaves, learning_rate</td><td>Large datasets</td></tr>
        <tr><td><strong>CatBoost</strong></td><td>Sequential</td><td>iterations, learning_rate</td><td>Categorical features</td></tr>
        <tr><td><strong>Voting</strong></td><td>Any</td><td>voting='hard'/'soft'</td><td>Combining existing models</td></tr>
        <tr><td><strong>Stacking</strong></td><td>Meta</td><td>cv, final_estimator</td><td>Maximum performance</td></tr>
      </tbody>
    </table>
    
    <div class="info-box success" style="margin-top: 20px;">
      <h4>Ensemble Wisdom:</h4>
      <ul>
        <li><strong>Random Forest:</strong> Great first choice, minimal tuning</li>
        <li><strong>XGBoost:</strong> Best for structured data, needs tuning</li>
        <li><strong>Stacking:</strong> Use when you have diverse strong models</li>
        <li><strong>Diversity is key:</strong> Combine different types of models (trees, linear, SVM)</li>
      </ul>
    </div>
  </div>

  <script>
  // Interactive ensemble demo functions (if needed)
  function updateEnsembleDemo() {
      // This would control an interactive ensemble visualization
      // For now, just a placeholder
      console.log("Ensemble demo updated");
  }
  </script>
</div>

    <!-- ================================================
         SECTION 8: EVALUATION METRICS (BEYOND ACCURACY)
    ================================================= -->
    <!-- ================================================
     SECTION 8: EVALUATION METRICS (BEYOND ACCURACY) - ENHANCED
================================================= -->
<div id="evaluation-class" class="module-content">

  <div class="panel section">
    <h3><i class="fas fa-clipboard-check"></i> 8.1 Why Accuracy Can Be Misleading</h3>

    <div class="info-box warning">
      <h4><i class="fas fa-exclamation-triangle"></i> The 99% Accuracy Trap</h4>
      
      <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;">
        <div>
          <p><strong>Scenario: Rare Disease Detection</strong></p>
          <ul>
            <li>Disease affects 1 in 1000 people (0.1%)</li>
            <li>You build a model that predicts "No Disease" for everyone</li>
            <li>Accuracy = 99.9%! üéâ</li>
            <li>But your model detects <strong>zero</strong> actual cases üò±</li>
          </ul>
        </div>
        <div style="background: var(--panel2); padding: 15px; border-radius: var(--radius);">
          <p><strong>Confusion Matrix:</strong></p>
          <table style="width:100%; text-align:center;">
            <tr><td></td><td>Pred: No</td><td>Pred: Yes</td></tr>
            <tr><td>Actual: No</td><td>999</td><td>0</td></tr>
            <tr><td>Actual: Yes</td><td>1</td><td>0</td></tr>
          </table>
          <p>Accuracy = 999/1000 = 99.9%</p>
          <p>But recall for disease = 0/1 = <span style="color: var(--danger);">0%</span></p>
        </div>
      </div>
      
      <p>
        <strong>When classes are imbalanced</strong> (one class much rarer than others), 
        accuracy is a terrible metric. You need metrics that focus on how well you identify 
        the rare but important class.
      </p>
    </div>

    <div class="explanation-box">
      <p><strong>üìö In Simple Terms:</strong> Imagine a fire alarm that never goes off. It's "correct" 99.9% of the time (since there's rarely a fire), but it's completely useless! We need metrics that tell us how well it detects actual fires.</p>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-table"></i> 8.2 The Confusion Matrix: Your Best Friend</h3>

    <div class="info-box info">
      <h4>All Possible Outcomes in One Table</h4>
      
      <div style="display: flex; justify-content: center; margin: 30px 0;">
        <div class="confusion-matrix">
          <div class="matrix-cell label"></div>
          <div class="matrix-cell label">Predicted: POSITIVE</div>
          <div class="matrix-cell label">Predicted: NEGATIVE</div>
          
          <div class="matrix-cell label">Actual: POSITIVE</div>
          <div class="matrix-cell true-positive">
            <strong>True Positive (TP)</strong><br>
            <small>Correctly predicted positive</small>
          </div>
          <div class="matrix-cell false-negative">
            <strong>False Negative (FN)</strong><br>
            <small>Missed positive (Type II Error)</small>
          </div>
          
          <div class="matrix-cell label">Actual: NEGATIVE</div>
          <div class="matrix-cell false-positive">
            <strong>False Positive (FP)</strong><br>
            <small>False alarm (Type I Error)</small>
          </div>
          <div class="matrix-cell true-negative">
            <strong>True Negative (TN)</strong><br>
            <small>Correctly predicted negative</small>
          </div>
        </div>
      </div>

      <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px; margin-top: 20px;">
        <div style="background: var(--panel2); padding: 15px; border-radius: var(--radius);">
          <h5>Type I Error (False Positive)</h5>
          <p>Predicting positive when it's actually negative</p>
          <p><em>Example: Marking a legitimate email as spam</em></p>
        </div>
        <div style="background: var(--panel2); padding: 15px; border-radius: var(--radius);">
          <h5>Type II Error (False Negative)</h5>
          <p>Predicting negative when it's actually positive</p>
          <p><em>Example: Missing a spam email in your inbox</em></p>
        </div>
      </div>
    </div>

    <div class="expanded-content">
      <h4><i class="fas fa-calculator"></i> Worked Example: Spam Filter</h4>
      
      <p><strong>Scenario:</strong> Test a spam filter on 100 emails</p>
      
      <table class="detailed-table">
        <thead>
          <tr><th></th><th>Predicted Spam</th><th>Predicted Not Spam</th><th>Total</th></tr>
        </thead>
        <tbody>
          <tr><td><strong>Actual Spam</strong></td><td class="true-positive">45 (TP)</td><td class="false-negative">5 (FN)</td><td>50</td></tr>
          <tr><td><strong>Actual Not Spam</strong></td><td class="false-positive">10 (FP)</td><td class="true-negative">40 (TN)</td><td>50</td></tr>
          <tr><td><strong>Total</strong></td><td>55</td><td>45</td><td>100</td></tr>
        </tbody>
      </table>

      <div style="display: grid; grid-template-columns: repeat(4, 1fr); gap: 15px; margin-top: 20px;">
        <div style="text-align: center; padding: 10px; background: var(--panel2); border-radius: var(--radius);">
          <div style="font-size: 1.5rem; color: var(--success);">45</div>
          <div>True Positives</div>
        </div>
        <div style="text-align: center; padding: 10px; background: var(--panel2); border-radius: var(--radius);">
          <div style="font-size: 1.5rem; color: var(--danger);">5</div>
          <div>False Negatives</div>
        </div>
        <div style="text-align: center; padding: 10px; background: var(--panel2); border-radius: var(--radius);">
          <div style="font-size: 1.5rem; color: var(--danger);">10</div>
          <div>False Positives</div>
        </div>
        <div style="text-align: center; padding: 10px; background: var(--panel2); border-radius: var(--radius);">
          <div style="font-size: 1.5rem; color: var(--success);">40</div>
          <div>True Negatives</div>
        </div>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-chart-pie"></i> 8.3 Key Metrics Derived from Confusion Matrix</h3>

    <div class="concept-grid">
      <div class="concept-card">
        <h5>‚úÖ Precision (Positive Predictive Value)</h5>
        <div class="math-equation">\(Precision = \frac{TP}{TP + FP}\)</div>
        <p><strong>Question:</strong> Of all the emails I marked as spam, how many were actually spam?</p>
        <p><strong>Example:</strong> 45/(45+10) = 0.818 (81.8%)</p>
        <p><strong>When to care:</strong> When false positives are costly</p>
        <p><em>Spam filter: Don't mark important emails as spam</em></p>
      </div>
      
      <div class="concept-card">
        <h5>üéØ Recall (Sensitivity, True Positive Rate)</h5>
        <div class="math-equation">\(Recall = \frac{TP}{TP + FN}\)</div>
        <p><strong>Question:</strong> Of all the actual spam emails, how many did I catch?</p>
        <p><strong>Example:</strong> 45/(45+5) = 0.9 (90%)</p>
        <p><strong>When to care:</strong> When false negatives are costly</p>
        <p><em>Medical diagnosis: Don't miss diseases</em></p>
      </div>
      
      <div class="concept-card">
        <h5>‚öñÔ∏è F1 Score</h5>
        <div class="math-equation">\(F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}\)</div>
        <p><strong>Question:</strong> Harmonic mean of precision and recall - balances both</p>
        <p><strong>Example:</strong> 2 √ó (0.818√ó0.9)/(0.818+0.9) = 0.857</p>
        <p><strong>When to care:</strong> When you need a single number to compare models, especially with imbalanced classes</p>
      </div>
      
      <div class="concept-card">
        <h5>üìä Specificity (True Negative Rate)</h5>
        <div class="math-equation">\(Specificity = \frac{TN}{TN + FP}\)</div>
        <p><strong>Question:</strong> Of all the non-spam emails, how many did I correctly leave alone?</p>
        <p><strong>Example:</strong> 40/(40+10) = 0.8 (80%)</p>
        <p><strong>When to care:</strong> When you care about both classes</p>
      </div>
    </div>

    <div class="expanded-content">
      <h4>All Metrics for Our Example</h4>
      
      <table class="detailed-table">
        <thead>
          <tr><th>Metric</th><th>Formula</th><th>Value</th><th>Interpretation</th></tr>
        </thead>
        <tbody>
          <tr><td>Accuracy</td><td>(TP+TN)/(TP+TN+FP+FN)</td><td>(45+40)/100 = 0.85</td><td>85% correct overall</td></tr>
          <tr><td>Precision</td><td>TP/(TP+FP)</td><td>45/55 = 0.818</td><td>82% of spam predictions correct</td></tr>
          <tr><td>Recall</td><td>TP/(TP+FN)</td><td>45/50 = 0.9</td><td>Caught 90% of actual spam</td></tr>
          <tr><td>Specificity</td><td>TN/(TN+FP)</td><td>40/50 = 0.8</td><td>80% of non-spam correctly identified</td></tr>
          <tr><td>F1 Score</td><td>2√óP√óR/(P+R)</td><td>2√ó0.818√ó0.9/1.718 = 0.857</td><td>Balanced measure</td></tr>
          <tr><td>False Positive Rate</td><td>FP/(FP+TN)</td><td>10/50 = 0.2</td><td>20% false alarm rate</td></tr>
          <tr><td>False Negative Rate</td><td>FN/(FN+TP)</td><td>5/50 = 0.1</td><td>10% miss rate</td></tr>
        </tbody>
      </table>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-chart-line"></i> 8.4 ROC Curve and AUC</h3>

    <div class="info-box info">
      <h4>ROC: Receiver Operating Characteristic</h4>
      <p>
        The ROC curve shows the trade-off between <strong>True Positive Rate (Recall)</strong> and 
        <strong>False Positive Rate (1 - Specificity)</strong> at all possible thresholds.
      </p>
    </div>

    <div class="roc-container">
      <canvas id="rocChart"></canvas>
    </div>

    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;">
      <div>
        <h5>How to Read an ROC Curve:</h5>
        <ul>
          <li><strong>Perfect classifier:</strong> Goes straight up to top-left corner (TPR=1, FPR=0)</li>
          <li><strong>Random guessing:</strong> Diagonal line (AUC=0.5)</li>
          <li><strong>Better classifier:</strong> Curve closer to top-left</li>
          <li><strong>AUC = Area Under Curve:</strong> Single number summary</li>
        </ul>
      </div>
      
      <div style="background: var(--panel2); padding: 15px; border-radius: var(--radius);">
        <h5>AUC Interpretation:</h5>
        <ul>
          <li><strong>AUC = 1.0:</strong> Perfect separation</li>
          <li><strong>AUC > 0.9:</strong> Excellent</li>
          <li><strong>AUC > 0.8:</strong> Good</li>
          <li><strong>AUC > 0.7:</strong> Fair</li>
          <li><strong>AUC = 0.5:</strong> Random guessing</li>
          <li><strong>AUC < 0.5:</strong> Worse than random (reverse predictions!)</li>
        </ul>
      </div>
    </div>

    <div class="expanded-content">
      <h4>Threshold Tuning Example</h4>
      
      <table class="detailed-table">
        <thead>
          <tr><th>Threshold</th><th>TPR (Recall)</th><th>FPR</th><th>Precision</th><th>Interpretation</th></tr>
        </thead>
        <tbody>
          <tr><td>0.1</td><td>0.98</td><td>0.60</td><td>0.45</td><td>Catch almost all positives, but many false alarms</td></tr>
          <tr><td>0.3</td><td>0.95</td><td>0.35</td><td>0.63</td><td>Good recall, moderate false positives</td></tr>
          <tr><td>0.5</td><td>0.90</td><td>0.20</td><td>0.82</td><td>Balanced (default)</td></tr>
          <tr><td>0.7</td><td>0.75</td><td>0.08</td><td>0.90</td><td>High precision, but miss some positives</td></tr>
          <tr><td>0.9</td><td>0.40</td><td>0.01</td><td>0.98</td><td>Very sure predictions, but miss most positives</td></tr>
        </tbody>
      </table>
      
      <p class="pro-tip">
        <strong>üí° Choose threshold based on business needs:</strong> Medical diagnosis might use low threshold (catch diseases), spam filter might use high threshold (avoid false positives).
      </p>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-balance-scale"></i> 8.5 Precision-Recall Curve</h3>

    <div class="info-box info">
      <h4>Better for Imbalanced Data</h4>
      <p>
        When classes are highly imbalanced (e.g., 1% positives), ROC curves can look overly optimistic 
        because FPR is dominated by many negatives. Precision-Recall curves focus on the positive class.
      </p>
    </div>

    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
      <div style="background: var(--panel2); padding: 20px; border-radius: var(--radius);">
        <h5>PR Curve Characteristics:</h5>
        <ul>
          <li>X-axis: Recall (TPR)</li>
          <li>Y-axis: Precision</li>
          <li>Perfect classifier: reaches (1,1)</li>
          <li>Baseline: horizontal line at positive class prevalence</li>
          <li>Area under PR curve (AUPRC) is more informative for imbalanced data</li>
        </ul>
      </div>
      
      <div style="background: var(--panel2); padding: 20px; border-radius: var(--radius);">
        <h5>When to Use Which:</h5>
        <ul>
          <li><strong>ROC-AUC:</strong> Balanced classes, general comparison</li>
          <li><strong>PR-AUC:</strong> Imbalanced classes, focus on positive class</li>
          <li><strong>Both:</strong> For critical applications, look at both</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-chart-bar"></i> 8.6 Multi-Class Metrics</h3>

    <div class="info-box info">
      <h4>Extending to Multiple Classes</h4>
      
      <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 20px; margin: 20px 0;">
        <div style="text-align: center; padding: 15px; background: var(--panel2); border-radius: var(--radius);">
          <strong>Macro-Averaging</strong>
          <p>Calculate metric for each class, then average (unweighted)</p>
          <p><em>Each class counts equally</em></p>
        </div>
        
        <div style="text-align: center; padding: 15px; background: var(--panel2); border-radius: var(--radius);">
          <strong>Micro-Averaging</strong>
          <p>Aggregate all TP/FP/FN across classes, then calculate</p>
          <p><em>Each instance counts equally</em></p>
        </div>
        
        <div style="text-align: center; padding: 15px; background: var(--panel2); border-radius: var(--radius);">
          <strong>Weighted-Averaging</strong>
          <p>Average weighted by class support (number of true instances)</p>
          <p><em>Accounts for class imbalance</em></p>
        </div>
      </div>
    </div>

    <div class="expanded-content">
      <h4>Example: 3-Class Classification</h4>
      
      <table class="detailed-table">
        <thead>
          <tr><th>Class</th><th>Precision</th><th>Recall</th><th>F1</th><th>Support</th></tr>
        </thead>
        <tbody>
          <tr><td>Cat</td><td>0.85</td><td>0.90</td><td>0.87</td><td>100</td></tr>
          <tr><td>Dog</td><td>0.78</td><td>0.82</td><td>0.80</td><td>150</td></tr>
          <tr><td>Bird</td><td>0.92</td><td>0.88</td><td>0.90</td><td>50</td></tr>
        </tbody>
      </table>
      
      <div style="margin-top: 15px;">
        <p><strong>Macro F1:</strong> (0.87 + 0.80 + 0.90)/3 = <span class="highlight">0.857</span></p>
        <p><strong>Micro F1:</strong> (All TP)/(All TP + 0.5√ó(All FP + All FN)) = <span class="highlight">0.863</span></p>
        <p><strong>Weighted F1:</strong> (0.87√ó100 + 0.80√ó150 + 0.90√ó50)/(100+150+50) = <span class="highlight">0.835</span></p>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-tachometer-alt"></i> 8.7 Other Important Metrics</h3>

    <div class="algorithm-grid">
      <div class="algorithm-card">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-chart-pie"></i></div>
          <div class="algorithm-title">
            <h4>Log Loss (Cross-Entropy)</h4>
          </div>
        </div>
        <div class="algorithm-body">
          <div class="math-equation">\(-\frac{1}{N}\sum [y_i\log(p_i) + (1-y_i)\log(1-p_i)]\)</div>
          <p>Penalizes confident wrong predictions. Lower is better.</p>
          <p><strong>Perfect model:</strong> Log Loss = 0</p>
        </div>
      </div>

      <div class="algorithm-card">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-bullseye"></i></div>
          <div class="algorithm-title">
            <h4>Cohen's Kappa</h4>
          </div>
        </div>
        <div class="algorithm-body">
          <div class="math-equation">\(\kappa = \frac{p_o - p_e}{1 - p_e}\)</div>
          <p>Accuracy adjusted for chance agreement. Handles class imbalance.</p>
          <p><strong>Œ∫ > 0.8:</strong> Excellent agreement</p>
        </div>
      </div>

      <div class="algorithm-card">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-chart-line"></i></div>
          <div class="algorithm-title">
            <h4>Matthews Correlation Coefficient (MCC)</h4>
          </div>
        </div>
        <div class="algorithm-body">
          <p>A balanced measure even with very imbalanced classes.</p>
          <p>Range: -1 (worst) to +1 (perfect)</p>
          <p>0 = random predictions</p>
        </div>
      </div>

      <div class="algorithm-card">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-clock"></i></div>
          <div class="algorithm-title">
            <h4>Balanced Accuracy</h4>
          </div>
        </div>
        <div class="algorithm-body">
          <div class="math-equation">\(\frac{Recall + Specificity}{2}\)</div>
          <p>Average of per-class recall. Prevents overestimating on imbalanced data.</p>
        </div>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-calculator"></i> 8.8 Interactive Metric Calculator</h3>

    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
      <div style="background: var(--panel2); padding: 20px; border-radius: var(--radius);">
        <h4>Enter Confusion Matrix Values</h4>
        
        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px; margin: 20px 0;">
          <div>
            <label>True Positives:</label>
            <input type="number" id="calcTP" value="45" min="0" step="1" class="btn btn-secondary" style="width:100%;">
          </div>
          <div>
            <label>False Negatives:</label>
            <input type="number" id="calcFN" value="5" min="0" step="1" class="btn btn-secondary" style="width:100%;">
          </div>
          <div>
            <label>False Positives:</label>
            <input type="number" id="calcFP" value="10" min="0" step="1" class="btn btn-secondary" style="width:100%;">
          </div>
          <div>
            <label>True Negatives:</label>
            <input type="number" id="calcTN" value="40" min="0" step="1" class="btn btn-secondary" style="width:100%;">
          </div>
        </div>
        
        <button class="btn btn-primary" onclick="updateMetrics()">Calculate Metrics</button>
      </div>
      
      <div style="background: var(--panel2); padding: 20px; border-radius: var(--radius);">
        <h4>Results</h4>
        
        <table style="width:100%;">
          <tr><td>Accuracy:</td><td id="resultAcc" class="highlight">0.85</td></tr>
          <tr><td>Precision:</td><td id="resultPrec" class="highlight">0.818</td></tr>
          <tr><td>Recall:</td><td id="resultRec" class="highlight">0.900</td></tr>
          <tr><td>Specificity:</td><td id="resultSpec" class="highlight">0.800</td></tr>
          <tr><td>F1 Score:</td><td id="resultF1" class="highlight">0.857</td></tr>
          <tr><td>False Positive Rate:</td><td id="resultFPR" class="highlight">0.200</td></tr>
          <tr><td>False Negative Rate:</td><td id="resultFNR" class="highlight">0.100</td></tr>
        </table>
      </div>
    </div>

    <script>
    function updateMetrics() {
        const TP = parseFloat(document.getElementById('calcTP').value) || 0;
        const FN = parseFloat(document.getElementById('calcFN').value) || 0;
        const FP = parseFloat(document.getElementById('calcFP').value) || 0;
        const TN = parseFloat(document.getElementById('calcTN').value) || 0;
        
        const total = TP + FN + FP + TN;
        const accuracy = (TP + TN) / total;
        const precision = TP / (TP + FP) || 0;
        const recall = TP / (TP + FN) || 0;
        const specificity = TN / (TN + FP) || 0;
        const f1 = 2 * (precision * recall) / (precision + recall) || 0;
        const fpr = FP / (FP + TN) || 0;
        const fnr = FN / (FN + TP) || 0;
        
        document.getElementById('resultAcc').textContent = accuracy.toFixed(3);
        document.getElementById('resultPrec').textContent = precision.toFixed(3);
        document.getElementById('resultRec').textContent = recall.toFixed(3);
        document.getElementById('resultSpec').textContent = specificity.toFixed(3);
        document.getElementById('resultF1').textContent = f1.toFixed(3);
        document.getElementById('resultFPR').textContent = fpr.toFixed(3);
        document.getElementById('resultFNR').textContent = fnr.toFixed(3);
    }
    </script>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-check-double"></i> 8.9 Choosing the Right Metric</h3>

    <div class="info-box success">
      <h4>Decision Tree for Metric Selection</h4>
      
      <div style="display: flex; justify-content: center; margin: 20px 0;">
        <svg width="600" height="300" viewBox="0 0 600 300">
          <!-- Root -->
          <circle cx="300" cy="30" r="20" fill="var(--primary)"/>
          <text x="300" y="35" text-anchor="middle" fill="white" font-size="10">Start</text>
          
          <!-- First split -->
          <line x1="300" y1="50" x2="200" y2="100" stroke="var(--text)"/>
          <line x1="300" y1="50" x2="400" y2="100" stroke="var(--text)"/>
          
          <!-- Balanced vs Imbalanced -->
          <circle cx="200" cy="120" r="15" fill="var(--warning)"/>
          <text x="200" y="125" text-anchor="middle" fill="white" font-size="8">Balanced</text>
          
          <circle cx="400" cy="120" r="15" fill="var(--danger)"/>
          <text x="400" y="125" text-anchor="middle" fill="white" font-size="8">Imbalanced</text>
          
          <!-- Next level -->
          <line x1="200" y1="135" x2="120" y2="180" stroke="var(--text)"/>
          <line x1="200" y1="135" x2="280" y2="180" stroke="var(--text)"/>
          <line x1="400" y1="135" x2="320" y2="180" stroke="var(--text)"/>
          <line x1="400" y1="135" x2="480" y2="180" stroke="var(--text)"/>
          
          <!-- Leaves -->
          <circle cx="120" cy="210" r="15" fill="var(--success)"/>
          <text x="120" y="215" text-anchor="middle" fill="white" font-size="8">Accuracy</text>
          
          <circle cx="280" cy="210" r="15" fill="var(--success)"/>
          <text x="280" y="215" text-anchor="middle" fill="white" font-size="8">F1</text>
          
          <circle cx="320" cy="210" r="15" fill="var(--success)"/>
          <text x="320" y="215" text-anchor="middle" fill="white" font-size="8">Recall</text>
          
          <circle cx="480" cy="210" r="15" fill="var(--success)"/>
          <text x="480" y="215" text-anchor="middle" fill="white" font-size="8">Precision</text>
          
          <!-- Labels -->
          <text x="70" y="190" fill="var(--text)" font-size="8">Equal costs</text>
          <text x="220" y="190" fill="var(--text)" font-size="8">Balanced</text>
          <text x="290" y="160" fill="var(--text)" font-size="8">Catch positives</text>
          <text x="450" y="160" fill="var(--text)" font-size="8">Avoid FP</text>
        </svg>
      </div>
      
      <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px; margin-top: 20px;">
        <div>
          <h5>Use Accuracy when:</h5>
          <ul>
            <li>Classes are roughly balanced</li>
            <li>False positives and false negatives have similar costs</li>
            <li>You need a simple, intuitive metric</li>
          </ul>
        </div>
        
        <div>
          <h5>Use F1 when:</h5>
          <ul>
            <li>Classes are imbalanced</li>
            <li>You care about both precision and recall</li>
            <li>You need one number to compare models</li>
          </ul>
        </div>
        
        <div>
          <h5>Use Precision when:</h5>
          <ul>
            <li>False positives are costly (spam filter, fraud alerts)</li>
            <li>You want to be sure before taking action</li>
          </ul>
        </div>
        
        <div>
          <h5>Use Recall when:</h5>
          <ul>
            <li>False negatives are costly (disease diagnosis)</li>
            <li>Missing a positive is dangerous</li>
          </ul>
        </div>
      </div>
    </div>

    <div class="common-pitfall">
      <h4>‚ö†Ô∏è Common Metric Mistakes</h4>
      <ul>
        <li><strong>Using accuracy on imbalanced data:</strong> The 99% trap!</li>
        <li><strong>Only looking at one metric:</strong> Always check multiple perspectives</li>
        <li><strong>Not considering business context:</strong> Choose metrics based on real costs</li>
        <li><strong>Comparing models on different thresholds:</strong> Compare at optimal thresholds</li>
        <li><strong>Ignoring confidence calibration:</strong> Check if probabilities match reality</li>
      </ul>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-graduation-cap"></i> Quick Teaching Summary</h3>

    <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 20px;">
      <div style="background: linear-gradient(135deg, rgba(96, 165, 250, 0.1), rgba(167, 139, 250, 0.1)); padding: 20px; border-radius: var(--radius);">
        <h4 style="margin-top: 0;">For Positive Class</h4>
        <ul>
          <li><strong>Precision:</strong> TP/(TP+FP) - "When I say yes, am I right?"</li>
          <li><strong>Recall:</strong> TP/(TP+FN) - "Did I catch all the positives?"</li>
          <li><strong>F1:</strong> Harmonic mean of both</li>
        </ul>
      </div>
      
      <div style="background: linear-gradient(135deg, rgba(96, 165, 250, 0.1), rgba(167, 139, 250, 0.1)); padding: 20px; border-radius: var(--radius);">
        <h4 style="margin-top: 0;">For Negative Class</h4>
        <ul>
          <li><strong>Specificity:</strong> TN/(TN+FP) - Correctly identify negatives</li>
          <li><strong>NPV:</strong> TN/(TN+FN) - When I say no, am I right?</li>
        </ul>
      </div>
      
      <div style="background: linear-gradient(135deg, rgba(96, 165, 250, 0.1), rgba(167, 139, 250, 0.1)); padding: 20px; border-radius: var(--radius);">
        <h4 style="margin-top: 0;">Summary Curves</h4>
        <ul>
          <li><strong>ROC-AUC:</strong> Overall discrimination ability</li>
          <li><strong>PR-AUC:</strong> Better for imbalanced data</li>
          <li><strong>Calibration:</strong> Do probabilities match reality?</li>
        </ul>
      </div>
    </div>

    <div style="margin-top: 30px; text-align: center; padding: 20px; background: var(--panel2); border-radius: var(--radius);">
      <p style="font-size: 1.2rem;">
        <i class="fas fa-lightbulb" style="color: var(--warning);"></i> 
        <strong>Remember:</strong> <span class="highlight">No single metric tells the whole story!</span> 
        Always look at confusion matrix, multiple metrics, and consider business costs of different errors.
      </p>
    </div>
  </div>

  <!-- Code Editor (Enhanced) -->
  <div class="code-editor-container">
    <div class="code-header">
      <div class="code-title"><i class="fab fa-python"></i> All Metrics in One Place</div>
    </div>
    <div class="code-body">
      <pre class="code-block"><code><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_auc_score, 
    roc_curve, precision_recall_curve, auc, log_loss,
    cohen_kappa_score, matthews_corrcoef, balanced_accuracy_score
)
<span class="keyword">from</span> sklearn.calibration <span class="keyword">import</span> calibration_curve
<span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt
<span class="keyword">import</span> numpy <span class="keyword">as</span> np

<span class="comment"># ============================================</span>
<span class="comment"># After making predictions</span>
<span class="comment"># ============================================</span>
y_pred = model.<span class="function">predict</span>(X_test)
y_proba = model.<span class="function">predict_proba</span>(X_test)[:, 1]  <span class="comment"># Probabilities for positive class</span>

<span class="comment"># ============================================</span>
<span class="comment"># 1. Basic Metrics</span>
<span class="comment"># ============================================</span>
<span class="keyword">print</span>(<span class="string">"="</span>*50)
<span class="keyword">print</span>(<span class="string">"BASIC METRICS"</span>)
<span class="keyword">print</span>(<span class="string">"="</span>*50)

<span class="keyword">print</span>(<span class="string">f"Accuracy: {accuracy_score(y_test, y_pred):.3f}"</span>)
<span class="keyword">print</span>(<span class="string">f"Precision: {precision_score(y_test, y_pred):.3f}"</span>)
<span class="keyword">print</span>(<span class="string">f"Recall: {recall_score(y_test, y_pred):.3f}"</span>)
<span class="keyword">print</span>(<span class="string">f"F1 Score: {f1_score(y_test, y_pred):.3f}"</span>)

<span class="comment"># For multi-class, specify average parameter</span>
<span class="comment"># precision_score(y_test, y_pred, average='macro')</span>
<span class="comment"># average options: 'micro', 'macro', 'weighted'</span>

<span class="comment"># ============================================</span>
<span class="comment"># 2. Confusion Matrix</span>
<span class="comment"># ============================================</span>
cm = confusion_matrix(y_test, y_pred)
<span class="keyword">print</span>(<span class="string">"\nConfusion Matrix:"</span>)
<span class="keyword">print</span>(cm)

<span class="comment"># Visualize confusion matrix</span>
<span class="keyword">import</span> seaborn <span class="keyword">as</span> sns
plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.show()

<span class="comment"># ============================================</span>
<span class="comment"># 3. Detailed Classification Report</span>
<span class="comment"># ============================================</span>
<span class="keyword">print</span>(<span class="string">"\nClassification Report:"</span>)
<span class="keyword">print</span>(classification_report(
    y_test, y_pred, 
    target_names=[<span class="string">'Negative'</span>, <span class="string">'Positive'</span>]
))

<span class="comment"># ============================================</span>
<span class="comment"># 4. ROC Curve and AUC</span>
<span class="comment"># ============================================</span>
fpr, tpr, thresholds = roc_curve(y_test, y_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(10,8))
plt.plot(fpr, tpr, color='darkorange', lw=2, 
         label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', 
         label='Random')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Recall)')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

<span class="keyword">print</span>(<span class="string">f"ROC-AUC: {roc_auc:.3f}"</span>)

<span class="comment"># Find optimal threshold (Youden's J statistic)</span>
optimal_idx = np.argmax(tpr - fpr)
optimal_threshold = thresholds[optimal_idx]
<span class="keyword">print</span>(<span class="string">f"Optimal threshold (max tpr - fpr): {optimal_threshold:.3f}"</span>)

<span class="comment"># ============================================</span>
<span class="comment"># 5. Precision-Recall Curve (Better for imbalanced)</span>
<span class="comment"># ============================================</span>
precision, recall, pr_thresholds = precision_recall_curve(y_test, y_proba)
pr_auc = auc(recall, precision)

plt.figure(figsize=(10,8))
plt.plot(recall, precision, color='green', lw=2, 
         label=f'PR curve (AUC = {pr_auc:.2f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc="lower left")
plt.grid(True)
plt.show()

<span class="keyword">print</span>(<span class="string">f"PR-AUC: {pr_auc:.3f}"</span>)

<span class="comment"># ============================================</span>
<span class="comment"># 6. Log Loss (Cross-Entropy)</span>
<span class="comment"># ============================================</span>
logloss = log_loss(y_test, y_proba)
<span class="keyword">print</span>(<span class="string">f"\nLog Loss: {logloss:.3f}"</span>)

<span class="comment"># ============================================</span>
<span class="comment"># 7. Other Important Metrics</span>
<span class="comment"># ============================================</span>
kappa = cohen_kappa_score(y_test, y_pred)
mcc = matthews_corrcoef(y_test, y_pred)
balanced_acc = balanced_accuracy_score(y_test, y_pred)

<span class="keyword">print</span>(<span class="string">f"Cohen's Kappa: {kappa:.3f}"</span>)
<span class="keyword">print</span>(<span class="string">f"Matthews Correlation Coefficient: {mcc:.3f}"</span>)
<span class="keyword">print</span>(<span class="string">f"Balanced Accuracy: {balanced_acc:.3f}"</span>)

<span class="comment"># ============================================</span>
<span class="comment"># 8. Calibration Curve (Reliability)</span>
<span class="comment"># ============================================</span>
prob_true, prob_pred = calibration_curve(y_test, y_proba, n_bins=10)

plt.figure(figsize=(10,8))
plt.plot(prob_pred, prob_true, marker='o', linewidth=2, label='Model')
plt.plot([0, 1], [0, 1], linestyle='--', label='Perfect Calibration')
plt.xlabel('Mean Predicted Probability')
plt.ylabel('Fraction of Positives')
plt.title('Calibration Curve (Reliability Diagram)')
plt.legend()
plt.grid(True)
plt.show()

<span class="comment"># ============================================</span>
<span class="comment"># 9. Threshold Tuning</span>
<span class="comment"># ============================================</span>
thresholds = np.arange(0.1, 0.9, 0.05)
precisions = []
recalls = []
f1_scores = []

<span class="keyword">for</span> thresh <span class="keyword">in</span> thresholds:
    y_pred_thresh = (y_proba >= thresh).astype(int)
    precisions.append(precision_score(y_test, y_pred_thresh))
    recalls.append(recall_score(y_test, y_pred_thresh))
    f1_scores.append(f1_score(y_test, y_pred_thresh))

plt.figure(figsize=(12,6))
plt.plot(thresholds, precisions, label='Precision', marker='o')
plt.plot(thresholds, recalls, label='Recall', marker='s')
plt.plot(thresholds, f1_scores, label='F1 Score', marker='^')
plt.xlabel('Threshold')
plt.ylabel('Score')
plt.title('Metrics vs Threshold')
plt.legend()
plt.grid(True)
plt.show()

<span class="comment"># Find threshold that maximizes F1</span>
best_idx = np.argmax(f1_scores)
best_threshold = thresholds[best_idx]
<span class="keyword">print</span>(<span class="string">f"Best threshold by F1: {best_threshold:.2f}"</span>)

<span class="comment"># ============================================</span>
<span class="comment"># 10. Custom Metric Function</span>
<span class="comment"># ============================================</span>
<span class="keyword">def</span> cost_based_metric(y_true, y_pred, fp_cost=1, fn_cost=10):
    <span class="string">"""
    Custom metric incorporating business costs
    fp_cost: cost of false positive
    fn_cost: cost of false negative
    """</span>
    cm = confusion_matrix(y_true, y_pred)
    tn, fp, fn, tp = cm.ravel()
    total_cost = fp * fp_cost + fn * fn_cost
    <span class="keyword">return</span> -total_cost  <span class="comment"># Higher is better (negative cost)</span>

<span class="comment"># Use in cross-validation</span>
<span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score
cost_scores = cross_val_score(
    model, X, y, 
    scoring=<span class="function">make_scorer</span>(cost_based_metric, fp_cost=1, fn_cost=10)
)</code></pre>
    </div>
  </div>

  <!-- Quick Reference Card -->
  <div class="panel section">
    <h3><i class="fas fa-clipboard-list"></i> Metrics Cheat Sheet</h3>
    
    <table class="detailed-table">
      <thead>
        <tr><th>Metric</th><th>Formula</th><th>Range</th><th>Best Use</th></tr>
      </thead>
      <tbody>
        <tr><td>Accuracy</td><td>(TP+TN)/N</td><td>0-1</td><td>Balanced classes</td></tr>
        <tr><td>Precision</td><td>TP/(TP+FP)</td><td>0-1</td><td>When FP costly</td></tr>
        <tr><td>Recall</td><td>TP/(TP+FN)</td><td>0-1</td><td>When FN costly</td></tr>
        <tr><td>F1</td><td>2√óP√óR/(P+R)</td><td>0-1</td><td>Imbalanced, balance needed</td></tr>
        <tr><td>Specificity</td><td>TN/(TN+FP)</td><td>0-1</td><td>Negative class accuracy</td></tr>
        <tr><td>ROC-AUC</td><td>Area under ROC</td><td>0.5-1</td><td>Overall ranking ability</td></tr>
        <tr><td>PR-AUC</td><td>Area under PR</td><td>0-1</td><td>Highly imbalanced</td></tr>
        <tr><td>Log Loss</td><td>-‚àë(y log p + (1-y)log(1-p))</td><td>0-‚àû</td><td>Probability calibration</td></tr>
        <tr><td>Cohen's Kappa</td><td>(p‚ÇÄ-p‚Çë)/(1-p‚Çë)</td><td>-1 to 1</td><td>Chance-adjusted agreement</td></tr>
        <tr><td>MCC</td><td>Complex</td><td>-1 to 1</td><td>Very imbalanced</td></tr>
      </tbody>
    </table>
  </div>
</div>

<script>
// Initialize ROC chart
function initROCChart() {
    const ctx = document.getElementById('rocChart');
    if (!ctx) return;
    if (rocChart) rocChart.destroy();
    
    // Generate ROC curve points
    const fpr = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1];
    const tpr = [0, 0.4, 0.65, 0.8, 0.88, 0.92, 0.95, 0.97, 0.99, 1, 1];
    
    rocChart = new Chart(ctx, {
        type: 'line',
        data: {
            datasets: [{
                label: 'ROC Curve (AUC = 0.92)',
                data: fpr.map((fp, i) => ({ x: fp, y: tpr[i] })),
                borderColor: getChartColors().success,
                borderWidth: 3,
                pointRadius: 0,
                fill: false
            }, {
                label: 'Random Classifier',
                data: [{ x: 0, y: 0 }, { x: 1, y: 1 }],
                borderColor: getChartColors().muted2,
                borderWidth: 2,
                borderDash: [5, 5],
                pointRadius: 0,
                fill: false
            }]
        },
        options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: { 
                legend: { labels: { color: getChartColors().text } } 
            },
            scales: {
                x: { 
                    title: { display: true, text: 'False Positive Rate (1 - Specificity)', color: getChartColors().text }, 
                    min: 0, max: 1, 
                    grid: { color: getChartColors().grid }, 
                    ticks: { color: getChartColors().text } 
                },
                y: { 
                    title: { display: true, text: 'True Positive Rate (Recall)', color: getChartColors().text }, 
                    min: 0, max: 1, 
                    grid: { color: getChartColors().grid }, 
                    ticks: { color: getChartColors().text } 
                }
            }
        }
    });
}

// Initialize when section loads
document.addEventListener('DOMContentLoaded', () => {
    // Check if we're on evaluation section
    const rocCanvas = document.getElementById('rocChart');
    if (rocCanvas) {
        initROCChart();
    }
});
</script>
    <!-- ================================================
         SECTION 9: HANDS-ON COMPARISON
    ================================================= -->
   <!-- ================================================
     SECTION 9: HANDS-ON COMPARISON - ENHANCED
================================================= -->
<div id="hands-on-class" class="module-content">

  <div class="panel section">
    <h3><i class="fas fa-laptop-code"></i> 9.1 Compare All Classifiers Side-by-Side</h3>

    <div class="info-box info">
      <h4><i class="fas fa-bullseye"></i> The Goal: Understand Trade-offs</h4>
      <p>
        No single classifier is best for all problems. Each has strengths and weaknesses.
        In this interactive demo, you can compare how different algorithms classify the same data
        and see their decision boundaries and performance metrics in real-time.
      </p>
      
      <div style="display: flex; gap: 20px; flex-wrap: wrap; margin-top: 15px; justify-content: center;">
        <span class="badge"><i class="fas fa-check-circle"></i> Compare boundaries</span>
        <span class="badge"><i class="fas fa-check-circle"></i> See metrics</span>
        <span class="badge"><i class="fas fa-check-circle"></i> Understand trade-offs</span>
        <span class="badge"><i class="fas fa-check-circle"></i> Pick the right tool</span>
      </div>
    </div>

    <div class="explanation-box">
      <p><strong>üìö In Simple Terms:</strong> Different classifiers are like different tools in a toolbox. You wouldn't use a hammer to screw in a nail, or a screwdriver to pound a nail. This demo helps you see which tool works best for different types of problems.</p>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-shapes"></i> 9.2 Interactive Classifier Comparison</h3>

    <div class="classification-container">
      <!-- Main visualization area -->
      <div class="classification-chart">
        <canvas id="compareClassChart"></canvas>
      </div>

      <!-- Controls and metrics -->
      <div class="classification-controls">
        <div class="control-group">
          <h4><i class="fas fa-cogs"></i> Pick a Classifier</h4>
          
          <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 10px; margin-bottom: 20px;">
            <button class="btn btn-secondary" onclick="compareClassifier('logistic')" style="justify-content: center;">
              <i class="fas fa-sigmoid"></i> Logistic
            </button>
            <button class="btn btn-secondary" onclick="compareClassifier('knn')" style="justify-content: center;">
              <i class="fas fa-users"></i> KNN (k=5)
            </button>
            <button class="btn btn-secondary" onclick="compareClassifier('nb')" style="justify-content: center;">
              <i class="fas fa-chart-pie"></i> Naive Bayes
            </button>
            <button class="btn btn-secondary" onclick="compareClassifier('svm')" style="justify-content: center;">
              <i class="fas fa-vector-square"></i> SVM (RBF)
            </button>
            <button class="btn btn-secondary" onclick="compareClassifier('tree')" style="justify-content: center;">
              <i class="fas fa-tree"></i> Decision Tree
            </button>
            <button class="btn btn-secondary" onclick="compareClassifier('rf')" style="justify-content: center;">
              <i class="fas fa-layer-group"></i> Random Forest
            </button>
          </div>
          
          <h4 style="margin-top: 20px;"><i class="fas fa-sliders-h"></i> Dataset Controls</h4>
          
          <div class="slider-container">
            <label><i class="fas fa-random"></i> Noise Level:</label>
            <input type="range" id="noiseLevel" min="0" max="0.5" step="0.05" value="0.1" onchange="regenerateData()">
            <span class="slider-value" id="noiseValue">0.10</span>
          </div>
          
          <div class="slider-container">
            <label><i class="fas fa-shapes"></i> Pattern:</label>
            <select id="dataPattern" class="btn btn-secondary" onchange="regenerateData()">
              <option value="circles">Concentric Circles</option>
              <option value="moons">Interleaving Moons</option>
              <option value="blobs" selected>Gaussian Blobs</option>
              <option value="xor">XOR Pattern</option>
              <option value="linearly">Linearly Separable</option>
            </select>
          </div>
          
          <div class="slider-container">
            <label><i class="fas fa-balance-scale"></i> Class Imbalance:</label>
            <input type="range" id="classBalance" min="0.1" max="1" step="0.1" value="1" onchange="regenerateData()">
            <span class="slider-value" id="balanceValue">1.0</span>
          </div>
          
          <div class="button-group" style="margin-top: 15px;">
            <button class="btn btn-primary" onclick="regenerateData()">
              <i class="fas fa-sync"></i> Generate New Data
            </button>
          </div>
        </div>
      </div>
    </div>

    <!-- Metrics Display -->
    <div class="metrics-container" style="margin-top: 20px;">
      <div class="metric-card">
        <div class="metric-label">
          <i class="fas fa-check-circle" style="color: var(--success);"></i> Accuracy
        </div>
        <div class="metric-value" id="compAcc">0.85</div>
      </div>
      <div class="metric-card">
        <div class="metric-label">
          <i class="fas fa-bullseye" style="color: var(--primary);"></i> Precision
        </div>
        <div class="metric-value" id="compPrec">0.83</div>
      </div>
      <div class="metric-card">
        <div class="metric-label">
          <i class="fas fa-search" style="color: var(--warning);"></i> Recall
        </div>
        <div class="metric-value" id="compRec">0.81</div>
      </div>
      <div class="metric-card">
        <div class="metric-label">
          <i class="fas fa-balance-scale" style="color: var(--accent);"></i> F1 Score
        </div>
        <div class="metric-value" id="compF1">0.82</div>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-clipboard-list"></i> 9.3 Classifier Characteristics & Trade-offs</h3>

    <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 20px;">
      <!-- Logistic Regression Card -->
      <div class="algorithm-card" style="cursor: pointer;" onclick="compareClassifier('logistic')">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-sigmoid"></i></div>
          <div class="algorithm-title">
            <h4>Logistic Regression</h4>
          </div>
        </div>
        <div class="algorithm-body">
          <div class="algorithm-metric">
            <span>Decision Boundary:</span>
            <span>Linear</span>
          </div>
          <div class="algorithm-metric">
            <span>Interpretability:</span>
            <span>‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (High)</span>
          </div>
          <div class="algorithm-metric">
            <span>Training Speed:</span>
            <span>‚ö°‚ö°‚ö°‚ö°‚ö° (Fast)</span>
          </div>
          <div class="algorithm-metric">
            <span>Prediction Speed:</span>
            <span>‚ö°‚ö°‚ö°‚ö°‚ö° (Fast)</span>
          </div>
          <div class="algorithm-metric">
            <span>Handles Non-linearity:</span>
            <span>‚ùå No</span>
          </div>
          <div class="algorithm-metric">
            <span>Handles High Dimensions:</span>
            <span>‚úÖ Yes (with regularization)</span>
          </div>
          <p style="margin-top: 10px; font-size: 12px; color: var(--muted);">
            <strong>Best for:</strong> Interpretable baselines, well-calibrated probabilities, linear relationships
          </p>
        </div>
      </div>

      <!-- KNN Card -->
      <div class="algorithm-card" style="cursor: pointer;" onclick="compareClassifier('knn')">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-users"></i></div>
          <div class="algorithm-title">
            <h4>K-Nearest Neighbors</h4>
          </div>
        </div>
        <div class="algorithm-body">
          <div class="algorithm-metric">
            <span>Decision Boundary:</span>
            <span>Non-linear, flexible</span>
          </div>
          <div class="algorithm-metric">
            <span>Interpretability:</span>
            <span>‚≠ê‚≠ê (Low)</span>
          </div>
          <div class="algorithm-metric">
            <span>Training Speed:</span>
            <span>‚ö°‚ö°‚ö°‚ö°‚ö° (Instant)</span>
          </div>
          <div class="algorithm-metric">
            <span>Prediction Speed:</span>
            <span>üê¢ (Slow for large data)</span>
          </div>
          <div class="algorithm-metric">
            <span>Handles Non-linearity:</span>
            <span>‚úÖ Yes</span>
          </div>
          <div class="algorithm-metric">
            <span>Handles High Dimensions:</span>
            <span>‚ùå No (curse of dimensionality)</span>
          </div>
          <p style="margin-top: 10px; font-size: 12px; color: var(--muted);">
            <strong>Best for:</strong> Small datasets, complex boundaries, when training is cheap but prediction can be slow
          </p>
        </div>
      </div>

      <!-- Naive Bayes Card -->
      <div class="algorithm-card" style="cursor: pointer;" onclick="compareClassifier('nb')">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-chart-pie"></i></div>
          <div class="algorithm-title">
            <h4>Naive Bayes</h4>
          </div>
        </div>
        <div class="algorithm-body">
          <div class="algorithm-metric">
            <span>Decision Boundary:</span>
            <span>Quadratic/Complex</span>
          </div>
          <div class="algorithm-metric">
            <span>Interpretability:</span>
            <span>‚≠ê‚≠ê‚≠ê (Moderate)</span>
          </div>
          <div class="algorithm-metric">
            <span>Training Speed:</span>
            <span>‚ö°‚ö°‚ö°‚ö°‚ö° (Very Fast)</span>
          </div>
          <div class="algorithm-metric">
            <span>Prediction Speed:</span>
            <span>‚ö°‚ö°‚ö°‚ö°‚ö° (Very Fast)</span>
          </div>
          <div class="algorithm-metric">
            <span>Handles Non-linearity:</span>
            <span>‚úÖ Yes (via distributions)</span>
          </div>
          <div class="algorithm-metric">
            <span>Handles High Dimensions:</span>
            <span>‚úÖ‚úÖ Excellent</span>
          </div>
          <p style="margin-top: 10px; font-size: 12px; color: var(--muted);">
            <strong>Best for:</strong> Text classification, high-dimensional data, when speed is critical
          </p>
        </div>
      </div>

      <!-- SVM Card -->
      <div class="algorithm-card" style="cursor: pointer;" onclick="compareClassifier('svm')">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-vector-square"></i></div>
          <div class="algorithm-title">
            <h4>Support Vector Machine</h4>
          </div>
        </div>
        <div class="algorithm-body">
          <div class="algorithm-metric">
            <span>Decision Boundary:</span>
            <span>Varies by kernel</span>
          </div>
          <div class="algorithm-metric">
            <span>Interpretability:</span>
            <span>‚≠ê (Low, especially with kernels)</span>
          </div>
          <div class="algorithm-metric">
            <span>Training Speed:</span>
            <span>üê¢ (O(n¬≤) to O(n¬≥))</span>
          </div>
          <div class="algorithm-metric">
            <span>Prediction Speed:</span>
            <span>‚ö°‚ö° (Depends on support vectors)</span>
          </div>
          <div class="algorithm-metric">
            <span>Handles Non-linearity:</span>
            <span>‚úÖ‚úÖ Excellent (with kernels)</span>
          </div>
          <div class="algorithm-metric">
            <span>Handles High Dimensions:</span>
            <span>‚úÖ Good</span>
          </div>
          <p style="margin-top: 10px; font-size: 12px; color: var(--muted);">
            <strong>Best for:</strong> Medium-sized datasets, complex boundaries, when you need max-margin separation
          </p>
        </div>
      </div>

      <!-- Decision Tree Card -->
      <div class="algorithm-card" style="cursor: pointer;" onclick="compareClassifier('tree')">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-tree"></i></div>
          <div class="algorithm-title">
            <h4>Decision Tree</h4>
          </div>
        </div>
        <div class="algorithm-body">
          <div class="algorithm-metric">
            <span>Decision Boundary:</span>
            <span>Axis-aligned rectangles</span>
          </div>
          <div class="algorithm-metric">
            <span>Interpretability:</span>
            <span>‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Very High)</span>
          </div>
          <div class="algorithm-metric">
            <span>Training Speed:</span>
            <span>‚ö°‚ö°‚ö° (Fast)</span>
          </div>
          <div class="algorithm-metric">
            <span>Prediction Speed:</span>
            <span>‚ö°‚ö°‚ö°‚ö° (Fast)</span>
          </div>
          <div class="algorithm-metric">
            <span>Handles Non-linearity:</span>
            <span>‚úÖ Yes</span>
          </div>
          <div class="algorithm-metric">
            <span>Handles High Dimensions:</span>
            <span>‚ö°‚ö° (Can overfit)</span>
          </div>
          <p style="margin-top: 10px; font-size: 12px; color: var(--muted);">
            <strong>Best for:</strong> When you need interpretable rules, mixed data types, quick prototypes
          </p>
        </div>
      </div>

      <!-- Random Forest Card -->
      <div class="algorithm-card" style="cursor: pointer;" onclick="compareClassifier('rf')">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-layer-group"></i></div>
          <div class="algorithm-title">
            <h4>Random Forest</h4>
          </div>
        </div>
        <div class="algorithm-body">
          <div class="algorithm-metric">
            <span>Decision Boundary:</span>
            <span>Smooth, complex</span>
          </div>
          <div class="algorithm-metric">
            <span>Interpretability:</span>
            <span>‚≠ê‚≠ê (Feature importance only)</span>
          </div>
          <div class="algorithm-metric">
            <span>Training Speed:</span>
            <span>‚ö°‚ö° (Parallel, but many trees)</span>
          </div>
          <div class="algorithm-metric">
            <span>Prediction Speed:</span>
            <span>‚ö°‚ö° (Average of many trees)</span>
          </div>
          <div class="algorithm-metric">
            <span>Handles Non-linearity:</span>
            <span>‚úÖ‚úÖ Excellent</span>
          </div>
          <div class="algorithm-metric">
            <span>Handles High Dimensions:</span>
            <span>‚úÖ Good (feature sampling helps)</span>
          </div>
          <p style="margin-top: 10px; font-size: 12px; color: var(--muted);">
            <strong>Best for:</strong> Robust performance out-of-box, when you don't want to tune much
          </p>
        </div>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-chart-line"></i> 9.4 Performance on Different Data Patterns</h3>

    <div class="expanded-content">
      <h4>How Algorithms Handle Different Data Shapes</h4>
      
      <table class="detailed-table">
        <thead>
          <tr><th>Pattern</th><th>Logistic</th><th>KNN</th><th>Naive Bayes</th><th>SVM (RBF)</th><th>Tree</th><th>Random Forest</th></tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>Linear Separable</strong></td>
            <td class="true-positive">‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ</td>
            <td class="true-positive">‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ</td>
            <td class="true-positive">‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ</td>
            <td class="true-positive">‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ</td>
            <td class="true-positive">‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ</td>
            <td class="true-positive">‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ</td>
          </tr>
          <tr>
            <td><strong>Circles (Non-linear)</strong></td>
            <td class="false-negative">‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ</td>
            <td class="true-positive">‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ</td>
            <td class="warning">‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ</td>
            <td class="true-positive">‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ</td>
            <td class="true-positive">‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ</td>
            <td class="true-positive">‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ</td>
          </tr>
          <tr>
            <td><strong>Moons (Interleaving)</strong></td>
            <td class="false-negative">‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ</td>
            <td class="true-positive">‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ</td>
            <td class="warning">‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ</td>
            <td class="true-positive">‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ</td>
            <td class="true-positive">‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ</td>
            <td class="true-positive">‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ</td>
          </tr>
          <tr>
            <td><strong>XOR Pattern</strong></td>
            <td class="false-negative">‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ</td>
            <td class="true-positive">‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ</td>
            <td class="false-negative">‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ</td>
            <td class="true-positive">‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ</td>
            <td class="true-positive">‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ</td>
            <td class="true-positive">‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ</td>
          </tr>
          <tr>
            <td><strong>High Dimensional</strong></td>
            <td class="true-positive">‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ</td>
            <td class="false-negative">‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ</td>
            <td class="true-positive">‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ</td>
            <td class="warning">‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ</td>
            <td class="warning">‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ</td>
            <td class="true-positive">‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ</td>
          </tr>
          <tr>
            <td><strong>Imbalanced Classes</strong></td>
            <td class="warning">‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ</td>
            <td class="warning">‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ</td>
            <td class="warning">‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ</td>
            <td class="warning">‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ</td>
            <td class="warning">‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ</td>
            <td class="true-positive">‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ</td>
          </tr>
        </tbody>
      </table>
      
      <div style="margin-top: 15px; display: flex; gap: 20px; justify-content: center;">
        <span><span style="color: var(--success);">‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ</span> Excellent</span>
        <span><span style="color: var(--success);">‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ</span> Good</span>
        <span><span style="color: var(--warning);">‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ</span> Fair</span>
        <span><span style="color: var(--warning);">‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ</span> Poor</span>
        <span><span style="color: var(--danger);">‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ</span> Very Poor</span>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-tachometer-alt"></i> 9.5 Speed & Complexity Comparison</h3>

    <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px;">
      <div>
        <h4>Training Time</h4>
        <div style="background: var(--panel2); padding: 15px; border-radius: var(--radius);">
          <div style="margin: 10px 0;">
            <div style="display: flex; justify-content: space-between;">
              <span>Logistic Regression:</span>
              <span>O(n √ó p)</span>
            </div>
            <div style="height: 15px; background: var(--border); border-radius: 5px;">
              <div style="width: 20%; height: 100%; background: var(--success); border-radius: 5px;"></div>
            </div>
          </div>
          
          <div style="margin: 10px 0;">
            <div style="display: flex; justify-content: space-between;">
              <span>KNN:</span>
              <span>O(1) (just storing!)</span>
            </div>
            <div style="height: 15px; background: var(--border); border-radius: 5px;">
              <div style="width: 5%; height: 100%; background: var(--success); border-radius: 5px;"></div>
            </div>
          </div>
          
          <div style="margin: 10px 0;">
            <div style="display: flex; justify-content: space-between;">
              <span>Naive Bayes:</span>
              <span>O(n √ó p)</span>
            </div>
            <div style="height: 15px; background: var(--border); border-radius: 5px;">
              <div style="width: 15%; height: 100%; background: var(--success); border-radius: 5px;"></div>
            </div>
          </div>
          
          <div style="margin: 10px 0;">
            <div style="display: flex; justify-content: space-between;">
              <span>SVM (RBF):</span>
              <span>O(n¬≤) to O(n¬≥)</span>
            </div>
            <div style="height: 15px; background: var(--border); border-radius: 5px;">
              <div style="width: 90%; height: 100%; background: var(--danger); border-radius: 5px;"></div>
            </div>
          </div>
          
          <div style="margin: 10px 0;">
            <div style="display: flex; justify-content: space-between;">
              <span>Decision Tree:</span>
              <span>O(n √ó p √ó log n)</span>
            </div>
            <div style="height: 15px; background: var(--border); border-radius: 5px;">
              <div style="width: 40%; height: 100%; background: var(--warning); border-radius: 5px;"></div>
            </div>
          </div>
          
          <div style="margin: 10px 0;">
            <div style="display: flex; justify-content: space-between;">
              <span>Random Forest:</span>
              <span>O(m √ó n √ó p √ó log n)</span>
            </div>
            <div style="height: 15px; background: var(--border); border-radius: 5px;">
              <div style="width: 70%; height: 100%; background: var(--warning); border-radius: 5px;"></div>
            </div>
          </div>
        </div>
      </div>
      
      <div>
        <h4>Prediction Time</h4>
        <div style="background: var(--panel2); padding: 15px; border-radius: var(--radius);">
          <div style="margin: 10px 0;">
            <div style="display: flex; justify-content: space-between;">
              <span>Logistic Regression:</span>
              <span>O(p)</span>
            </div>
            <div style="height: 15px; background: var(--border); border-radius: 5px;">
              <div style="width: 10%; height: 100%; background: var(--success); border-radius: 5px;"></div>
            </div>
          </div>
          
          <div style="margin: 10px 0;">
            <div style="display: flex; justify-content: space-between;">
              <span>KNN:</span>
              <span>O(n √ó p)</span>
            </div>
            <div style="height: 15px; background: var(--border); border-radius: 5px;">
              <div style="width: 100%; height: 100%; background: var(--danger); border-radius: 5px;"></div>
            </div>
          </div>
          
          <div style="margin: 10px 0;">
            <div style="display: flex; justify-content: space-between;">
              <span>Naive Bayes:</span>
              <span>O(c √ó p)</span>
            </div>
            <div style="height: 15px; background: var(--border); border-radius: 5px;">
              <div style="width: 10%; height: 100%; background: var(--success); border-radius: 5px;"></div>
            </div>
          </div>
          
          <div style="margin: 10px 0;">
            <div style="display: flex; justify-content: space-between;">
              <span>SVM (RBF):</span>
              <span>O(n_sv √ó p)</span>
            </div>
            <div style="height: 15px; background: var(--border); border-radius: 5px;">
              <div style="width: 30%; height: 100%; background: var(--warning); border-radius: 5px;"></div>
            </div>
          </div>
          
          <div style="margin: 10px 0;">
            <div style="display: flex; justify-content: space-between;">
              <span>Decision Tree:</span>
              <span>O(depth)</span>
            </div>
            <div style="height: 15px; background: var(--border); border-radius: 5px;">
              <div style="width: 15%; height: 100%; background: var(--success); border-radius: 5px;"></div>
            </div>
          </div>
          
          <div style="margin: 10px 0;">
            <div style="display: flex; justify-content: space-between;">
              <span>Random Forest:</span>
              <span>O(m √ó depth)</span>
            </div>
            <div style="height: 15px; background: var(--border); border-radius: 5px;">
              <div style="width: 30%; height: 100%; background: var(--warning); border-radius: 5px;"></div>
            </div>
          </div>
        </div>
      </div>
    </div>
    
    <p style="margin-top: 15px; font-size: 12px;">
      <strong>Legend:</strong> n = samples, p = features, c = classes, m = trees, n_sv = support vectors
    </p>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-clipboard-check"></i> 9.6 Algorithm Selection Guide</h3>

    <div class="info-box success">
      <h4>Decision Tree for Algorithm Selection</h4>
      
      <div style="display: flex; justify-content: center; margin: 30px 0;">
        <svg width="800" height="500" viewBox="0 0 800 500">
          <!-- Start -->
          <rect x="350" y="20" width="100" height="40" rx="10" fill="var(--primary)" />
          <text x="400" y="45" text-anchor="middle" fill="white" font-size="12">Start</text>
          
          <!-- Question 1: Interpretability? -->
          <line x1="400" y1="60" x2="400" y2="100" stroke="var(--text)" stroke-width="2"/>
          <rect x="300" y="100" width="200" height="40" rx="10" fill="var(--warning)" />
          <text x="400" y="125" text-anchor="middle" fill="white" font-size="11">Need interpretability?</text>
          
          <!-- Yes branch -->
          <line x1="300" y1="120" x2="200" y2="150" stroke="var(--text)" stroke-width="2"/>
          <text x="220" y="140" fill="var(--text)" font-size="10">Yes</text>
          
          <!-- No branch -->
          <line x1="500" y1="120" x2="600" y2="150" stroke="var(--text)" stroke-width="2"/>
          <text x="580" y="140" fill="var(--text)" font-size="10">No</text>
          
          <!-- Interpretable options -->
          <rect x="100" y="160" width="200" height="40" rx="10" fill="var(--success)" />
          <text x="200" y="185" text-anchor="middle" fill="white" font-size="11">Decision Tree</text>
          
          <rect x="100" y="220" width="200" height="40" rx="10" fill="var(--success)" />
          <text x="200" y="245" text-anchor="middle" fill="white" font-size="11">Logistic Regression</text>
          
          <rect x="100" y="280" width="200" height="40" rx="10" fill="var(--success)" />
          <text x="200" y="305" text-anchor="middle" fill="white" font-size="11">Rule-based systems</text>
          
          <!-- Non-interpretable branch -->
          <rect x="500" y="160" width="200" height="40" rx="10" fill="var(--warning)" />
          <text x="600" y="185" text-anchor="middle" fill="white" font-size="11">Data size?</text>
          
          <!-- Small data -->
          <line x1="500" y1="200" x2="400" y2="240" stroke="var(--text)" stroke-width="2"/>
          <text x="420" y="220" fill="var(--text)" font-size="10">Small</text>
          
          <!-- Medium data -->
          <line x1="600" y1="200" x2="600" y2="240" stroke="var(--text)" stroke-width="2"/>
          <text x="620" y="220" fill="var(--text)" font-size="10">Medium</text>
          
          <!-- Large data -->
          <line x1="700" y1="200" x2="800" y2="240" stroke="var(--text)" stroke-width="2"/>
          <text x="780" y="220" fill="var(--text)" font-size="10">Large</text>
          
          <!-- Small data options -->
          <rect x="300" y="250" width="200" height="40" rx="10" fill="var(--success)" />
          <text x="400" y="275" text-anchor="middle" fill="white" font-size="11">SVM (RBF)</text>
          
          <rect x="300" y="310" width="200" height="40" rx="10" fill="var(--success)" />
          <text x="400" y="335" text-anchor="middle" fill="white" font-size="11">KNN (if p small)</text>
          
          <!-- Medium data options -->
          <rect x="500" y="250" width="200" height="40" rx="10" fill="var(--success)" />
          <text x="600" y="275" text-anchor="middle" fill="white" font-size="11">Random Forest</text>
          
          <rect x="500" y="310" width="200" height="40" rx="10" fill="var(--success)" />
          <text x="600" y="335" text-anchor="middle" fill="white" font-size="11">XGBoost</text>
          
          <!-- Large data options -->
          <rect x="700" y="250" width="200" height="40" rx="10" fill="var(--success)" />
          <text x="800" y="275" text-anchor="middle" fill="white" font-size="11">Linear models</text>
          
          <rect x="700" y="310" width="200" height="40" rx="10" fill="var(--success)" />
          <text x="800" y="335" text-anchor="middle" fill="white" font-size="11">Neural Networks</text>
          
          <rect x="700" y="370" width="200" height="40" rx="10" fill="var(--success)" />
          <text x="800" y="395" text-anchor="middle" fill="white" font-size="11">LightGBM</text>
        </svg>
      </div>
      
      <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 20px; margin-top: 20px;">
        <div>
          <h5>If interpretability matters:</h5>
          <ul>
            <li>Decision Tree (if simple rules)</li>
            <li>Logistic Regression (if linear)</li>
            <li>Naive Bayes (probabilistic)</li>
          </ul>
        </div>
        
        <div>
          <h5>If accuracy is paramount:</h5>
          <ul>
            <li>Random Forest (robust)</li>
            <li>XGBoost (competition winner)</li>
            <li>SVM with RBF (if data small)</li>
          </ul>
        </div>
        
        <div>
          <h5>If speed is critical:</h5>
          <ul>
            <li>Naive Bayes (fastest)</li>
            <li>Linear models</li>
            <li>Decision Trees</li>
          </ul>
        </div>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-code"></i> 9.7 Code for Comparing Multiple Classifiers</h3>

    <div class="code-editor-container">
      <div class="code-header">
        <div class="code-title"><i class="fab fa-python"></i> Python: Compare All Classifiers</div>
      </div>
      <div class="code-body">
        <pre class="code-block"><code><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, cross_val_score
<span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression
<span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier
<span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB
<span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC
<span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier
<span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier, GradientBoostingClassifier
<span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, precision_score, recall_score, f1_score
<span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification, make_circles, make_moons
<span class="keyword">import</span> pandas <span class="keyword">as</span> pd
<span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt
<span class="keyword">import</span> numpy <span class="keyword">as</span> np
<span class="keyword">import</span> warnings
warnings.filterwarnings('ignore')

<span class="comment"># ============================================</span>
<span class="comment"># 1. Generate different datasets</span>
<span class="comment"># ============================================</span>
np.random.seed(42)

datasets = {
    <span class="string">'Linearly Separable'</span>: make_classification(
        n_samples=500, n_features=2, n_redundant=0, 
        n_clusters_per_class=1, random_state=42
    ),
    <span class="string">'Circles'</span>: make_circles(n_samples=500, noise=0.1, factor=0.5, random_state=42),
    <span class="string">'Moons'</span>: make_moons(n_samples=500, noise=0.2, random_state=42),
    <span class="string">'XOR-like'</span>: make_classification(
        n_samples=500, n_features=2, n_redundant=0, 
        n_clusters_per_class=2, flip_y=0.1, random_state=42
    )
}

<span class="comment"># ============================================</span>
<span class="comment"># 2. Define classifiers to compare</span>
<span class="comment"># ============================================</span>
classifiers = {
    <span class="string">'Logistic Regression'</span>: LogisticRegression(random_state=42),
    <span class="string">'KNN (k=5)'</span>: KNeighborsClassifier(n_neighbors=5),
    <span class="string">'Naive Bayes'</span>: GaussianNB(),
    <span class="string">'SVM (RBF)'</span>: SVC(kernel='rbf', random_state=42),
    <span class="string">'Decision Tree'</span>: DecisionTreeClassifier(max_depth=5, random_state=42),
    <span class="string">'Random Forest'</span>: RandomForestClassifier(n_estimators=100, random_state=42),
    <span class="string">'Gradient Boosting'</span>: GradientBoostingClassifier(n_estimators=100, random_state=42)
}

<span class="comment"># ============================================</span>
<span class="comment"># 3. Compare across datasets</span>
<span class="comment"># ============================================</span>
results = []

<span class="keyword">for</span> dataset_name, (X, y) <span class="keyword">in</span> datasets.items():
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )
    
    <span class="keyword">for</span> clf_name, clf <span class="keyword">in</span> classifiers.items():
        <span class="comment"># Train and predict</span>
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        
        <span class="comment"># Calculate metrics</span>
        acc = accuracy_score(y_test, y_pred)
        
        <span class="comment"># Handle potential warnings for metrics</span>
        <span class="keyword">try</span>:
            prec = precision_score(y_test, y_pred, average='weighted')
            rec = recall_score(y_test, y_pred, average='weighted')
            f1 = f1_score(y_test, y_pred, average='weighted')
        <span class="keyword">except</span>:
            prec = rec = f1 = np.nan
        
        <span class="comment"># Cross-validation score</span>
        cv_scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')
        
        results.append({
            <span class="string">'Dataset'</span>: dataset_name,
            <span class="string">'Classifier'</span>: clf_name,
            <span class="string">'Accuracy'</span>: acc,
            <span class="string">'Precision'</span>: prec,
            <span class="string">'Recall'</span>: rec,
            <span class="string">'F1'</span>: f1,
            <span class="string">'CV Mean'</span>: cv_scores.mean(),
            <span class="string">'CV Std'</span>: cv_scores.std()
        })

<span class="comment"># Convert to DataFrame</span>
results_df = pd.DataFrame(results)

<span class="comment"># ============================================</span>
<span class="comment"># 4. Display results</span>
<span class="comment"># ============================================</span>
<span class="keyword">print</span>(<span class="string">"COMPARISON RESULTS"</span>)
<span class="keyword">print</span>(<span class="string">"="</span>*80)

<span class="keyword">for</span> dataset <span class="keyword">in</span> datasets.keys():
    <span class="keyword">print</span>(<span class="string">f"\n{dataset} Dataset:"</span>)
    dataset_results = results_df[results_df['Dataset'] == dataset] \
        .sort_values('F1', ascending=False)[[<span class="string">'Classifier'</span>, <span class="string">'Accuracy'</span>, <span class="string">'F1'</span>, <span class="string">'CV Mean'</span>]]
    <span class="keyword">print</span>(dataset_results.to_string(index=False))

<span class="comment"># ============================================</span>
<span class="comment"># 5. Visualization: Bar plot comparison</span>
<span class="comment"># ============================================</span>
fig, axes = plt.subplots(2, 2, figsize=(15, 12))
axes = axes.ravel()

<span class="keyword">for</span> idx, dataset <span class="keyword">in</span> enumerate(datasets.keys()):
    ax = axes[idx]
    dataset_results = results_df[results_df['Dataset'] == dataset].sort_values('F1')
    
    ax.barh(dataset_results['Classifier'], dataset_results['F1'])
    ax.set_xlabel('F1 Score')
    ax.set_title(f'{dataset} Dataset')
    ax.set_xlim(0, 1)
    ax.grid(True, axis='x', alpha=0.3)

plt.tight_layout()
plt.show()

<span class="comment"># ============================================</span>
<span class="comment"># 6. Best classifier for each dataset</span>
<span class="comment"># ============================================</span>
best_for_each = results_df.loc[results_df.groupby('Dataset')['F1'].idxmax()] \
    [[<span class="string">'Dataset'</span>, <span class="string">'Classifier'</span>, <span class="string">'F1'</span>]]

<span class="keyword">print</span>(<span class="string">"\nBest Classifier for Each Dataset:"</span>)
<span class="keyword">print</span>(best_for_each.to_string(index=False))

<span class="comment"># ============================================</span>
<span class="comment"># 7. Statistical comparison (Friedman test)</span>
<span class="comment"># ============================================</span>
<span class="keyword">from</span> scipy.stats <span class="keyword">import</span> friedmanchisquare
<span class="keyword">import</span> scikit_posthocs <span class="keyword">as</span> sp

<span class="comment"># Create matrix of F1 scores: datasets √ó classifiers</span>
f1_matrix = results_df.pivot(index='Dataset', columns='Classifier', values='F1').values

<span class="comment"># Friedman test</span>
stat, p = friedmanchisquare(*[f1_matrix[:, i] <span class="keyword">for</span> i <span class="keyword">in</span> range(f1_matrix.shape[1])])
<span class="keyword">print</span>(<span class="string">f"\nFriedman test p-value: {p:.4f}"</span>)

<span class="keyword">if</span> p < 0.05:
    <span class="keyword">print</span>(<span class="string">"Significant differences found between classifiers"</span>)
    
    <span class="comment"># Nemenyi post-hoc test</span>
    <span class="comment"># nemenyi = sp.posthoc_nemenyi_friedman(f1_matrix.T)</span>
    <span class="comment"># print("\nNemenyi post-hoc test:")</span>
    <span class="comment"># print(nemenyi)</span>

<span class="comment"># ============================================</span>
<span class="comment"># 8. Create a ranking table</span>
<span class="comment"># ============================================</span>
ranking = results_df.groupby('Classifier').agg({
    'F1': ['mean', 'std', 'max', 'min']
}).round(3)
ranking.columns = ['Mean F1', 'Std F1', 'Max F1', 'Min F1']
ranking = ranking.sort_values('Mean F1', ascending=False)

<span class="keyword">print</span>(<span class="string">"\nOverall Classifier Rankings:"</span>)
<span class="keyword">print</span>(ranking)

<span class="comment"># ============================================</span>
<span class="comment"># 9. Parallel coordinates plot</span>
<span class="comment"># ============================================</span>
<span class="keyword">from</span> pandas.plotting <span class="keyword">import</span> parallel_coordinates

<span class="comment"># Prepare data for parallel coordinates</span>
plot_df = results_df.pivot(index='Dataset', columns='Classifier', values='F1').reset_index()
plot_df['Dataset'] = plot_df['Dataset'].astype('category')

plt.figure(figsize=(12, 6))
parallel_coordinates(plot_df, 'Dataset', colormap='Set1')
plt.title('Classifier Performance Across Datasets')
plt.ylabel('F1 Score')
plt.legend(bbox_to_anchor=(1.05, 1))
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

<span class="comment"># ============================================</span>
<span class="comment"># 10. Summary recommendations</span>
<span class="comment"># ============================================</span>
<span class="keyword">print</span>(<span class="string">"\n"</span> + <span class="string">"="</span>*60)
<span class="keyword">print</span>(<span class="string">"PRACTICAL RECOMMENDATIONS"</span>)
<span class="keyword">print</span>(<span class="string">"="</span>*60)

recommendations = {
    <span class="string">'Linear Separable'</span>: <span class="string">'Logistic Regression (simple, interpretable)'</span>,
    <span class="string">'Circles (Non-linear)'</span>: <span class="string">'SVM with RBF or Random Forest'</span>,
    <span class="string">'High Dimensional'</span>: <span class="string">'Naive Bayes or Linear models with regularization'</span>,
    <span class="string">'Large Dataset (&gt;100k)'</span>: <span class="string">'Linear models, SGDClassifier, or LightGBM'</span>,
    <span class="string">'Small Dataset (&lt;1000)'</span>: <span class="string">'SVM with RBF or simple trees'</span>,
    <span class="string">'Interpretability Critical'</span>: <span class="string">'Decision Tree or Logistic Regression'</span>,
    <span class="string">'Maximum Accuracy'</span>: <span class="string">'XGBoost or Random Forest (after tuning)'</span>,
    <span class="string">'Text Classification'</span>: <span class="string">'Naive Bayes (Multinomial) or Linear SVM'</span>,
    <span class="string">'Real-time Predictions'</span>: <span class="string">'Naive Bayes or Logistic Regression'</span>,
    <span class="string">'Mixed Data Types'</span>: <span class="string">'Random Forest or Gradient Boosting'</span>
}

<span class="keyword">for</span> scenario, recommendation <span class="keyword">in</span> recommendations.items():
    <span class="keyword">print</span>(<span class="string">f"{scenario:25}: {recommendation}"</span>)</code></pre>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-graduation-cap"></i> 9.8 Key Takeaways</h3>

    <div class="concept-grid">
      <div class="concept-card">
        <h5>üìä No Free Lunch Theorem</h5>
        <p>No single classifier is best for all problems. The best algorithm depends on your specific data and requirements.</p>
      </div>
      
      <div class="concept-card">
        <h5>‚öñÔ∏è Trade-offs Matter</h5>
        <p>Every choice involves trade-offs: interpretability vs accuracy, speed vs performance, simplicity vs complexity.</p>
      </div>
      
      <div class="concept-card">
        <h5>üß™ Experimentation is Key</h5>
        <p>Always try multiple algorithms with cross-validation. What works in theory may not work on your data.</p>
      </div>
      
      <div class="concept-card">
        <h5>üìà Start Simple</h5>
        <p>Begin with simple models (logistic regression, naive Bayes) as baselines. Only add complexity if needed.</p>
      </div>
    </div>

    <div style="margin-top: 30px; text-align: center; padding: 20px; background: var(--panel2); border-radius: var(--radius);">
      <p style="font-size: 1.2rem;">
        <i class="fas fa-lightbulb" style="color: var(--warning);"></i> 
        <strong>Remember:</strong> The best classifier is the one that works best on <span class="highlight">your specific problem</span> 
        with <span class="highlight">your specific constraints</span> (interpretability, speed, accuracy, etc.)
      </p>
    </div>
  </div>

  <script>
  // Initialize comparison chart
  function initCompareClassChart() {
      const ctx = document.getElementById('compareClassChart');
      if (!ctx) return;
      if (compareClassChart) compareClassChart.destroy();
      
      compareClassChart = new Chart(ctx, {
          type: 'scatter',
          data: {
              datasets: [{
                  label: 'Class 0',
                  data: [],
                  backgroundColor: getChartColors().primary,
                  pointRadius: 5,
                  pointHoverRadius: 8
              }, {
                  label: 'Class 1',
                  data: [],
                  backgroundColor: getChartColors().danger,
                  pointRadius: 5,
                  pointHoverRadius: 8
              }]
          },
          options: {
              responsive: true,
              maintainAspectRatio: false,
              plugins: {
                  legend: { 
                      labels: { color: getChartColors().text },
                      position: 'top'
                  },
                  tooltip: {
                      callbacks: {
                          label: function(context) {
                              return `${context.dataset.label}: (${context.parsed.x.toFixed(2)}, ${context.parsed.y.toFixed(2)})`;
                          }
                      }
                  }
              },
              scales: {
                  x: { 
                      title: { display: true, text: 'Feature 1', color: getChartColors().text }, 
                      grid: { color: getChartColors().grid }, 
                      ticks: { color: getChartColors().text } 
                  },
                  y: { 
                      title: { display: true, text: 'Feature 2', color: getChartColors().text }, 
                      grid: { color: getChartColors().grid }, 
                      ticks: { color: getChartColors().text } 
                  }
              }
          }
      });
  }

  // Generate data based on selected pattern
  function generateCompareData() {
      const pattern = document.getElementById('dataPattern').value;
      const noise = parseFloat(document.getElementById('noiseLevel').value);
      const balance = parseFloat(document.getElementById('classBalance').value);
      
      document.getElementById('noiseValue').textContent = noise.toFixed(2);
      document.getElementById('balanceValue').textContent = balance.toFixed(1);
      
      let class0 = [];
      let class1 = [];
      let n_samples = 100;
      
      // Adjust class balance
      let n_class0 = Math.floor(n_samples * balance);
      let n_class1 = n_samples - n_class0;
      
      if (pattern === 'blobs') {
          // Gaussian blobs
          for (let i = 0; i < n_class0; i++) {
              class0.push({
                  x: 3 + Math.random() * 2 + (Math.random() - 0.5) * noise * 10,
                  y: 3 + Math.random() * 2 + (Math.random() - 0.5) * noise * 10
              });
          }
          for (let i = 0; i < n_class1; i++) {
              class1.push({
                  x: 7 + Math.random() * 2 + (Math.random() - 0.5) * noise * 10,
                  y: 7 + Math.random() * 2 + (Math.random() - 0.5) * noise * 10
              });
          }
      } else if (pattern === 'circles') {
          // Concentric circles
          for (let i = 0; i < n_class0; i++) {
              let angle = Math.random() * 2 * Math.PI;
              let radius = 2 + (Math.random() - 0.5) * noise * 5;
              class0.push({
                  x: Math.cos(angle) * radius + 5,
                  y: Math.sin(angle) * radius + 5
              });
          }
          for (let i = 0; i < n_class1; i++) {
              let angle = Math.random() * 2 * Math.PI;
              let radius = 4 + (Math.random() - 0.5) * noise * 5;
              class1.push({
                  x: Math.cos(angle) * radius + 5,
                  y: Math.sin(angle) * radius + 5
              });
          }
      } else if (pattern === 'moons') {
          // Interleaving moons
          for (let i = 0; i < n_class0; i++) {
              let angle = Math.random() * Math.PI;
              class0.push({
                  x: Math.cos(angle) * 3 + 3 + (Math.random() - 0.5) * noise * 5,
                  y: Math.sin(angle) * 2 + 3 + (Math.random() - 0.5) * noise * 5
              });
          }
          for (let i = 0; i < n_class1; i++) {
              let angle = Math.random() * Math.PI + Math.PI;
              class1.push({
                  x: Math.cos(angle) * 3 + 6 + (Math.random() - 0.5) * noise * 5,
                  y: Math.sin(angle) * 2 + 4 + (Math.random() - 0.5) * noise * 5
              });
          }
      } else if (pattern === 'xor') {
          // XOR pattern
          for (let i = 0; i < n_class0 / 2; i++) {
              class0.push({
                  x: 3 + Math.random() * 2 + (Math.random() - 0.5) * noise * 5,
                  y: 3 + Math.random() * 2 + (Math.random() - 0.5) * noise * 5
              });
              class0.push({
                  x: 7 + Math.random() * 2 + (Math.random() - 0.5) * noise * 5,
                  y: 7 + Math.random() * 2 + (Math.random() - 0.5) * noise * 5
              });
          }
          for (let i = 0; i < n_class1 / 2; i++) {
              class1.push({
                  x: 3 + Math.random() * 2 + (Math.random() - 0.5) * noise * 5,
                  y: 7 + Math.random() * 2 + (Math.random() - 0.5) * noise * 5
              });
              class1.push({
                  x: 7 + Math.random() * 2 + (Math.random() - 0.5) * noise * 5,
                  y: 3 + Math.random() * 2 + (Math.random() - 0.5) * noise * 5
              });
          }
      } else if (pattern === 'linearly') {
          // Linearly separable
          for (let i = 0; i < n_class0; i++) {
              class0.push({
                  x: 3 + Math.random() * 3 + (Math.random() - 0.5) * noise * 8,
                  y: 3 + Math.random() * 3 + (Math.random() - 0.5) * noise * 8
              });
          }
          for (let i = 0; i < n_class1; i++) {
              class1.push({
                  x: 7 + Math.random() * 3 + (Math.random() - 0.5) * noise * 8,
                  y: 7 + Math.random() * 3 + (Math.random() - 0.5) * noise * 8
              });
          }
      }
      
      if (compareClassChart) {
          compareClassChart.data.datasets[0].data = class0;
          compareClassChart.data.datasets[1].data = class1;
          compareClassChart.update();
      }
  }

  // Switch classifier and update metrics
  function compareClassifier(algo) {
      let acc, prec, rec, f1;
      
      // Get current data pattern
      const pattern = document.getElementById('dataPattern').value;
      const noise = parseFloat(document.getElementById('noiseLevel').value);
      
      // Simulate realistic metrics based on algorithm and pattern
      switch(algo) {
          case 'logistic':
              if (pattern === 'linearly') {
                  acc = 0.92 + (Math.random() * 0.05 - 0.025);
                  prec = 0.91;
                  rec = 0.90;
              } else if (pattern === 'blobs') {
                  acc = 0.85 + (Math.random() * 0.05 - 0.025);
                  prec = 0.84;
                  rec = 0.83;
              } else {
                  acc = 0.72 + (Math.random() * 0.05 - 0.025) - noise * 0.3;
                  prec = 0.70;
                  rec = 0.68;
              }
              f1 = 2 * (prec * rec) / (prec + rec);
              break;
              
          case 'knn':
              if (pattern === 'linearly') {
                  acc = 0.94;
                  prec = 0.93;
                  rec = 0.92;
              } else if (['circles', 'moons', 'xor'].includes(pattern)) {
                  acc = 0.88 - noise * 0.2;
                  prec = 0.86;
                  rec = 0.85;
              } else {
                  acc = 0.85;
                  prec = 0.83;
                  rec = 0.82;
              }
              f1 = 2 * (prec * rec) / (prec + rec);
              break;
              
          case 'nb':
              if (pattern === 'blobs') {
                  acc = 0.83;
                  prec = 0.82;
                  rec = 0.81;
              } else if (pattern === 'linearly') {
                  acc = 0.88;
                  prec = 0.87;
                  rec = 0.86;
              } else {
                  acc = 0.75 - noise * 0.2;
                  prec = 0.73;
                  rec = 0.72;
              }
              f1 = 2 * (prec * rec) / (prec + rec);
              break;
              
          case 'svm':
              if (['circles', 'moons', 'xor'].includes(pattern)) {
                  acc = 0.92 - noise * 0.15;
                  prec = 0.91;
                  rec = 0.90;
              } else if (pattern === 'linearly') {
                  acc = 0.95;
                  prec = 0.94;
                  rec = 0.93;
              } else {
                  acc = 0.88;
                  prec = 0.87;
                  rec = 0.86;
              }
              f1 = 2 * (prec * rec) / (prec + rec);
              break;
              
          case 'tree':
              if (pattern === 'linearly') {
                  acc = 0.90;
                  prec = 0.89;
                  rec = 0.88;
              } else {
                  acc = 0.82 - noise * 0.25;
                  prec = 0.80;
                  rec = 0.79;
              }
              f1 = 2 * (prec * rec) / (prec + rec);
              break;
              
          case 'rf':
              if (['circles', 'moons', 'xor'].includes(pattern)) {
                  acc = 0.94 - noise * 0.1;
                  prec = 0.93;
                  rec = 0.92;
              } else {
                  acc = 0.91;
                  prec = 0.90;
                  rec = 0.89;
              }
              f1 = 2 * (prec * rec) / (prec + rec);
              break;
      }
      
      document.getElementById('compAcc').textContent = acc.toFixed(3);
      document.getElementById('compPrec').textContent = prec.toFixed(3);
      document.getElementById('compRec').textContent = rec.toFixed(3);
      document.getElementById('compF1').textContent = f1.toFixed(3);
      
      // Highlight selected algorithm in cards
      document.querySelectorAll('.algorithm-card').forEach(card => {
          card.style.border = 'none';
      });
      event.currentTarget.style.border = `2px solid var(--primary)`;
  }

  function regenerateData() {
      generateCompareData();
      // Reset to default classifier (logistic)
      compareClassifier('logistic');
  }

  // Initialize on load
  document.addEventListener('DOMContentLoaded', () => {
      initCompareClassChart();
      generateCompareData();
      compareClassifier('logistic');
  });
  </script>
</div>

    <!-- ================================================
         SECTION 10: RESOURCES
    ================================================= -->
    <!-- ================================================
     SECTION 10: PRACTICE RESOURCES - ENHANCED
================================================= -->
<div id="resources-class" class="module-content">

  <div class="panel section">
    <h3><i class="fas fa-book"></i> 10.1 Where to Practice Classification</h3>

    <div class="info-box info">
      <h4><i class="fas fa-bullseye"></i> The Best Way to Learn: Practice, Practice, Practice!</h4>
      <p>
        Classification is best learned by doing. This section provides curated resources:
        datasets to experiment with, coding challenges, competitions, and further reading.
      </p>
      
      <div style="display: flex; gap: 20px; flex-wrap: wrap; margin-top: 15px; justify-content: center;">
        <span class="badge"><i class="fas fa-check-circle"></i> Beginner datasets</span>
        <span class="badge"><i class="fas fa-check-circle"></i> Real-world problems</span>
        <span class="badge"><i class="fas fa-check-circle"></i> Coding challenges</span>
        <span class="badge"><i class="fas fa-check-circle"></i> Competition platforms</span>
      </div>
    </div>

    <div class="explanation-box">
      <p><strong>üìö Learning Path:</strong> Start with small, clean datasets to understand the algorithms. Then move to messier real-world data. Finally, try competitions to push your skills to the limit.</p>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-database"></i> 10.2 Classic Classification Datasets</h3>

    <div class="algorithm-grid">
      <!-- Iris Dataset -->
      <div class="algorithm-card">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-seedling"></i></div>
          <div class="algorithm-title">
            <h4>Iris Flower Dataset</h4>
            <p>The "Hello World" of ML</p>
          </div>
        </div>
        <div class="algorithm-body">
          <p><strong>Task:</strong> Classify iris species (Setosa, Versicolor, Virginica)</p>
          <ul style="font-size: 12px;">
            <li><strong>Samples:</strong> 150</li>
            <li><strong>Features:</strong> 4 (sepal length/width, petal length/width)</li>
            <li><strong>Classes:</strong> 3 (perfectly balanced)</li>
            <li><strong>Difficulty:</strong> ‚≠ê (Very Easy)</li>
          </ul>
          <div style="margin-top: 10px;">
            <span class="library-badge sklearn-badge"><i class="fab fa-python"></i> sklearn.datasets.load_iris()</span>
          </div>
          <p style="margin-top: 10px; font-size: 12px;"><strong>Best for:</strong> First steps, understanding basics, visualization practice</p>
        </div>
      </div>

      <!-- Titanic Dataset -->
      <div class="algorithm-card">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-ship"></i></div>
          <div class="algorithm-title">
            <h4>Titanic Survival</h4>
            <p>Kaggle'sÂÖ•Èó®Á´ûËµõ</p>
          </div>
        </div>
        <div class="algorithm-body">
          <p><strong>Task:</strong> Predict survival on the Titanic</p>
          <ul style="font-size: 12px;">
            <li><strong>Samples:</strong> 891 (train) + 418 (test)</li>
            <li><strong>Features:</strong> 11 (mixed numeric/categorical)</li>
            <li><strong>Classes:</strong> 2 (Survived/Not)</li>
            <li><strong>Difficulty:</strong> ‚≠ê‚≠ê (Easy-Moderate)</li>
          </ul>
          <div style="margin-top: 10px;">
            <span class="library-badge sklearn-badge"><i class="fas fa-link"></i> kaggle.com/c/titanic</span>
          </div>
          <p style="margin-top: 10px; font-size: 12px;"><strong>Best for:</strong> Feature engineering, handling missing data, categorical variables</p>
        </div>
      </div>

      <!-- Breast Cancer -->
      <div class="algorithm-card">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-heartbeat"></i></div>
          <div class="algorithm-title">
            <h4>Breast Cancer Wisconsin</h4>
          </div>
        </div>
        <div class="algorithm-body">
          <p><strong>Task:</strong> Diagnose malignant vs benign tumors</p>
          <ul style="font-size: 12px;">
            <li><strong>Samples:</strong> 569</li>
            <li><strong>Features:</strong> 30 (cell nucleus measurements)</li>
            <li><strong>Classes:</strong> 2 (Malignant/Benign)</li>
            <li><strong>Difficulty:</strong> ‚≠ê‚≠ê (Easy-Moderate)</li>
          </ul>
          <div style="margin-top: 10px;">
            <span class="library-badge sklearn-badge"><i class="fab fa-python"></i> sklearn.datasets.load_breast_cancer()</span>
          </div>
          <p style="margin-top: 10px; font-size: 12px;"><strong>Best for:</strong> Medical applications, feature importance, imbalanced classes (37% malignant)</p>
        </div>
      </div>

      <!-- MNIST -->
      <div class="algorithm-card">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-font"></i></div>
          <div class="algorithm-title">
            <h4>MNIST Digits</h4>
            <p>The "Image Net" of beginner vision</p>
          </div>
        </div>
        <div class="algorithm-body">
          <p><strong>Task:</strong> Recognize handwritten digits 0-9</p>
          <ul style="font-size: 12px;">
            <li><strong>Samples:</strong> 70,000</li>
            <li><strong>Features:</strong> 784 (28x28 pixels)</li>
            <li><strong>Classes:</strong> 10</li>
            <li><strong>Difficulty:</strong> ‚≠ê‚≠ê‚≠ê (Moderate)</li>
          </ul>
          <div style="margin-top: 10px;">
            <span class="library-badge sklearn-badge"><i class="fab fa-python"></i> sklearn.datasets.fetch_openml('mnist_784')</span>
          </div>
          <p style="margin-top: 10px; font-size: 12px;"><strong>Best for:</strong> High-dimensional data, SVM practice, neural network introduction</p>
        </div>
      </div>

      <!-- Spam Collection -->
      <div class="algorithm-card">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-envelope"></i></div>
          <div class="algorithm-title">
            <h4>Spam Collection</h4>
            <p>Text classification classic</p>
          </div>
        </div>
        <div class="algorithm-body">
          <p><strong>Task:</strong> Classify SMS messages as spam or ham</p>
          <ul style="font-size: 12px;">
            <li><strong>Samples:</strong> 5,574</li>
            <li><strong>Features:</strong> Text messages (variable length)</li>
            <li><strong>Classes:</strong> 2 (Spam/Ham)</li>
            <li><strong>Difficulty:</strong> ‚≠ê‚≠ê (Easy-Moderate)</li>
          </ul>
          <div style="margin-top: 10px;">
            <span class="library-badge sklearn-badge"><i class="fas fa-link"></i> archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection</span>
          </div>
          <p style="margin-top: 10px; font-size: 12px;"><strong>Best for:</strong> Text preprocessing, Naive Bayes, TF-IDF</p>
        </div>
      </div>

      <!-- Wine Quality -->
      <div class="algorithm-card">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fas fa-wine-bottle"></i></div>
          <div class="algorithm-title">
            <h4>Wine Quality</h4>
          </div>
        </div>
        <div class="algorithm-body">
          <p><strong>Task:</strong> Predict wine quality (binary: good/bad or multi-class)</p>
          <ul style="font-size: 12px;">
            <li><strong>Samples:</strong> 4,898 (white) + 1,599 (red)</li>
            <li><strong>Features:</strong> 11 (chemical properties)</li>
            <li><strong>Classes:</strong> 3-10 (depending on task)</li>
            <li><strong>Difficulty:</strong> ‚≠ê‚≠ê (Moderate)</li>
          </ul>
          <div style="margin-top: 10px;">
            <span class="library-badge sklearn-badge"><i class="fas fa-link"></i> archive.ics.uci.edu/ml/datasets/Wine+Quality</span>
          </div>
          <p style="margin-top: 10px; font-size: 12px;"><strong>Best for:</strong> Regression vs classification comparison, imbalanced classes</p>
        </div>
      </div>
    </div>

    <div class="expanded-content">
      <h4><i class="fas fa-download"></i> Download Practice Dataset</h4>
      <p>Click below to download a synthetic classification dataset (500 samples, 5 features, binary target) perfect for practicing all algorithms in this module:</p>
      
      <div style="display: flex; gap: 20px; align-items: center;">
        <button class="btn btn-primary" onclick="downloadClassDataset()">
          <i class="fas fa-download"></i> classification_practice.csv
        </button>
        <span class="badge"><i class="fas fa-tag"></i> 500 samples</span>
        <span class="badge"><i class="fas fa-tag"></i> 5 features</span>
        <span class="badge"><i class="fas fa-tag"></i> Binary target</span>
      </div>
      
      <div style="margin-top: 15px; background: var(--panel2); padding: 15px; border-radius: var(--radius);">
        <p><strong>Preview (first 5 rows):</strong></p>
        <table class="detailed-table" style="font-size: 12px;">
          <thead><tr><th>feature1</th><th>feature2</th><th>feature3</th><th>feature4</th><th>feature5</th><th>target</th></tr></thead>
          <tbody>
            <tr><td>0.23</td><td>0.45</td><td>1.0</td><td>0.0</td><td>0.67</td><td>0</td></tr>
            <tr><td>0.89</td><td>0.12</td><td>0.0</td><td>1.0</td><td>0.23</td><td>1</td></tr>
            <tr><td>0.45</td><td>0.78</td><td>1.0</td><td>1.0</td><td>0.89</td><td>1</td></tr>
            <tr><td>0.12</td><td>0.34</td><td>0.0</td><td>0.0</td><td>0.45</td><td>0</td></tr>
            <tr><td>0.67</td><td>0.23</td><td>1.0</td><td>0.0</td><td>0.12</td><td>1</td></tr>
          </tbody>
        </table>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-laptop-code"></i> 10.3 Interactive Coding Platforms</h3>

    <div class="concept-grid">
      <!-- Kaggle -->
      <div class="concept-card">
        <h5><i class="fab fa-kaggle"></i> Kaggle</h5>
        <p><strong>The world's largest data science community</strong></p>
        <ul style="padding-left:20px;">
          <li>Hundreds of datasets ready to use</li>
          <li>Free GPUs and TPUs for training</li>
          <li>Competitions with prize money</li>
          <li>Thousands of notebooks to learn from</li>
        </ul>
        <p><strong>Start with:</strong> Titanic, Spaceship Titanic, Digit Recognizer</p>
        <a href="https://www.kaggle.com" target="_blank" class="btn btn-secondary" style="margin-top: 10px;">
          <i class="fab fa-kaggle"></i> kaggle.com
        </a>
      </div>

      <!-- Google Colab -->
      <div class="concept-card">
        <h5><i class="fas fa-cloud"></i> Google Colab</h5>
        <p><strong>Free Jupyter notebooks in the cloud</strong></p>
        <ul style="padding-left:20px;">
          <li>Free GPU (Tesla K80, T4, V100 sometimes)</li>
          <li>Pre-installed ML libraries</li>
          <li>Easy sharing and collaboration</li>
          <li>Integrates with Google Drive</li>
        </ul>
        <p><strong>Start with:</strong> Run any notebook without installation</p>
        <a href="https://colab.research.google.com" target="_blank" class="btn btn-secondary" style="margin-top: 10px;">
          <i class="fas fa-cloud"></i> colab.research.google.com
        </a>
      </div>

      <!-- Scikit-learn Documentation -->
      <div class="concept-card">
        <h5><i class="fas fa-book-open"></i> Scikit-learn Tutorials</h5>
        <p><strong>Official documentation with examples</strong></p>
        <ul style="padding-left:20px;">
          <li>Well-documented algorithms</li>
          <li>Working code examples</li>
          <li>Visualizations included</li>
          <li>Covers all classification algorithms</li>
        </ul>
        <p><strong>Start with:</strong> Classification tutorials, comparing algorithms</p>
        <a href="https://scikit-learn.org/stable/tutorial/index.html" target="_blank" class="btn btn-secondary" style="margin-top: 10px;">
          <i class="fas fa-book"></i> scikit-learn.org
        </a>
      </div>

      <!-- Analytics Vidhya -->
      <div class="concept-card">
        <h5><i class="fas fa-chart-line"></i> Analytics Vidhya</h5>
        <p><strong>Practice problems and hackathons</strong></p>
        <ul style="padding-left:20px;">
          <li>Beginner-friendly practice problems</li>
          <li>Regular hackathons with prizes</li>
          <li>Detailed solution explanations</li>
          <li>Learning paths for beginners</li>
        </ul>
        <p><strong>Start with:</strong> Loan Prediction, HR Analytics problems</p>
        <a href="https://www.analyticsvidhya.com" target="_blank" class="btn btn-secondary" style="margin-top: 10px;">
          <i class="fas fa-chart-line"></i> analyticsvidhya.com
        </a>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-trophy"></i> 10.4 Competitions to Challenge Yourself</h3>

    <table class="detailed-table">
      <thead>
        <tr><th>Competition</th><th>Description</th><th>Difficulty</th><th>Skills</th></tr>
      </thead>
      <tbody>
        <tr>
          <td><strong>Titanic</strong></td>
          <td>Predict survival on the Titanic</td>
          <td><span class="badge" style="background: var(--success);">Beginner</span></td>
          <td>Feature engineering, ensemble methods</td>
        </tr>
        <tr>
          <td><strong>Spaceship Titanic</strong></td>
          <td>Predict which passengers transported to another dimension</td>
          <td><span class="badge" style="background: var(--success);">Beginner</span></td>
          <td>Classification, missing data, categorical features</td>
        </tr>
        <tr>
          <td><strong>Digit Recognizer</strong></td>
          <td>MNIST handwritten digit recognition</td>
          <td><span class="badge" style="background: var(--warning);">Intermediate</span></td>
          <td>Image classification, SVM, neural networks</td>
        </tr>
        <tr>
          <td><strong>Porto Seguro Safe Driver</strong></td>
          <td>Predict which drivers will file a claim</td>
          <td><span class="badge" style="background: var(--warning);">Intermediate</span></td>
          <td>Imbalanced classes, feature engineering</td>
        </tr>
        <tr>
          <td><strong>IEEE-CIS Fraud Detection</strong></td>
          <td>Identify fraudulent transactions</td>
          <td><span class="badge" style="background: var(--danger);">Advanced</span></td>
          <td>Imbalanced data, feature interactions, ensembles</td>
        </tr>
        <tr>
          <td><strong>Otto Group Product Classification</strong></td>
          <td>Classify products into 9 categories</td>
          <td><span class="badge" style="background: var(--danger);">Advanced</span></td>
          <td>Multi-class, imbalanced, feature transformations</td>
        </tr>
      </tbody>
    </table>

    <div class="pro-tip" style="margin-top: 20px;">
      <p><strong>üí° Competition Strategy:</strong> Start with Titanic to understand the workflow. Then try Spaceship Titanic (similar but newer). Move to Digit Recognizer for image data. Finally, tackle the advanced competitions!</p>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-video"></i> 10.5 Video Courses & Tutorials</h3>

    <div class="algorithm-grid">
      <!-- StatQuest -->
      <div class="algorithm-card">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fab fa-youtube" style="color: #FF0000;"></i></div>
          <div class="algorithm-title">
            <h4>StatQuest with Josh Starmer</h4>
          </div>
        </div>
        <div class="algorithm-body">
          <p>Best explainer videos on YouTube! Clear, visual, and entertaining.</p>
          <ul style="font-size: 12px;">
            <li>Logistic Regression</li>
            <li>Decision Trees</li>
            <li>Random Forest</li>
            <li>SVM</li>
            <li>Naive Bayes</li>
          </ul>
          <a href="https://www.youtube.com/c/joshstarmer" target="_blank" class="btn btn-secondary" style="margin-top: 10px; width: 100%;">
            <i class="fab fa-youtube"></i> Watch on YouTube
          </a>
        </div>
      </div>

      <!-- 3Blue1Brown -->
      <div class="algorithm-card">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fab fa-youtube" style="color: #FF0000;"></i></div>
          <div class="algorithm-title">
            <h4>3Blue1Brown</h4>
          </div>
        </div>
        <div class="algorithm-body">
          <p>Beautiful visual explanations of mathematical concepts behind ML.</p>
          <ul style="font-size: 12px;">
            <li>Bayes Theorem</li>
            <li>Neural Networks</li>
            <li>Entropy & Information Theory</li>
          </ul>
          <a href="https://www.youtube.com/c/3blue1brown" target="_blank" class="btn btn-secondary" style="margin-top: 10px; width: 100%;">
            <i class="fab fa-youtube"></i> Watch on YouTube
          </a>
        </div>
      </div>

      <!-- sentdex -->
      <div class="algorithm-card">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fab fa-youtube" style="color: #FF0000;"></i></div>
          <div class="algorithm-title">
            <h4>sentdex</h4>
          </div>
        </div>
        <div class="algorithm-body">
          <p>Practical Python tutorials with real implementations.</p>
          <ul style="font-size: 12px;">
            <li>Machine Learning with Python</li>
            <li>Practical sklearn tutorials</li>
            <li>Real-world projects</li>
          </ul>
          <a href="https://www.youtube.com/c/sentdex" target="_blank" class="btn btn-secondary" style="margin-top: 10px; width: 100%;">
            <i class="fab fa-youtube"></i> Watch on YouTube
          </a>
        </div>
      </div>

      <!-- Kaggle Learn -->
      <div class="algorithm-card">
        <div class="algorithm-header">
          <div class="algorithm-icon"><i class="fab fa-kaggle"></i></div>
          <div class="algorithm-title">
            <h4>Kaggle Learn</h4>
          </div>
        </div>
        <div class="algorithm-body">
          <p>Free micro-courses with interactive exercises.</p>
          <ul style="font-size: 12px;">
            <li>Intro to Machine Learning</li>
            <li>Intermediate Machine Learning</li>
            <li>Feature Engineering</li>
            <li>Model Tuning</li>
          </ul>
          <a href="https://www.kaggle.com/learn" target="_blank" class="btn btn-secondary" style="margin-top: 10px; width: 100%;">
            <i class="fab fa-kaggle"></i> Start Learning
          </a>
        </div>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-book-reader"></i> 10.6 Books & Reading</h3>

    <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 20px;">
      <!-- ISLR -->
      <div style="background: var(--panel2); padding: 20px; border-radius: var(--radius);">
        <div style="font-size: 3rem; text-align: center; margin-bottom: 10px;">üìò</div>
        <h4 style="text-align: center;">ISLR</h4>
        <p style="text-align: center; font-style: italic;">Introduction to Statistical Learning</p>
        <p><strong>Best for:</strong> Beginners, intuition, R code examples</p>
        <p><strong>Chapters:</strong> 4 (Classification), 9 (SVM), 8 (Trees)</p>
        <a href="https://www.statlearning.com" target="_blank" style="display: block; text-align: center; margin-top: 10px;">Free PDF available</a>
      </div>

      <!-- ESL -->
      <div style="background: var(--panel2); padding: 20px; border-radius: var(--radius);">
        <div style="font-size: 3rem; text-align: center; margin-bottom: 10px;">üìô</div>
        <h4 style="text-align: center;">ESL</h4>
        <p style="text-align: center; font-style: italic;">Elements of Statistical Learning</p>
        <p><strong>Best for:</strong> Advanced learners, mathematical depth</p>
        <p><strong>Chapters:</strong> 4-7, 9-10, 12-13</p>
        <a href="https://web.stanford.edu/~hastie/ElemStatLearn/" target="_blank" style="display: block; text-align: center; margin-top: 10px;">Free PDF available</a>
      </div>

      <!-- Hands-On ML -->
      <div style="background: var(--panel2); padding: 20px; border-radius: var(--radius);">
        <div style="font-size: 3rem; text-align: center; margin-bottom: 10px;">üìï</div>
        <h4 style="text-align: center;">Hands-On ML</h4>
        <p style="text-align: center; font-style: italic;">with Scikit-Learn & TensorFlow</p>
        <p><strong>Best for:</strong> Practical implementation, Python code</p>
        <p><strong>Chapters:</strong> 3-7 (Classification)</p>
        <a href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/" target="_blank" style="display: block; text-align: center; margin-top: 10px;">O'Reilly Media</a>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-checklist"></i> 10.7 Practice Projects by Level</h3>

    <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 20px;">
      <div>
        <h4 style="color: var(--success);">üå± Beginner</h4>
        <ul style="padding-left:20px;">
          <li>Iris flower classifier</li>
          <li>Titanic survival prediction</li>
          <li>Wine type classification</li>
          <li>Breast cancer diagnosis</li>
          <li>Spam SMS detector</li>
        </ul>
        <p><em>Focus on: understanding workflow, basic preprocessing, interpreting results</em></p>
      </div>

      <div>
        <h4 style="color: var(--warning);">üåø Intermediate</h4>
        <ul style="padding-left:20px;">
          <li>MNIST digit recognizer</li>
          <li>Customer churn prediction</li>
          <li>Credit card fraud detection</li>
          <li>News article categorization</li>
          <li>Sentiment analysis on reviews</li>
        </ul>
        <p><em>Focus on: feature engineering, handling imbalance, model tuning</em></p>
      </div>

      <div>
        <h4 style="color: var(--danger);">üå≥ Advanced</h4>
        <ul style="padding-left:20px;">
          <li>ImageNet classification subset</li>
          <li>Protein structure prediction</li>
          <li>Galaxy morphology classification</li>
          <li>Multi-label music genre classification</li>
          <li>Real-time video classification</li>
        </ul>
        <p><em>Focus on: deep learning, large-scale data, state-of-the-art architectures</em></p>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-code"></i> 10.8 Useful Code Snippets for Practice</h3>

    <div class="code-editor-container">
      <div class="code-header">
        <div class="code-title"><i class="fab fa-python"></i> Quick Start Template for Any Classification Problem</div>
      </div>
      <div class="code-body">
        <pre class="code-block"><code><span class="keyword">import</span> pandas <span class="keyword">as</span> pd
<span class="keyword">import</span> numpy <span class="keyword">as</span> np
<span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, cross_val_score, GridSearchCV
<span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler, LabelEncoder
<span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report, confusion_matrix, roc_auc_score
<span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier
<span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt
<span class="keyword">import</span> seaborn <span class="keyword">as</span> sns

<span class="comment"># ============================================</span>
<span class="comment"># 1. Load and explore data</span>
<span class="comment"># ============================================</span>
<span class="comment"># df = pd.read_csv('your_data.csv')</span>
<span class="comment"># For demo, create synthetic data</span>
<span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification
X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)

<span class="comment"># Quick exploration</span>
<span class="keyword">print</span>(<span class="string">"Shape:"</span>, X.shape)
<span class="keyword">print</span>(<span class="string">"Class distribution:\n"</span>, pd.Series(y).value_counts())

<span class="comment"># ============================================</span>
<span class="comment"># 2. Split data</span>
<span class="comment"># ============================================</span>
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

<span class="comment"># ============================================</span>
<span class="comment"># 3. Scale features (important for many algorithms)</span>
<span class="comment"># ============================================</span>
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

<span class="comment"># ============================================</span>
<span class="comment"># 4. Quick baseline with multiple models</span>
<span class="comment"># ============================================</span>
<span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression
<span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier
<span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC
<span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier
<span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier, GradientBoostingClassifier
<span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB

models = {
    <span class="string">'Logistic Regression'</span>: LogisticRegression(),
    <span class="string">'KNN'</span>: KNeighborsClassifier(),
    <span class="string">'SVM'</span>: SVC(),
    <span class="string">'Decision Tree'</span>: DecisionTreeClassifier(),
    <span class="string">'Random Forest'</span>: RandomForestClassifier(),
    <span class="string">'Gradient Boosting'</span>: GradientBoostingClassifier(),
    <span class="string">'Naive Bayes'</span>: GaussianNB()
}

results = {}
<span class="keyword">for</span> name, model <span class="keyword">in</span> models.items():
    model.fit(X_train_scaled, y_train)
    score = model.score(X_test_scaled, y_test)
    results[name] = score
    <span class="keyword">print</span>(<span class="string">f"{name:20}: {score:.4f}"</span>)

<span class="comment"># ============================================</span>
<span class="comment"># 5. Pick best model and tune</span>
<span class="comment"># ============================================</span>
best_model_name = max(results, key=results.get)
<span class="keyword">print</span>(<span class="string">f"\nBest baseline model: {best_model_name}"</span>)

<span class="keyword">if</span> best_model_name == <span class="string">'Random Forest'</span>:
    param_grid = {
        'n_estimators': [100, 200],
        'max_depth': [10, 20, None],
        'min_samples_split': [2, 5, 10]
    }
    grid_search = GridSearchCV(
        RandomForestClassifier(random_state=42),
        param_grid, cv=5, scoring='accuracy', n_jobs=-1
    )
    grid_search.fit(X_train_scaled, y_train)
    <span class="keyword">print</span>(<span class="string">"Best parameters:"</span>, grid_search.best_params_)
    <span class="keyword">print</span>(<span class="string">"Best CV score:"</span>, grid_search.best_score_)
    best_model = grid_search.best_estimator_
<span class="keyword">else</span>:
    best_model = models[best_model_name]

<span class="comment"># ============================================</span>
<span class="comment"># 6. Final evaluation</span>
<span class="comment"># ============================================</span>
y_pred = best_model.predict(X_test_scaled)
y_proba = best_model.predict_proba(X_test_scaled)[:, 1]

<span class="keyword">print</span>(<span class="string">"\nClassification Report:"</span>)
<span class="keyword">print</span>(classification_report(y_test, y_pred))

<span class="keyword">print</span>(<span class="string">f"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}"</span>)

<span class="comment"># Confusion matrix</span>
plt.figure(figsize=(8,6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.show()

<span class="comment"># Feature importance (if available)</span>
<span class="keyword">if</span> hasattr(best_model, 'feature_importances_'):
    plt.figure(figsize=(10,6))
    importance = best_model.feature_importances_
    plt.barh(range(len(importance)), importance)
    plt.xlabel('Feature Importance')
    plt.title('Feature Importance Plot')
    plt.show()</code></pre>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-link"></i> 10.9 Essential Links & References</h3>

    <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 20px;">
      <div>
        <h4>üìö Documentation</h4>
        <ul>
          <li><a href="https://scikit-learn.org/stable/supervised_learning.html" target="_blank">scikit-learn docs</a></li>
          <li><a href="https://xgboost.readthedocs.io/" target="_blank">XGBoost documentation</a></li>
          <li><a href="https://lightgbm.readthedocs.io/" target="_blank">LightGBM docs</a></li>
          <li><a href="https://catboost.ai/" target="_blank">CatBoost</a></li>
        </ul>
      </div>

      <div>
        <h4>üéì Courses</h4>
        <ul>
          <li><a href="https://www.coursera.org/learn/machine-learning" target="_blank">Andrew Ng's ML Course</a></li>
          <li><a href="https://www.fast.ai/" target="_blank">fast.ai Practical Deep Learning</a></li>
          <li><a href="https://developers.google.com/machine-learning/crash-course" target="_blank">Google's ML Crash Course</a></li>
        </ul>
      </div>

      <div>
        <h4>üìä Data Sources</h4>
        <ul>
          <li><a href="https://www.kaggle.com/datasets" target="_blank">Kaggle Datasets</a></li>
          <li><a href="https://archive.ics.uci.edu/ml/index.php" target="_blank">UCI ML Repository</a></li>
          <li><a href="https://paperswithcode.com/datasets" target="_blank">Papers with Code</a></li>
          <li><a href="https://www.openml.org/" target="_blank">OpenML</a></li>
        </ul>
      </div>
    </div>
  </div>

  <div class="panel section">
    <h3><i class="fas fa-graduation-cap"></i> 10.10 Your Next Steps</h3>

    <div class="info-box success">
      <h4>üöÄ 30-Day Practice Plan</h4>
      
      <div style="display: grid; grid-template-columns: repeat(5, 1fr); gap: 10px; margin-top: 20px;">
        <div style="text-align: center; padding: 10px; background: var(--panel2); border-radius: var(--radius);">
          <strong>Week 1</strong>
          <p>Iris + Titanic<br>Understand basics</p>
        </div>
        <div style="text-align: center; padding: 10px; background: var(--panel2); border-radius: var(--radius);">
          <strong>Week 2</strong>
          <p>Breast Cancer + Wine<br>Feature importance</p>
        </div>
        <div style="text-align: center; padding: 10px; background: var(--panel2); border-radius: var(--radius);">
          <strong>Week 3</strong>
          <p>Spam + MNIST<br>Different data types</p>
        </div>
        <div style="text-align: center; padding: 10px; background: var(--panel2); border-radius: var(--radius);">
          <strong>Week 4</strong>
          <p>Kaggle Competition<br>End-to-end project</p>
        </div>
        <div style="text-align: center; padding: 10px; background: var(--panel2); border-radius: var(--radius);">
          <strong>Beyond</strong>
          <p>Advanced competitions<br>Real-world problems</p>
        </div>
      </div>
      
      <div style="margin-top: 20px;">
        <p><strong>Remember:</strong></p>
        <ul>
          <li>Start simple, then add complexity</li>
          <li>Always do EDA before modeling</li>
          <li>Try multiple algorithms</li>
          <li>Use cross-validation</li>
          <li>Document your experiments</li>
          <li>Share your work on GitHub</li>
        </ul>
      </div>
    </div>

    <div style="margin-top: 30px; text-align: center; padding: 20px; background: linear-gradient(135deg, rgba(96, 165, 250, 0.2), rgba(167, 139, 250, 0.2)); border-radius: var(--radius);">
      <p style="font-size: 1.3rem;">
        <i class="fas fa-rocket" style="color: var(--warning);"></i> 
        <strong>You've completed Module 6!</strong>
      </p>
      <p>
        Now go forth and classify! Remember: every expert was once a beginner who never gave up.
      </p>
      <p style="margin-top: 15px;">
        <span class="badge"><i class="fas fa-check"></i> Classification basics</span>
        <span class="badge"><i class="fas fa-check"></i> 7 algorithms</span>
        <span class="badge"><i class="fas fa-check"></i> Evaluation metrics</span>
        <span class="badge"><i class="fas fa-check"></i> Practice resources</span>
      </p>
    </div>
  </div>

  <footer>
    <p>üìö Module 6: Supervised Machine Learning - Classification</p>
    <p>¬© 2026 | IAF 604 | Remember: No single classifier is best - try them all and validate!</p>
    <p style="font-size: 12px; margin-top: 10px;">
      <i class="fas fa-star" style="color: var(--warning);"></i> 
      Next: Unsupervised Learning - Clustering & Dimensionality Reduction
      <i class="fas fa-star" style="color: var(--warning);"></i>
    </p>
  </footer>
</div>

<script>
// Dataset download function
function downloadClassDataset() {
    // Generate synthetic dataset
    const headers = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5', 'target'];
    const rows = [];
    
    // Generate 500 samples
    for (let i = 0; i < 500; i++) {
        const row = [];
        for (let j = 0; j < 5; j++) {
            row.push((Math.random() * 2 - 0.5).toFixed(3));
        }
        // Create non-linear target based on features
        const f1 = parseFloat(row[0]);
        const f2 = parseFloat(row[1]);
        const f3 = parseFloat(row[2]);
        const target = (f1 * f2 + f3 > 0.2) ? 1 : 0;
        row.push(target);
        rows.push(row.join(','));
    }
    
    const content = [headers.join(','), ...rows].join('\n');
    
    const blob = new Blob([content], { type: 'text/csv;charset=utf-8;' });
    const url = URL.createObjectURL(blob);
    const a = document.createElement('a');
    a.href = url;
    a.download = 'classification_practice.csv';
    document.body.appendChild(a);
    a.click();
    document.body.removeChild(a);
    URL.revokeObjectURL(url);
    
    // Show confirmation
    alert('Dataset downloaded! Contains 500 samples, 5 features, binary target.');
}
</script>

    <footer>
      <p>üìö Module 6: Supervised Machine Learning - Classification</p>
      <p>¬© 2026 | IAF 604 | Remember: No single classifier is best - try them all!</p>
    </footer>
  </div>

  <script>
    /************************************************************
     * JAVASCRIPT FOR CLASSIFICATION MODULE
     ************************************************************/
    let logisticChart, knnChart, svmChart, treeClassChart, compareClassChart, rocChart;
    
    // Theme management (same as Module 5)
    const themeToggle = document.getElementById('themeToggle');
    const themeLabel = document.getElementById('themeLabel');
    const body = document.body;
    const THEME_KEY = 'iaf604-class-theme';

    function applyTheme(theme) {
      body.setAttribute('data-theme', theme);
      const isDay = theme === 'day';
      themeLabel.textContent = isDay ? 'Day Mode' : 'Night Mode';
      themeToggle.setAttribute('aria-label', `Theme: ${themeLabel.textContent} (click to toggle)`);
      updateChartColors();
    }

    function initTheme() {
      const savedTheme = localStorage.getItem(THEME_KEY);
      const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
      if (savedTheme === 'day' || savedTheme === 'night') {
        applyTheme(savedTheme);
      } else {
        const defaultTheme = prefersDark ? 'night' : 'day';
        applyTheme(defaultTheme);
        localStorage.setItem(THEME_KEY, defaultTheme);
      }
    }

    function toggleTheme() {
      const current = body.getAttribute('data-theme');
      const next = current === 'night' ? 'day' : 'night';
      applyTheme(next);
      localStorage.setItem(THEME_KEY, next);
    }

    themeToggle.addEventListener('click', toggleTheme);

    // Chart colors
    function getChartColors() {
      const isDay = body.getAttribute('data-theme') === 'day';
      return {
        primary: isDay ? 'rgba(37, 99, 235, 0.8)' : 'rgba(96, 165, 250, 0.8)',
        secondary: isDay ? 'rgba(124, 58, 237, 0.8)' : 'rgba(167, 139, 250, 0.8)',
        success: isDay ? 'rgba(16, 185, 129, 0.8)' : 'rgba(52, 211, 153, 0.8)',
        danger: isDay ? 'rgba(239, 68, 68, 0.8)' : 'rgba(251, 113, 133, 0.8)',
        warning: isDay ? 'rgba(245, 158, 11, 0.8)' : 'rgba(251, 191, 36, 0.8)',
        grid: isDay ? 'rgba(148, 163, 184, 0.25)' : 'rgba(148, 163, 184, 0.14)',
        text: isDay ? '#0F172A' : '#EAF0FF',
        background: isDay ? '#FFFFFF' : '#0F172A'
      };
    }

    function updateChartColors() {
      [logisticChart, knnChart, svmChart, treeClassChart, compareClassChart, rocChart].forEach(chart => {
        if (!chart) return;
        if (chart.options.scales) {
          if (chart.options.scales.x) {
            chart.options.scales.x.grid.color = getChartColors().grid;
            chart.options.scales.x.ticks.color = getChartColors().text;
          }
          if (chart.options.scales.y) {
            chart.options.scales.y.grid.color = getChartColors().grid;
            chart.options.scales.y.ticks.color = getChartColors().text;
          }
        }
        if (chart.options.plugins && chart.options.plugins.legend) {
          chart.options.plugins.legend.labels.color = getChartColors().text;
        }
        chart.update();
      });
    }

    // Module navigation
    function switchModule(moduleId) {
      document.querySelectorAll('.module-content').forEach(module => module.classList.remove('active'));
      document.querySelectorAll('.nav-btn').forEach(btn => btn.classList.remove('active'));

      document.getElementById(moduleId).classList.add('active');
      document.querySelector(`.nav-btn[data-module="${moduleId}"]`).classList.add('active');

      // Initialize charts when sections become active
      if (moduleId === 'logistic') {
        setTimeout(() => { initLogisticChart(); generateLogisticData(); }, 100);
      } else if (moduleId === 'knn') {
        setTimeout(() => { initKNNChart(); generateKNNData(); }, 100);
      } else if (moduleId === 'svm') {
        setTimeout(() => { initSVMChart(); generateSVMData(); }, 100);
      } else if (moduleId === 'tree-classification') {
        setTimeout(() => { initTreeClassChart(); generateTreeClassData(); }, 100);
      } else if (moduleId === 'hands-on-class') {
        setTimeout(() => { initCompareClassChart(); generateCompareData(); }, 100);
      } else if (moduleId === 'evaluation-class') {
        setTimeout(() => { initROCChart(); }, 100);
      }
    }

    document.querySelectorAll('.nav-btn').forEach(btn => {
      btn.addEventListener('click', () => switchModule(btn.dataset.module));
    });

    // ============================================
    // LOGISTIC REGRESSION
    // ============================================
    function initLogisticChart() {
      const ctx = document.getElementById('logisticChart');
      if (!ctx) return;
      if (logisticChart) logisticChart.destroy();
      logisticChart = new Chart(ctx, {
        type: 'scatter',
        data: { datasets: [{
            label: 'Class 0',
            data: [],
            backgroundColor: getChartColors().primary,
            pointRadius: 5
          }, {
            label: 'Class 1',
            data: [],
            backgroundColor: getChartColors().danger,
            pointRadius: 5
          }, {
            label: 'Sigmoid Curve',
            data: [],
            type: 'line',
            borderColor: getChartColors().success,
            borderWidth: 3,
            pointRadius: 0,
            fill: false
          }]
        },
        options: {
          responsive: true,
          maintainAspectRatio: false,
          plugins: { legend: { labels: { color: getChartColors().text } } },
          scales: {
            x: { title: { display: true, text: 'Feature Value', color: getChartColors().text }, grid: { color: getChartColors().grid }, ticks: { color: getChartColors().text } },
            y: { title: { display: true, text: 'Probability (Class 1)', color: getChartColors().text }, min: -0.1, max: 1.1, grid: { color: getChartColors().grid }, ticks: { color: getChartColors().text } }
          }
        }
      });
    }

    function generateLogisticData() {
      const slope = parseFloat(document.getElementById('logisticSlope').value);
      const intercept = parseFloat(document.getElementById('logisticIntercept').value);
      const threshold = parseFloat(document.getElementById('logisticThreshold').value);
      
      document.getElementById('logisticSlopeValue').textContent = slope.toFixed(1);
      document.getElementById('logisticInterceptValue').textContent = intercept.toFixed(1);
      document.getElementById('logisticThresholdValue').textContent = threshold.toFixed(2);
      
      // Generate synthetic data
      const class0 = [];
      const class1 = [];
      for (let i = 0; i < 30; i++) {
        const x = Math.random() * 10 - 5;
        const prob = 1 / (1 + Math.exp(-(intercept + slope * x)));
        const actual = Math.random() < prob ? 1 : 0;
        
        if (actual === 0) {
          class0.push({ x, y: prob + (Math.random() - 0.5) * 0.1 });
        } else {
          class1.push({ x, y: prob + (Math.random() - 0.5) * 0.1 });
        }
      }
      
      // Sigmoid curve
      const curve = [];
      for (let x = -5; x <= 5; x += 0.1) {
        curve.push({ x, y: 1 / (1 + Math.exp(-(intercept + slope * x))) });
      }
      
      if (logisticChart) {
        logisticChart.data.datasets[0].data = class0;
        logisticChart.data.datasets[1].data = class1;
        logisticChart.data.datasets[2].data = curve;
        logisticChart.update();
      }
      
      // Update metrics
      document.getElementById('logisticAcc').textContent = (0.75 + Math.random() * 0.15).toFixed(2);
      document.getElementById('logisticPrec').textContent = (0.7 + Math.random() * 0.2).toFixed(2);
      document.getElementById('logisticRec').textContent = (0.7 + Math.random() * 0.2).toFixed(2);
      document.getElementById('logisticF1').textContent = (0.7 + Math.random() * 0.2).toFixed(2);
    }

    function updateLogistic() { generateLogisticData(); }

    // ============================================
    // KNN CHART
    // ============================================
    function initKNNChart() {
      const ctx = document.getElementById('knnChart');
      if (!ctx) return;
      if (knnChart) knnChart.destroy();
      knnChart = new Chart(ctx, {
        type: 'scatter',
        data: { datasets: [{
            label: 'Class 0',
            data: [],
            backgroundColor: getChartColors().primary,
            pointRadius: 5
          }, {
            label: 'Class 1',
            data: [],
            backgroundColor: getChartColors().danger,
            pointRadius: 5
          }]
        },
        options: {
          responsive: true,
          maintainAspectRatio: false,
          plugins: { legend: { labels: { color: getChartColors().text } } },
          scales: {
            x: { title: { display: true, text: 'Feature 1', color: getChartColors().text }, grid: { color: getChartColors().grid }, ticks: { color: getChartColors().text } },
            y: { title: { display: true, text: 'Feature 2', color: getChartColors().text }, grid: { color: getChartColors().grid }, ticks: { color: getChartColors().text } }
          }
        }
      });
    }

    function generateKNNData() {
      const k = parseInt(document.getElementById('kValue').value);
      document.getElementById('kValueValue').textContent = k;
      
      const class0 = [];
      const class1 = [];
      
      // Generate two interlocking crescents (classic non-linear pattern)
      for (let i = 0; i < 50; i++) {
        const angle = Math.random() * Math.PI;
        const radius = 3 + Math.random() * 2;
        class0.push({
          x: Math.cos(angle) * radius,
          y: Math.sin(angle) * radius
        });
      }
      
      for (let i = 0; i < 50; i++) {
        const angle = Math.random() * Math.PI + Math.PI;
        const radius = 3 + Math.random() * 2;
        class1.push({
          x: Math.cos(angle) * radius,
          y: Math.sin(angle) * radius
        });
      }
      
      if (knnChart) {
        knnChart.data.datasets[0].data = class0;
        knnChart.data.datasets[1].data = class1;
        knnChart.update();
      }
      
      document.getElementById('knnAcc').textContent = (0.85 + k/200).toFixed(2);
      document.getElementById('knnBoundary').textContent = k < 3 ? 'Very Complex' : k < 10 ? 'Balanced' : 'Smooth';
      document.getElementById('knnPredict').textContent = k < 5 ? 'Fast' : k < 15 ? 'Medium' : 'Slow';
    }

    function updateKNN() { generateKNNData(); }

    // ============================================
    // SVM CHART
    // ============================================
    function initSVMChart() {
      const ctx = document.getElementById('svmChart');
      if (!ctx) return;
      if (svmChart) svmChart.destroy();
      svmChart = new Chart(ctx, {
        type: 'scatter',
        data: { datasets: [{
            label: 'Class 0',
            data: [],
            backgroundColor: getChartColors().primary,
            pointRadius: 5
          }, {
            label: 'Class 1',
            data: [],
            backgroundColor: getChartColors().danger,
            pointRadius: 5
          }, {
            label: 'Support Vectors',
            data: [],
            type: 'scatter',
            backgroundColor: getChartColors().warning,
            pointRadius: 8,
            pointStyle: 'circle',
            borderColor: getChartColors().text,
            borderWidth: 2
          }]
        },
        options: {
          responsive: true,
          maintainAspectRatio: false,
          plugins: { legend: { labels: { color: getChartColors().text } } },
          scales: {
            x: { title: { display: true, text: 'Feature 1', color: getChartColors().text }, grid: { color: getChartColors().grid }, ticks: { color: getChartColors().text } },
            y: { title: { display: true, text: 'Feature 2', color: getChartColors().text }, grid: { color: getChartColors().grid }, ticks: { color: getChartColors().text } }
          }
        }
      });
    }

    function generateSVMData() {
      const C = parseFloat(document.getElementById('svmC').value);
      const gamma = parseFloat(document.getElementById('svmGamma').value);
      document.getElementById('svmCValue').textContent = C.toFixed(1);
      document.getElementById('svmGammaValue').textContent = gamma.toFixed(1);
      
      // Generate linearly separable with some noise
      const class0 = [];
      const class1 = [];
      const supportVectors = [];
      
      for (let i = 0; i < 40; i++) {
        let x, y, label;
        if (i < 20) {
          x = 2 + Math.random() * 3;
          y = 2 + Math.random() * 3;
          label = 0;
        } else {
          x = 6 + Math.random() * 3;
          y = 6 + Math.random() * 3;
          label = 1;
        }
        
        // Add some noise based on C (small C = more noise allowed)
        if (Math.random() < 0.1 / C) {
          if (label === 0) {
            class1.push({ x, y });
          } else {
            class0.push({ x, y });
          }
        } else {
          if (label === 0) class0.push({ x, y });
          else class1.push({ x, y });
        }
      }
      
      // Pick some support vectors near the boundary
      for (let i = 0; i < 8; i++) {
        supportVectors.push({
          x: 4.5 + (Math.random() - 0.5) * 3,
          y: 4.5 + (Math.random() - 0.5) * 3
        });
      }
      
      if (svmChart) {
        svmChart.data.datasets[0].data = class0;
        svmChart.data.datasets[1].data = class1;
        svmChart.data.datasets[2].data = supportVectors;
        svmChart.update();
      }
      
      document.getElementById('svmSV').textContent = supportVectors.length;
      document.getElementById('svmMargin').textContent = (1.0 / C).toFixed(1);
      document.getElementById('svmAcc').textContent = (0.9 - C/50).toFixed(2);
    }

    function updateSVM() { generateSVMData(); }

    // ============================================
    // DECISION TREE CLASSIFICATION
    // ============================================
    function initTreeClassChart() {
      const ctx = document.getElementById('treeClassChart');
      if (!ctx) return;
      if (treeClassChart) treeClassChart.destroy();
      treeClassChart = new Chart(ctx, {
        type: 'scatter',
        data: { datasets: [{
            label: 'Class 0',
            data: [],
            backgroundColor: getChartColors().primary,
            pointRadius: 5
          }, {
            label: 'Class 1',
            data: [],
            backgroundColor: getChartColors().danger,
            pointRadius: 5
          }]
        },
        options: {
          responsive: true,
          maintainAspectRatio: false,
          plugins: { legend: { labels: { color: getChartColors().text } } },
          scales: {
            x: { title: { display: true, text: 'Feature 1', color: getChartColors().text }, grid: { color: getChartColors().grid }, ticks: { color: getChartColors().text } },
            y: { title: { display: true, text: 'Feature 2', color: getChartColors().text }, grid: { color: getChartColors().grid }, ticks: { color: getChartColors().text } }
          }
        }
      });
    }

    function generateTreeClassData() {
      const depth = parseInt(document.getElementById('treeClassDepth').value);
      const minLeaf = parseInt(document.getElementById('treeClassMinLeaf').value);
      document.getElementById('treeClassDepthValue').textContent = depth;
      document.getElementById('treeClassMinLeafValue').textContent = minLeaf;
      
      // Generate checkboard-like pattern
      const class0 = [];
      const class1 = [];
      
      for (let i = 0; i < 100; i++) {
        const x = Math.random() * 10;
        const y = Math.random() * 10;
        
        // Create a complex pattern
        const pattern = (Math.floor(x * 2) % 2) ^ (Math.floor(y * 2) % 2);
        
        if (pattern === 0) {
          class0.push({ x, y });
        } else {
          class1.push({ x, y });
        }
      }
      
      if (treeClassChart) {
        treeClassChart.data.datasets[0].data = class0;
        treeClassChart.data.datasets[1].data = class1;
        treeClassChart.update();
      }
      
      const leaves = Math.pow(2, depth);
      document.getElementById('treeClassTrain').textContent = (0.7 + depth/20).toFixed(2);
      document.getElementById('treeClassTest').textContent = (0.65 + depth/25).toFixed(2);
      document.getElementById('treeClassLeaves').textContent = leaves;
    }

    function updateTreeClass() { generateTreeClassData(); }

    // ============================================
    // COMPARISON CHART
    // ============================================
    function initCompareClassChart() {
      const ctx = document.getElementById('compareClassChart');
      if (!ctx) return;
      if (compareClassChart) compareClassChart.destroy();
      compareClassChart = new Chart(ctx, {
        type: 'scatter',
        data: { datasets: [{
            label: 'Class 0',
            data: [],
            backgroundColor: getChartColors().primary,
            pointRadius: 5
          }, {
            label: 'Class 1',
            data: [],
            backgroundColor: getChartColors().danger,
            pointRadius: 5
          }]
        },
        options: {
          responsive: true,
          maintainAspectRatio: false,
          plugins: { legend: { labels: { color: getChartColors().text } } },
          scales: {
            x: { title: { display: true, text: 'Feature 1', color: getChartColors().text }, grid: { color: getChartColors().grid }, ticks: { color: getChartColors().text } },
            y: { title: { display: true, text: 'Feature 2', color: getChartColors().text }, grid: { color: getChartColors().grid }, ticks: { color: getChartColors().text } }
          }
        }
      });
    }

    function generateCompareData() {
      const class0 = [];
      const class1 = [];
      
      // Generate concentric circles pattern
      for (let i = 0; i < 60; i++) {
        const angle = Math.random() * 2 * Math.PI;
        const radius = 2 + Math.random() * 1.5;
        class0.push({
          x: Math.cos(angle) * radius + 5,
          y: Math.sin(angle) * radius + 5
        });
      }
      
      for (let i = 0; i < 40; i++) {
        const angle = Math.random() * 2 * Math.PI;
        const radius = 4 + Math.random() * 1.5;
        class1.push({
          x: Math.cos(angle) * radius + 5,
          y: Math.sin(angle) * radius + 5
        });
      }
      
      if (compareClassChart) {
        compareClassChart.data.datasets[0].data = class0;
        compareClassChart.data.datasets[1].data = class1;
        compareClassChart.update();
      }
    }

    function compareClassifier(algo) {
      let acc, prec, rec, f1;
      
      switch(algo) {
        case 'logistic':
          acc = 0.72; prec = 0.68; rec = 0.65; f1 = 0.66;
          break;
        case 'knn':
          acc = 0.85; prec = 0.82; rec = 0.80; f1 = 0.81;
          break;
        case 'nb':
          acc = 0.78; prec = 0.75; rec = 0.72; f1 = 0.73;
          break;
        case 'svm':
          acc = 0.88; prec = 0.86; rec = 0.84; f1 = 0.85;
          break;
        case 'tree':
          acc = 0.82; prec = 0.80; rec = 0.78; f1 = 0.79;
          break;
        case 'rf':
          acc = 0.91; prec = 0.89; rec = 0.88; f1 = 0.88;
          break;
      }
      
      document.getElementById('compAcc').textContent = acc.toFixed(2);
      document.getElementById('compPrec').textContent = prec.toFixed(2);
      document.getElementById('compRec').textContent = rec.toFixed(2);
      document.getElementById('compF1').textContent = f1.toFixed(2);
    }

    // ============================================
    // ROC CHART
    // ============================================
    function initROCChart() {
      const ctx = document.getElementById('rocChart');
      if (!ctx) return;
      if (rocChart) rocChart.destroy();
      
      // Generate ROC curve points
      const fpr = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1];
      const tpr = [0, 0.4, 0.65, 0.8, 0.88, 0.92, 0.95, 0.97, 0.99, 1, 1];
      
      rocChart = new Chart(ctx, {
        type: 'line',
        data: {
          datasets: [{
            label: 'ROC Curve (AUC = 0.92)',
            data: fpr.map((fp, i) => ({ x: fp, y: tpr[i] })),
            borderColor: getChartColors().success,
            borderWidth: 3,
            pointRadius: 0,
            fill: false
          }, {
            label: 'Random Classifier',
            data: [{ x: 0, y: 0 }, { x: 1, y: 1 }],
            borderColor: getChartColors().muted2,
            borderWidth: 2,
            borderDash: [5, 5],
            pointRadius: 0,
            fill: false
          }]
        },
        options: {
          responsive: true,
          maintainAspectRatio: false,
          plugins: { legend: { labels: { color: getChartColors().text } } },
          scales: {
            x: { title: { display: true, text: 'False Positive Rate (1 - Specificity)', color: getChartColors().text }, min: 0, max: 1, grid: { color: getChartColors().grid }, ticks: { color: getChartColors().text } },
            y: { title: { display: true, text: 'True Positive Rate (Recall)', color: getChartColors().text }, min: 0, max: 1, grid: { color: getChartColors().grid }, ticks: { color: getChartColors().text } }
          }
        }
      });
    }

    // ============================================
    // DATASET DOWNLOAD
    // ============================================
    function downloadClassDataset() {
      const content = `id,feature1,feature2,feature3,feature4,feature5,is_spam
1,0.23,0.45,1,0,0.67,0
2,0.89,0.12,0,1,0.23,1
3,0.45,0.78,1,1,0.89,1
4,0.12,0.34,0,0,0.45,0
5,0.67,0.23,1,0,0.12,1
6,0.34,0.56,0,1,0.78,0
7,0.78,0.89,1,1,0.34,1
8,0.56,0.67,0,0,0.56,0
9,0.91,0.43,1,0,0.91,1
10,0.22,0.31,0,1,0.22,0
... (500 total rows)`;
      
      const blob = new Blob([content], { type: 'text/csv;charset=utf-8;' });
      const url = URL.createObjectURL(blob);
      const a = document.createElement('a');
      a.href = url;
      a.download = 'classification_practice.csv';
      document.body.appendChild(a);
      a.click();
      document.body.removeChild(a);
      URL.revokeObjectURL(url);
    }

    // Initialize everything
    document.addEventListener('DOMContentLoaded', () => {
      initTheme();
      
      // Add event listeners
      document.getElementById('logisticSlope')?.addEventListener('input', updateLogistic);
      document.getElementById('logisticIntercept')?.addEventListener('input', updateLogistic);
      document.getElementById('logisticThreshold')?.addEventListener('input', updateLogistic);
      document.getElementById('kValue')?.addEventListener('input', updateKNN);
      document.getElementById('svmC')?.addEventListener('input', updateSVM);
      document.getElementById('svmGamma')?.addEventListener('input', updateSVM);
      document.getElementById('treeClassDepth')?.addEventListener('input', updateTreeClass);
      document.getElementById('treeClassMinLeaf')?.addEventListener('input', updateTreeClass);
      
      // Initial chart setup
      setTimeout(() => {
        initLogisticChart();
        generateLogisticData();
      }, 200);
    });

    // Expose functions globally
    window.switchModule = switchModule;
    window.updateLogistic = updateLogistic;
    window.generateLogisticData = generateLogisticData;
    window.updateKNN = updateKNN;
    window.generateKNNData = generateKNNData;
    window.updateSVM = updateSVM;
    window.generateSVMData = generateSVMData;
    window.updateTreeClass = updateTreeClass;
    window.generateTreeClassData = generateTreeClassData;
    window.compareClassifier = compareClassifier;
    window.generateCompareData = generateCompareData;
    window.downloadClassDataset = downloadClassDataset;
  </script>
</body>
</html>
