<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Module 5: Supervised Machine Learning - Regression</title>

  <!-- Icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css"/>

  <!-- Chart.js -->
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>

  <!-- MathJax -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <style>
    /************************************************************
     * THEME: Aurora with Day/Night Mode (KEPT EXACTLY THE SAME)
     ************************************************************/
    :root {
      /* Night Mode (Default) */
      --bg0: #070A12;
      --bg1: #0B1020;
      --panel: #0F172A;
      --panel2: #0B1226;
      --card: #101B34;
      --text: #EAF0FF;
      --muted: rgba(234, 240, 255, 0.75);
      --muted2: rgba(234, 240, 255, 0.62);
      --border: rgba(148, 163, 184, 0.14);
      --shadow: 0 24px 60px rgba(0, 0, 0, 0.35);

      --primary: #60A5FA;
      --secondary: #A78BFA;
      --accent: #F472B6;
      --success: #34D399;
      --warning: #FBBF24;
      --danger: #FB7185;
      --cyan: #22D3EE;

      --radius: 16px;
      --radius2: 22px;
      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;

      --ring: 0 0 0 4px rgba(96, 165, 250, 0.22);
      --ring2: 0 0 0 4px rgba(167, 139, 250, 0.20);

      /* Theme toggle variables */
      --mode-icon: '\f186'; /* moon */
      --mode-label: 'Night Mode';
    }

    /* Day Mode Theme */
    [data-theme="day"] {
      --bg0: #F8FAFC;
      --bg1: #F1F5F9;
      --panel: #FFFFFF;
      --panel2: #F8FAFC;
      --card: #FFFFFF;
      --text: #0F172A;
      --muted: rgba(15, 23, 42, 0.75);
      --muted2: rgba(15, 23, 42, 0.62);
      --border: rgba(148, 163, 184, 0.25);
      --shadow: 0 20px 40px rgba(0, 0, 0, 0.08);

      --primary: #2563EB;
      --secondary: #7C3AED;
      --accent: #DB2777;
      --success: #10B981;
      --warning: #F59E0B;
      --danger: #EF4444;
      --cyan: #06B6D4;

      --ring: 0 0 0 4px rgba(37, 99, 235, 0.15);
      --ring2: 0 0 0 4px rgba(124, 58, 237, 0.15);

      --mode-icon: '\f185'; /* sun */
      --mode-label: 'Day Mode';
    }

    * { margin: 0; padding: 0; box-sizing: border-box; }
    html { scroll-behavior: smooth; }

    body {
      font-family: system-ui, -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
      background: var(--bg0);
      color: var(--text);
      line-height: 1.7;
      min-height: 100vh;
      transition: background-color 0.35s ease, color 0.35s ease;
      overflow-x: hidden;
    }

    /* Dynamic background gradients */
    body:not([data-theme="day"]) {
      background:
        radial-gradient(1200px 520px at 15% 0%, rgba(96, 165, 250, 0.18), transparent 55%),
        radial-gradient(900px 520px at 90% 10%, rgba(244, 114, 182, 0.16), transparent 55%),
        radial-gradient(900px 520px at 75% 90%, rgba(34, 211, 238, 0.10), transparent 55%),
        linear-gradient(180deg, var(--bg0), var(--bg1));
    }
    body[data-theme="day"] {
      background:
        radial-gradient(1200px 520px at 15% 0%, rgba(37, 99, 235, 0.08), transparent 55%),
        radial-gradient(900px 520px at 90% 10%, rgba(219, 39, 119, 0.08), transparent 55%),
        radial-gradient(900px 520px at 75% 90%, rgba(6, 182, 212, 0.06), transparent 55%),
        linear-gradient(180deg, var(--bg0), var(--bg1));
    }

    .container { max-width: 1400px; margin: 0 auto; padding: 22px; }

    /************************************************************
     * THEME TOGGLE
     ************************************************************/
    .theme-toggle-container {
      position: fixed;
      top: 20px;
      right: 20px;
      z-index: 1000;
    }
    .theme-toggle {
      display: inline-flex;
      align-items: center;
      gap: 10px;
      padding: 12px 18px;
      border-radius: 50px;
      background: var(--panel);
      border: 1px solid var(--border);
      box-shadow: var(--shadow);
      cursor: pointer;
      font-weight: 600;
      color: var(--text);
      backdrop-filter: blur(10px);
      transition: transform 0.2s ease, box-shadow 0.2s ease;
      border: none;
      font-family: inherit;
    }
    .theme-toggle:hover { transform: translateY(-2px); box-shadow: 0 8px 25px rgba(0, 0, 0, 0.15); }
    .theme-toggle::before {
      content: var(--mode-icon);
      font-family: 'Font Awesome 6 Free';
      font-weight: 900;
      font-size: 1.1em;
    }
    .theme-toggle .label { font-size: 0.9em; opacity: 0.9; white-space: nowrap; }

    /************************************************************
     * HEADER
     ************************************************************/
    header {
      background: linear-gradient(135deg, rgba(15, 23, 42, 0.90), rgba(16, 27, 52, 0.92));
      border: 1px solid rgba(148, 163, 184, 0.18);
      color: var(--text);
      padding: 28px;
      border-radius: var(--radius2);
      box-shadow: var(--shadow);
      margin-bottom: 24px;
      position: relative;
      overflow: hidden;
      backdrop-filter: blur(10px);
    }
    body[data-theme="day"] header {
      background: linear-gradient(135deg, rgba(255, 255, 255, 0.95), rgba(248, 250, 252, 0.98));
      border: 1px solid rgba(148, 163, 184, 0.25);
    }
    .course-info h1 {
      font-size: clamp(24px, 2.8vw, 38px);
      margin-bottom: 8px;
      letter-spacing: -0.5px;
      line-height: 1.2;
      background: linear-gradient(135deg, var(--primary), var(--secondary));
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
    }
    .course-info h2 {
      font-size: clamp(16px, 1.8vw, 20px);
      font-weight: 400;
      opacity: 0.9;
      margin-bottom: 12px;
      color: var(--muted);
    }
    .badge {
      display: inline-flex;
      align-items: center;
      gap: 10px;
      padding: 10px 18px;
      border-radius: 50px;
      font-weight: 700;
      letter-spacing: 0.2px;
      border: 1px solid var(--border);
      background: rgba(255, 255, 255, 0.08);
      width: fit-content;
      margin-top: 10px;
    }

    /************************************************************
     * NAVIGATION
     ************************************************************/
    .module-nav {
      display: flex;
      gap: 10px;
      margin: 20px 0;
      flex-wrap: wrap;
    }
    .nav-btn {
      display: inline-flex;
      align-items: center;
      gap: 8px;
      padding: 12px 20px;
      border-radius: 50px;
      background: var(--panel);
      border: 1px solid var(--border);
      color: var(--text);
      font-weight: 600;
      cursor: pointer;
      transition: all 0.2s ease;
      text-decoration: none;
      border: none;
      font-family: inherit;
      font-size: 14px;
    }
    .nav-btn:hover { transform: translateY(-2px); border-color: var(--primary); box-shadow: var(--ring); }
    .nav-btn.active { background: linear-gradient(135deg, var(--primary), var(--secondary)); color: white; border: none; }

    /************************************************************
     * MAIN CONTENT
     ************************************************************/
    .module-content { display: none; }
    .module-content.active { display: block; animation: fadeIn 0.5s ease; }
    @keyframes fadeIn { from { opacity: 0; transform: translateY(10px); } to { opacity: 1; transform: translateY(0); } }

    .panel {
      background: var(--panel);
      border-radius: var(--radius);
      border: 1px solid var(--border);
      box-shadow: var(--shadow);
      overflow: hidden;
      margin-bottom: 24px;
      transition: transform 0.2s ease, box-shadow 0.2s ease;
    }
    .panel:hover { box-shadow: 0 20px 40px rgba(0, 0, 0, 0.1); transform: translateY(-2px); }
    .section { padding: 24px; }
    .section h3 {
      color: var(--text);
      margin-bottom: 20px;
      font-size: 1.4rem;
      display: flex;
      align-items: center;
      gap: 12px;
      padding-bottom: 12px;
      border-bottom: 1px solid var(--border);
    }
    .section h4 {
      color: var(--text);
      margin: 20px 0 12px;
      font-size: 1.2rem;
      display: flex;
      align-items: center;
      gap: 10px;
    }
    .section p { margin-bottom: 15px; color: var(--muted); }

    /************************************************************
     * EXPANDED CONTENT STYLES (SIMPLIFIED FOR STUDENTS)
     ************************************************************/
    .expanded-content {
      margin: 20px 0;
      padding: 20px;
      background: var(--panel2);
      border-radius: var(--radius);
      border: 1px solid var(--border);
    }

    .numeric-example {
      font-family: var(--mono);
      background: rgba(15, 23, 42, 0.15);
      padding: 15px;
      border-radius: 8px;
      margin: 15px 0;
      border: 1px solid var(--border);
    }
    body[data-theme="day"] .numeric-example { background: rgba(15, 23, 42, 0.05); }
    .example-title {
      color: var(--accent);
      font-weight: 600;
      margin-bottom: 10px;
      display: flex;
      align-items: center;
      gap: 8px;
    }

    .formula-with-calc {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 20px;
      margin: 20px 0;
    }
    @media (max-width: 768px) { .formula-with-calc { grid-template-columns: 1fr; } }
    .formula-box {
      background: rgba(15, 23, 42, 0.1);
      border: 1px solid var(--border);
      border-radius: 12px;
      padding: 20px;
      margin: 16px 0;
      font-family: var(--mono);
      text-align: center;
      position: relative;
    }
    body[data-theme="day"] .formula-box { background: rgba(15, 23, 42, 0.05); }
    .formula-label {
      position: absolute;
      top: -10px;
      left: 20px;
      background: var(--panel);
      padding: 0 10px;
      font-size: 12px;
      color: var(--muted);
      font-weight: 600;
    }

    /************************************************************
     * STUDENT-FRIENDLY EXPLANATION BOXES (SIMPLIFIED LANGUAGE)
     ************************************************************/
    .explanation-box {
      background: linear-gradient(135deg, rgba(96, 165, 250, 0.05), rgba(167, 139, 250, 0.05));
      border-left: 4px solid var(--primary);
      padding: 20px;
      margin: 20px 0;
      border-radius: 0 var(--radius) var(--radius) 0;
      position: relative;
    }
    .explanation-box::before {
      content: 'üìö In Simple Terms';
      position: absolute;
      top: -10px;
      left: 20px;
      background: var(--panel);
      padding: 0 10px;
      font-size: 12px;
      font-weight: 600;
      color: var(--primary);
      text-transform: uppercase;
      letter-spacing: 0.5px;
    }
    .explanation-box p:last-child { margin-bottom: 0; }

    .pro-tip {
      background: linear-gradient(135deg, rgba(52, 211, 153, 0.05), rgba(16, 185, 129, 0.05));
      border-left: 4px solid var(--success);
      padding: 20px;
      margin: 20px 0;
      border-radius: 0 var(--radius) var(--radius) 0;
      position: relative;
    }
    .pro-tip::before {
      content: 'üí° Study Tip';
      position: absolute;
      top: -10px;
      left: 20px;
      background: var(--panel);
      padding: 0 10px;
      font-size: 12px;
      font-weight: 600;
      color: var(--success);
      text-transform: uppercase;
      letter-spacing: 0.5px;
    }

    .common-pitfall {
      background: linear-gradient(135deg, rgba(251, 113, 133, 0.05), rgba(239, 68, 68, 0.05));
      border-left: 4px solid var(--danger);
      padding: 20px;
      margin: 20px 0;
      border-radius: 0 var(--radius) var(--radius) 0;
      position: relative;
    }
    .common-pitfall::before {
      content: '‚ö†Ô∏è Watch Out!';
      position: absolute;
      top: -10px;
      left: 20px;
      background: var(--panel);
      padding: 0 10px;
      font-size: 12px;
      font-weight: 600;
      color: var(--danger);
      text-transform: uppercase;
      letter-spacing: 0.5px;
    }

    /************************************************************
     * REGRESSION VISUALIZATION
     ************************************************************/
    .regression-container {
      display: grid;
      grid-template-columns: 2fr 1fr;
      gap: 24px;
      margin: 20px 0;
    }
    @media (max-width: 1100px) { .regression-container { grid-template-columns: 1fr; } }
    .regression-chart {
      height: 400px;
      border-radius: var(--radius);
      border: 1px solid var(--border);
      background: var(--panel2);
      position: relative;
      overflow: hidden;
    }
    .regression-controls {
      padding: 20px;
      background: var(--panel2);
      border-radius: var(--radius);
      border: 1px solid var(--border);
    }
    .control-group { margin-bottom: 24px; }
    .control-group h4 {
      color: var(--text);
      margin-bottom: 16px;
      font-size: 15px;
      display: flex;
      align-items: center;
      gap: 10px;
    }
    .slider-container { display: flex; align-items: center; gap: 15px; margin-bottom: 15px; }
    .slider-container label { min-width: 160px; color: var(--muted); font-size: 14px; }
    input[type="range"] { flex: 1; height: 6px; border-radius: 3px; background: var(--border); outline: none; accent-color: var(--primary); }
    .slider-value { min-width: 50px; text-align: right; font-family: var(--mono); font-weight: 600; color: var(--text); }

    /************************************************************
     * 3D SURFACE PLOT FOR MULTIPLE REGRESSION
     ************************************************************/
    .surface-plot-container {
      width: 100%;
      height: 400px;
      position: relative;
      background: var(--panel2);
      border-radius: var(--radius);
      border: 1px solid var(--border);
      overflow: hidden;
      margin: 20px 0;
    }
    .surface-canvas {
      width: 100%;
      height: 100%;
      display: block;
    }
    .surface-controls {
      display: flex;
      gap: 15px;
      flex-wrap: wrap;
      margin: 15px 0;
    }

    /************************************************************
     * ALGORITHM COMPARISON
     ************************************************************/
    .algorithm-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
      gap: 20px;
      margin: 20px 0;
    }
    .algorithm-card {
      background: var(--panel2);
      border-radius: var(--radius);
      border: 1px solid var(--border);
      overflow: hidden;
      transition: transform 0.2s ease;
    }
    .algorithm-card:hover { transform: translateY(-4px); border-color: var(--primary); }
    .algorithm-header {
      padding: 16px;
      background: linear-gradient(135deg, rgba(96, 165, 250, 0.1), rgba(167, 139, 250, 0.1));
      border-bottom: 1px solid var(--border);
      display: flex;
      align-items: center;
      gap: 12px;
    }
    .algorithm-icon {
      width: 40px;
      height: 40px;
      border-radius: 10px;
      background: linear-gradient(135deg, var(--primary), var(--secondary));
      display: flex;
      align-items: center;
      justify-content: center;
      color: white;
    }
    .algorithm-title { flex: 1; }
    .algorithm-title h4 { margin: 0; font-size: 16px; color: var(--text); }
    .algorithm-title p { margin: 2px 0 0; font-size: 12px; color: var(--muted); }
    .algorithm-body { padding: 16px; }
    .algorithm-metric {
      display: flex;
      justify-content: space-between;
      margin-bottom: 8px;
      font-size: 13px;
      color: var(--muted);
    }
    .algorithm-metric span:last-child { font-weight: 600; color: var(--text); }
    .algorithm-progress { height: 4px; background: var(--border); border-radius: 2px; margin: 8px 0 16px; overflow: hidden; }
    .progress-bar { height: 100%; background: linear-gradient(90deg, var(--primary), var(--secondary)); border-radius: 2px; }

    /************************************************************
     * CODE EDITOR STYLES (WITH BETTER CODE COMMENTS)
     ************************************************************/
    .code-editor-container {
      margin: 20px 0;
      border-radius: var(--radius);
      overflow: hidden;
      border: 1px solid var(--border);
    }
    .code-header {
      background: var(--panel);
      padding: 12px 20px;
      border-bottom: 1px solid var(--border);
      display: flex;
      justify-content: space-between;
      align-items: center;
    }
    .code-title { font-weight: 600; color: var(--text); display: flex; align-items: center; gap: 8px; }
    .code-actions { display: flex; gap: 8px; }
    .code-body { background: var(--panel2); padding: 0; overflow: auto; }
    .code-block { font-family: var(--mono); font-size: 14px; line-height: 1.6; padding: 20px; margin: 0; tab-size: 2; white-space: pre-wrap; min-height: 200px; }
    .code-block code { display: block; color: var(--text); }
    .keyword { color: var(--danger); }
    .function { color: var(--primary); }
    .string { color: var(--success); }
    .comment { color: var(--muted2); font-style: italic; }
    .number { color: var(--warning); }
    .operator { color: var(--cyan); }

    /************************************************************
     * METRICS DISPLAY
     ************************************************************/
    .metrics-container {
      display: grid;
      grid-template-columns: repeat(4, 1fr);
      gap: 15px;
      margin: 20px 0;
    }
    @media (max-width: 1100px) { .metrics-container { grid-template-columns: repeat(2, 1fr); } }
    @media (max-width: 600px) { .metrics-container { grid-template-columns: 1fr; } }
    .metric-card { padding: 20px; border-radius: var(--radius); border: 1px solid var(--border); background: var(--panel2); text-align: center; }
    .metric-value { font-size: 28px; font-weight: 700; margin: 10px 0; }
    .metric-label { font-size: 13px; color: var(--muted); text-transform: uppercase; letter-spacing: 0.5px; }
    .metric-good { color: var(--success); }
    .metric-warning { color: var(--warning); }
    .metric-danger { color: var(--danger); }

    /************************************************************
     * DETAILED TABLES
     ************************************************************/
    .detailed-table {
      width: 100%;
      border-collapse: separate;
      border-spacing: 0;
      overflow: hidden;
      border-radius: 12px;
      border: 1px solid var(--border);
      background: var(--panel2);
      margin: 16px 0;
      font-size: 13px;
    }
    .detailed-table th, .detailed-table td {
      padding: 12px 16px;
      border-bottom: 1px solid var(--border);
      text-align: left;
    }
    .detailed-table th { background: var(--panel); color: var(--muted); font-weight: 600; }
    .detailed-table td { color: var(--text); font-family: var(--mono); }
    .detailed-table tr:last-child td { border-bottom: none; }
    .highlight-cell { background: rgba(96, 165, 250, 0.1); color: var(--primary); font-weight: 600; }

    /************************************************************
     * BUTTONS
     ************************************************************/
    .btn {
      display: inline-flex;
      align-items: center;
      gap: 10px;
      padding: 12px 20px;
      border-radius: 12px;
      font-weight: 600;
      border: none;
      cursor: pointer;
      transition: all 0.2s ease;
      font-family: inherit;
      font-size: 14px;
    }
    .btn-primary { background: linear-gradient(135deg, var(--primary), var(--secondary)); color: white; }
    .btn-secondary { background: var(--panel); color: var(--text); border: 1px solid var(--border); }
    .btn:hover:not(:disabled) { transform: translateY(-2px); box-shadow: var(--ring); }
    .btn:disabled { opacity: 0.5; cursor: not-allowed; transform: none !important; }
    .button-group { display: flex; gap: 10px; margin: 20px 0; flex-wrap: wrap; }

    /************************************************************
     * INFO BOXES
     ************************************************************/
    .info-box { padding: 20px; border-radius: 12px; border: 1px solid var(--border); margin: 20px 0; }
    .info-box.warning { border-left: 4px solid var(--warning); background: rgba(251, 191, 36, 0.08); }
    .info-box.success { border-left: 4px solid var(--success); background: rgba(52, 211, 153, 0.08); }
    .info-box.danger { border-left: 4px solid var(--danger); background: rgba(251, 113, 133, 0.08); }
    .info-box.info { border-left: 4px solid var(--primary); background: rgba(96, 165, 250, 0.08); }
    .info-box h4 { color: var(--text); margin-bottom: 10px; display: flex; align-items: center; gap: 10px; }

    /************************************************************
     * CONCEPT CARDS
     ************************************************************/
    .concept-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin: 20px 0; }
    .concept-card {
      padding: 20px;
      border-radius: var(--radius);
      border: 1px solid var(--border);
      background: var(--panel2);
      transition: transform 0.2s ease;
    }
    .concept-card:hover { transform: translateY(-4px); border-color: var(--primary); }
    .concept-card h5 { color: var(--text); margin-bottom: 12px; font-size: 16px; display: flex; align-items: center; gap: 10px; }
    .concept-card p { color: var(--muted); font-size: 14px; line-height: 1.6; }

    /************************************************************
     * TIMELINE STYLES
     ************************************************************/
    .timeline { position: relative; max-width: 1200px; margin: 40px auto; }
    .timeline::after {
      content: '';
      position: absolute;
      width: 4px;
      background: linear-gradient(to bottom, var(--primary), var(--secondary));
      top: 0;
      bottom: 0;
      left: 50%;
      margin-left: -2px;
      border-radius: 2px;
    }
    @media (max-width: 768px) { .timeline::after { left: 31px; } }
    .timeline-item { padding: 10px 40px; position: relative; width: 50%; box-sizing: border-box; margin: 20px 0; }
    .timeline-item:nth-child(odd) { left: 0; }
    .timeline-item:nth-child(even) { left: 50%; }
    @media (max-width: 768px) {
      .timeline-item { width: 100%; padding-left: 70px; padding-right: 25px; }
      .timeline-item:nth-child(even) { left: 0; }
    }
    .timeline-content {
      padding: 20px;
      background: var(--panel2);
      border-radius: var(--radius);
      border: 1px solid var(--border);
      position: relative;
      transition: transform 0.2s ease, box-shadow 0.2s ease;
    }
    .timeline-content:hover { transform: translateY(-3px); box-shadow: var(--shadow); border-color: var(--primary); }
    .timeline-item:nth-child(odd) .timeline-content::after,
    .timeline-item:nth-child(even) .timeline-content::after {
      content: '';
      position: absolute;
      width: 20px;
      height: 20px;
      background: var(--panel2);
      border: 2px solid var(--primary);
      top: 50%;
      transform: translateY(-50%) rotate(45deg);
      border-radius: 3px;
    }
    .timeline-item:nth-child(odd) .timeline-content::after { right: -10px; }
    .timeline-item:nth-child(even) .timeline-content::after { left: -10px; }
    @media (max-width: 768px) {
      .timeline-content::after { left: -10px !important; right: auto !important; }
    }
    .timeline-icon {
      position: absolute;
      width: 40px;
      height: 40px;
      background: linear-gradient(135deg, var(--primary), var(--secondary));
      border-radius: 50%;
      display: flex;
      align-items: center;
      justify-content: center;
      color: white;
      z-index: 1;
      top: 50%;
      transform: translateY(-50%);
      right: -20px;
    }
    .timeline-item:nth-child(even) .timeline-icon { left: -20px; right: auto; }
    @media (max-width: 768px) { .timeline-icon { left: 21px !important; right: auto !important; } }
    .timeline-date { font-weight: 600; color: var(--primary); margin-bottom: 8px; display: flex; align-items: center; gap: 8px; }
    .timeline-title { font-size: 1.1rem; margin-bottom: 10px; color: var(--text); }
    .timeline-description { color: var(--muted); font-size: 0.95rem; line-height: 1.6; }
    .tech-stack { display: flex; flex-wrap: wrap; gap: 8px; margin-top: 12px; }
    .tech-tag { padding: 4px 10px; background: rgba(96, 165, 250, 0.1); border-radius: 20px; font-size: 0.8rem; color: var(--primary); font-family: var(--mono); }

    /************************************************************
     * FOOTER
     ************************************************************/
    footer {
      text-align: center;
      margin-top: 32px;
      padding-top: 24px;
      border-top: 1px solid var(--border);
      color: var(--muted2);
      font-size: 14px;
      line-height: 1.6;
    }

    /************************************************************
     * UTILITY CLASSES
     ************************************************************/
    .highlight { background: rgba(96, 165, 250, 0.1); padding: 2px 6px; border-radius: 4px; color: var(--primary); font-weight: 600; }
    .mono { font-family: var(--mono); background: rgba(15, 23, 42, 0.1); padding: 2px 6px; border-radius: 4px; color: var(--text); }
    .math-equation { font-family: "Times New Roman", serif; font-style: italic; font-size: 1.1em; }

    .library-badge {
      display: inline-flex;
      align-items: center;
      gap: 6px;
      padding: 6px 12px;
      border-radius: 20px;
      font-size: 12px;
      font-weight: 600;
      margin: 2px;
    }
    .sklearn-badge { background: rgba(231, 76, 60, 0.1); color: #E74C3C; border: 1px solid rgba(231, 76, 60, 0.3); }
    .xgb-badge { background: rgba(0, 102, 102, 0.1); color: #006666; border: 1px solid rgba(0, 102, 102, 0.3); }
    .rf-badge { background: rgba(39, 174, 96, 0.1); color: #27AE60; border: 1px solid rgba(39, 174, 96, 0.3); }
    .gb-badge { background: rgba(241, 196, 15, 0.1); color: #F1C40F; border: 1px solid rgba(241, 196, 15, 0.3); }

    a { color: var(--primary); }
    a:hover { opacity: 0.9; }
  </style>
</head>

<body data-theme="night">
  <!-- Theme Toggle -->
  <div class="theme-toggle-container">
    <button class="theme-toggle" id="themeToggle" type="button" aria-label="Theme: Night Mode (click to toggle)">
      <span class="label" id="themeLabel">Night Mode</span>
    </button>
  </div>

  <div class="container">
    <header>
      <div class="course-info">
        <h1>üìà Module 5: Supervised Machine Learning - Regression</h1>
        <h2>Supervised Machine Learning</h2>
        <div class="badge"><i class="fas fa-chart-line"></i> IAF 604: Learn how to predict house prices, sales, and more!</div>
        <p style="margin-top: 15px; color: var(--muted);">
          <strong>üéØ Your Learning Goals:</strong> By the end of this module, you'll understand how different regression algorithms work, 
          when to use each one, and how to tell if your predictions can be trusted.
        </p>
      </div>
    </header>

    <!-- Navigation - Expanded with new sections -->
    <div class="module-nav">
      <button class="nav-btn active" data-module="intro"><i class="fas fa-graduation-cap"></i> 1. What is Regression?</button>
      <button class="nav-btn" data-module="simple-linear"><i class="fas fa-chart-line"></i> 2. Simple Linear</button>
      <button class="nav-btn" data-module="multiple-linear"><i class="fas fa-cubes"></i> 3. Multiple Linear</button>
      <button class="nav-btn" data-module="polynomial"><i class="fas fa-wave-square"></i> 4. Polynomial</button>
      <button class="nav-btn" data-module="regularized"><i class="fas fa-sliders-h"></i> 5. Regularized Linear</button>
      <button class="nav-btn" data-module="tree-models"><i class="fas fa-tree"></i> 6. Decision Trees</button>
      <button class="nav-btn" data-module="ensemble"><i class="fas fa-layer-group"></i> 7. Random Forest & XGBoost</button>
      <button class="nav-btn" data-module="evaluation"><i class="fas fa-clipboard-check"></i> 8. How to Measure Success</button>
      <button class="nav-btn" data-module="hands-on"><i class="fas fa-laptop-code"></i> 9. Try It Yourself!</button>
      <button class="nav-btn" data-module="resources"><i class="fas fa-book"></i> 10. Practice Resources</button>
    </div>

    <!-- ================================================
         SECTION 1: INTRODUCTION - SIMPLIFIED FOR STUDENTS
    ================================================= -->
    <div id="intro" class="module-content active">

      <div class="panel section">
        <h3><i class="fas fa-graduation-cap"></i> 1.1 Supervised Learning: Learning with Examples</h3>

        <div class="info-box info">
          <h4><i class="fas fa-brain"></i> The Basic Idea</h4>
          <p>
            Imagine you're trying to teach a friend to estimate house prices. You'd show them examples:
            <span class="highlight">"This house has 3 bedrooms, 1500 sq ft, and sold for $300,000"</span>.
            After seeing enough examples, they'd learn to predict prices for new houses.
          </p>
          <p>
            <strong>That's supervised learning!</strong> The computer learns from labeled examples
            (features + correct answers) to make predictions on new data.
          </p>
        </div>

        <div class="explanation-box">
          <p><strong>üìö In Simple Terms:</strong> Think of it like studying for a test with an answer key. You practice on problems where you know the answers (training data), and then you take a test with new problems (test data). The goal is to do well on both!</p>
          <p><strong>Real-Life Example:</strong> Netflix uses supervised learning to predict which movies you'll like. They've seen what you watched before (features) and your ratings (target), so they can recommend new movies you haven't seen yet.</p>
        </div>

        <div class="expanded-content">
          <h4><i class="fas fa-sitemap"></i> Two Flavors of Supervised Learning</h4>

          <div class="concept-grid">
            <div class="concept-card">
              <h5><i class="fas fa-chart-line"></i> Regression ‚Üí Predicts Numbers</h5>
              <p><strong>Question it answers:</strong> "How much?" or "How many?"</p>
              <p><strong>Examples:</strong> 
                <ul style="padding-left:20px; color:var(--muted);">
                  <li>üè† House price: $425,000</li>
                  <li>üì¶ Tomorrow's sales: 1,250 units</li>
                  <li>üå°Ô∏è Temperature: 72.5¬∞F</li>
                  <li>‚è±Ô∏è Delivery time: 35 minutes</li>
                </ul>
              </p>
            </div>

            <div class="concept-card">
              <h5><i class="fas fa-tags"></i> Classification ‚Üí Predicts Categories</h5>
              <p><strong>Question it answers:</strong> "Which category?"</p>
              <p><strong>Examples:</strong>
                <ul style="padding-left:20px; color:var(--muted);">
                  <li>üìß Spam? Yes or No</li>
                  <li>üè• Disease? Diabetes or Healthy</li>
                  <li>üí≥ Fraud? Fraudulent or Legitimate</li>
                  <li>‚≠ê Rating: 1, 2, 3, 4, or 5 stars</li>
                </ul>
              </p>
            </div>
          </div>

          <div class="pro-tip">
            <p><strong>üí° Easy Rule:</strong> If your answer is a number (like price, age, temperature) ‚Üí use regression. If your answer is a category (like "spam" or "not spam") ‚Üí use classification. But sometimes we use regression to predict probabilities (like 75% chance of rain) and then turn that into a category!</p>
          </div>
        </div>

        <h4><i class="fas fa-table"></i> Let's Look at Real Data</h4>
        <p>
          Here's a tiny dataset of house sales. Notice how we have <span class="mono">features</span> (what we know about the house) 
          and a <span class="mono">target</span> (what we want to predict).
        </p>

        <table class="detailed-table">
          <thead>
            <tr>
              <th>üè† sqft (Feature)</th><th>üõèÔ∏è bedrooms (Feature)</th><th>üìÖ age (Feature)</th><th>üí∞ price (Target)</th>
            </tr>
          </thead>
          <tbody>
            <tr><td>1200</td><td>3</td><td>10</td><td class="highlight-cell">$250,000</td></tr>
            <tr><td>1800</td><td>4</td><td>5</td><td class="highlight-cell">$450,000</td></tr>
            <tr><td>950</td><td>2</td><td>20</td><td class="highlight-cell">$180,000</td></tr>
            <tr><td>1500</td><td>3</td><td>12</td><td class="highlight-cell">$320,000</td></tr>
          </tbody>
        </table>

        <div class="explanation-box">
          <p><strong>üîç What patterns do you see?</strong> Bigger houses (sqft) tend to cost more. Newer houses (lower age) seem pricier. But a 3-bedroom house can cost $250k or $320k depending on size and age. The computer's job is to learn these patterns automatically!</p>
        </div>
      </div>

      <div class="panel section">
        <h3><i class="fas fa-question-circle"></i> 1.2 What Exactly IS Regression?</h3>

        <div class="info-box info">
          <h4><i class="fas fa-lightbulb"></i> The Definition</h4>
          <p>
            <strong>Regression</strong> is when we predict a <span class="highlight">continuous number</span>.
            "Continuous" just means it can be any value within a range, not just whole numbers.
          </p>
        </div>

        <div class="formula-with-calc">
          <div class="formula-box">
            <div class="formula-label">The Math Version</div>
            <div style="text-align:left; padding: 15px;">
              <span class="math-equation">y = f(X) + Œµ</span><br/><br/>
              <strong>Translation:</strong><br/>
              ‚Ä¢ <strong>y</strong> = what we're predicting (price)<br/>
              ‚Ä¢ <strong>X</strong> = our input features (sqft, bedrooms)<br/>
              ‚Ä¢ <strong>f</strong> = the pattern the computer learns<br/>
              ‚Ä¢ <strong>Œµ</strong> = things we can't predict (random noise)
            </div>
          </div>

          <div class="numeric-example">
            <div class="example-title"><i class="fas fa-bullseye"></i> Why "Œµ" (Error) Always Exists</div>
            <p>
              Two identical houses (same sqft, bedrooms, age) might sell for different prices because:
              <ul style="margin-left:20px;">
                <li>One has a better school district (we didn't measure that)</li>
                <li>One was sold in summer vs winter (timing matters)</li>
                <li>The buyers just really loved the paint color (random!)</li>
              </ul>
              Good models capture the pattern (f(X)), but can't predict the randomness (Œµ).
            </p>
          </div>
        </div>

        <div class="common-pitfall">
          <p><strong>‚ö†Ô∏è The "Perfect Prediction" Myth:</strong> New students often expect their model to predict perfectly every time. But if your model gets 100% accuracy on real data, you've probably made a mistake! Real data always has some randomness (Œµ) that no model can predict. A good model gets close, not perfect.</p>
        </div>
      </div>
    </div>

    <!-- ================================================
         SECTION 2: SIMPLE LINEAR REGRESSION (1 FEATURE)
    ================================================= -->
    <div id="simple-linear" class="module-content">

      <div class="panel section">
        <h3><i class="fas fa-chart-line"></i> 2.1 Simple Linear Regression: One Feature, One Line</h3>

        <div class="info-box info">
          <h4><i class="fas fa-lightbulb"></i> The Core Idea</h4>
          <p>
            <strong>Simple linear regression</strong> uses just ONE feature to predict your target.
            It's the line you drew in algebra class: <span class="math-equation">y = mx + b</span>
          </p>
          <p>
            <span class="mono">m</span> is the slope (how much y increases when x increases by 1), and 
            <span class="mono">b</span> is the intercept (the value when x=0).
          </p>
        </div>

        <div class="explanation-box">
          <p><strong>üìö In Simple Terms:</strong> Imagine you're looking at house sizes and prices. Simple linear regression tries to draw the straight line that comes closest to all your data points. It's like finding the best-fitting line through a scatter plot.</p>
          <p><strong>Real-Life Example:</strong> If you've ever used a ruler to draw a line through dots on graph paper, you've done the same thing simple linear regression does mathematically!</p>
        </div>

        <div class="regression-container">
          <div class="regression-chart"><canvas id="linearChart"></canvas></div>

          <div class="regression-controls">
            <div class="control-group">
              <h4><i class="fas fa-sliders-h"></i> Play with the Demo</h4>
              <div class="slider-container">
                <label><i class="fas fa-random"></i> Noise Level:</label>
                <input type="range" id="noiseLevel" min="0" max="50" step="5" value="20">
                <span class="slider-value" id="noiseLevelValue">20</span>
              </div>
              <div class="slider-container">
                <label><i class="fas fa-chart-simple"></i> Sample Size:</label>
                <input type="range" id="sampleSize" min="20" max="200" step="10" value="100">
                <span class="slider-value" id="sampleSizeValue">100</span>
              </div>
            </div>

            <div class="button-group">
              <button class="btn btn-primary" onclick="generateLinearData()">
                <i class="fas fa-sync"></i> Generate New Random Data
              </button>
            </div>

            <div class="metrics-container" style="margin-top: 20px;">
              <div class="metric-card">
                <div class="metric-label">True Slope</div>
                <div class="metric-value" id="trueSlope">2.5</div>
                <div style="font-size:11px;">(hidden pattern)</div>
              </div>
              <div class="metric-card">
                <div class="metric-label">Found Slope</div>
                <div class="metric-value" id="estSlope">0.00</div>
                <div style="font-size:11px;">(model's estimate)</div>
              </div>
              <div class="metric-card">
                <div class="metric-label">R¬≤ Score</div>
                <div class="metric-value" id="lrR2">0.00</div>
                <div style="font-size:11px;">0-1 scale, higher=better</div>
              </div>
              <div class="metric-card">
                <div class="metric-label">MSE</div>
                <div class="metric-value" id="lrMSE">0.00</div>
                <div style="font-size:11px;">lower=better</div>
              </div>
            </div>

            <div class="explanation-box" style="margin-top: 20px;">
              <p><strong>üëÜ Try this:</strong></p>
              <ul style="padding-left:20px;">
                <li><strong>Increase Noise:</strong> See how the points scatter more? The red line becomes less accurate.</li>
                <li><strong>Increase Samples:</strong> More data helps the red line find the true pattern (slope 2.5).</li>
                <li><strong>Watch R¬≤:</strong> When noise is low, R¬≤ is high (close to 1). When noise is high, R¬≤ drops.</li>
              </ul>
            </div>
          </div>
        </div>

        <div class="code-editor-container" style="margin-top: 20px;">
          <div class="code-header">
            <div class="code-title"><i class="fab fa-python"></i> Python Code: Simple Linear Regression</div>
            <div class="code-actions"><span class="library-badge sklearn-badge"><i class="fas fa-cube"></i> scikit-learn</span></div>
          </div>
          <div class="code-body">
            <pre class="code-block"><code><span class="comment"># 1. Import the tool</span>
<span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression
<span class="keyword">import</span> numpy <span class="keyword">as</span> np

<span class="comment"># 2. Create sample data (e.g., house sizes in sqft and prices)</span>
<span class="comment"># X must be 2D: rows = samples, columns = features</span>
X = np.array([1200, 1800, 950, 1500]).reshape(-1, 1)  <span class="comment"># One feature: sqft</span>
y = np.array([250000, 450000, 180000, 320000])        <span class="comment"># Target: price</span>

<span class="comment"># 3. Create and train the model</span>
model = LinearRegression()
model.<span class="function">fit</span>(X, y)

<span class="comment"># 4. See what it learned</span>
<span class="keyword">print</span>(<span class="string">f"Slope (coefficient): {model.coef_[0]:.2f}"</span>)
<span class="keyword">print</span>(<span class="string">f"Intercept: {model.intercept_:.2f}"</span>)
<span class="comment"># Interpretation: Each +1 sqft adds ${model.coef_[0]:.2f} to price</span>

<span class="comment"># 5. Make a prediction for a new 1650 sqft house</span>
new_house = np.array([[1650]])
predicted_price = model.<span class="function">predict</span>(new_house)
<span class="keyword">print</span>(<span class="string">f"Predicted price: ${predicted_price[0]:,.2f}"</span>)</code></pre>
          </div>
        </div>
        
        <div class="pro-tip">
          <p><strong>üí° Important:</strong> In scikit-learn, even with one feature, X must be a 2D array. Use <code>.reshape(-1, 1)</code> to convert a 1D array of samples into a 2D column.</p>
        </div>
      </div>
    </div>

    <!-- ================================================
         SECTION 3: MULTIPLE LINEAR REGRESSION (NEW!)
    ================================================= -->
    <div id="multiple-linear" class="module-content">

      <div class="panel section">
        <h3><i class="fas fa-cubes"></i> 3.1 Multiple Linear Regression: Many Features, One Line (in HD)</h3>

        <div class="info-box info">
          <h4><i class="fas fa-lightbulb"></i> From Lines to Hyperplanes</h4>
          <p>
            <strong>Multiple linear regression</strong> uses TWO OR MORE features to predict your target.
            With 2 features, it's a plane (like a flat sheet). With 3+ features, it's a "hyperplane" (can't visualize, but math works!).
          </p>
          <p class="math-equation" style="font-size: 1.2rem; text-align: center; margin: 20px 0;">
            y = b + m‚ÇÅx‚ÇÅ + m‚ÇÇx‚ÇÇ + m‚ÇÉx‚ÇÉ + ... + m‚Çôx‚Çô
          </p>
          <p>
            Each feature gets its own slope (coefficient) that tells you how much it influences the prediction,
            <strong>while holding all other features constant</strong>. This "holding constant" part is crucial!
          </p>
        </div>

        <div class="explanation-box">
          <p><strong>üìö In Simple Terms:</strong> Instead of drawing a 2D line, multiple regression finds a "best-fit surface" in higher dimensions. For house prices, it's like saying: "Price = $50,000 + ($200 √ó sqft) + ($15,000 √ó bedrooms) - ($1,000 √ó age)". Each feature adds or subtracts from the base price.</p>
          <p><strong>Why "holding constant" matters:</strong> The coefficient for bedrooms ($15,000) means: for two houses with the <strong>same sqft and age</strong>, the one with an extra bedroom costs $15,000 more. It isolates the effect of one feature.</p>
          <p><strong>Real-Life Example:</strong> Zillow's home value estimator uses multiple regression with dozens of features: square footage, bedrooms, bathrooms, location, school ratings, etc. Each feature has its own "weight" in the final calculation.</p>
        </div>

        <div class="expanded-content">
          <h4><i class="fas fa-table"></i> Reading Multiple Coefficients</h4>
          
          <table class="detailed-table">
            <thead>
              <tr><th>Feature</th><th>Coefficient</th><th>Interpretation</th></tr>
            </thead>
            <tbody>
              <tr><td>Intercept (b)</td><td>$50,000</td><td>Base price for a house with all features = 0 (theoretical)</td></tr>
              <tr><td>sqft (m‚ÇÅ)</td><td>$200</td><td>Each additional sqft ADDS $200 to price, holding bedrooms & age constant</td></tr>
              <tr><td>bedrooms (m‚ÇÇ)</td><td>$15,000</td><td>Each additional bedroom ADDS $15,000, holding sqft & age constant</td></tr>
              <tr><td>age (m‚ÇÉ)</td><td>-$1,000</td><td>Each year of age SUBTRACTS $1,000, holding sqft & bedrooms constant</td></tr>
            </tbody>
          </table>

          <p class="mono" style="margin-top:15px;">
            <strong>Prediction formula:</strong> Price = $50,000 + $200√ósqft + $15,000√óbedrooms - $1,000√óage
          </p>
        </div>

        <!-- Interactive 3D-like visualization (2D projection with color) -->
        <div class="regression-container">
          <div class="regression-chart"><canvas id="multipleChart"></canvas></div>

          <div class="regression-controls">
            <div class="control-group">
              <h4><i class="fas fa-sliders-h"></i> Adjust Feature Weights</h4>
              <div class="slider-container">
                <label><i class="fas fa-arrows-alt-v"></i> sqft Weight (m‚ÇÅ):</label>
                <input type="range" id="weightSqft" min="0" max="500" step="10" value="200">
                <span class="slider-value" id="weightSqftValue">200</span>
              </div>
              <div class="slider-container">
                <label><i class="fas fa-bed"></i> bedrooms Weight (m‚ÇÇ):</label>
                <input type="range" id="weightBeds" min="-10000" max="30000" step="1000" value="15000">
                <span class="slider-value" id="weightBedsValue">15000</span>
              </div>
              <div class="slider-container">
                <label><i class="fas fa-calendar-alt"></i> age Weight (m‚ÇÉ):</label>
                <input type="range" id="weightAge" min="-3000" max="1000" step="100" value="-1000">
                <span class="slider-value" id="weightAgeValue">-1000</span>
              </div>
              <div class="slider-container">
                <label><i class="fas fa-dollar-sign"></i> Intercept (b):</label>
                <input type="range" id="interceptVal" min="0" max="100000" step="5000" value="50000">
                <span class="slider-value" id="interceptValValue">50000</span>
              </div>
            </div>

            <div class="button-group">
              <button class="btn btn-primary" onclick="updateMultipleRegression()">
                <i class="fas fa-play"></i> Update Model
              </button>
              <button class="btn btn-secondary" onclick="generateMultipleData()">
                <i class="fas fa-sync"></i> New Data
              </button>
            </div>

            <div class="metrics-container" style="margin-top: 20px;">
              <div class="metric-card">
                <div class="metric-label">R¬≤ Score</div>
                <div class="metric-value" id="multipleR2">0.00</div>
                <div style="font-size:11px;">(1 is perfect)</div>
              </div>
              <div class="metric-card">
                <div class="metric-label">MSE (millions)</div>
                <div class="metric-value" id="multipleMSE">0.00</div>
                <div style="font-size:11px;">lower=better</div>
              </div>
              <div class="metric-card">
                <div class="metric-label">Samples</div>
                <div class="metric-value" id="multipleN">50</div>
              </div>
              <div class="metric-card">
                <div class="metric-label">Features</div>
                <div class="metric-value" id="multipleFeatures">3</div>
              </div>
            </div>

            <div class="explanation-box">
              <p><strong>üëÜ What you're seeing:</strong> The chart shows PRICE vs. SQFT (colored by bedrooms). Each dot is a house. The model tries to fit a plane through all features - you're adjusting the plane's tilt in each direction!</p>
              <p><strong>Try:</strong> Increase sqft weight ‚Üí steeper slope. Make age weight positive ‚Üí older houses cost more (unrealistic!). See how R¬≤ changes as you move weights away from the true pattern.</p>
            </div>
          </div>
        </div>

        <div class="common-pitfall">
          <p><strong>‚ö†Ô∏è The Curse of Dimensionality & Multicollinearity:</strong> </p>
          <ul>
            <li><strong>Curse of Dimensionality:</strong> With many features, you need MUCH more data. Rule of thumb: at least 10-20 examples per feature. With 10 features, you need 200+ houses! Too few data points with many features leads to overfitting.</li>
            <li><strong>Multicollinearity:</strong> If features are highly correlated (e.g., sqft and number of bedrooms), the coefficients become unstable and hard to interpret. The model can't tell which feature is truly causing the price change.</li>
          </ul>
        </div>

        <div class="code-editor-container">
          <div class="code-header">
            <div class="code-title"><i class="fab fa-python"></i> Multiple Linear Regression Code</div>
          </div>
          <div class="code-body">
            <pre class="code-block"><code><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression
<span class="keyword">import</span> pandas <span class="keyword">as</span> pd
<span class="keyword">import</span> numpy <span class="keyword">as</span> np

<span class="comment"># 1. Load your data (e.g., from CSV)</span>
<span class="comment"># df = pd.read_csv('housing_sample_dataset.csv')</span>
<span class="comment"># For this example, let's create sample data</span>
data = {
    'sqft': [1200, 1800, 950, 1500, 2100],
    'bedrooms': [3, 4, 2, 3, 4],
    'age': [10, 5, 20, 12, 4],
    'price': [250000, 450000, 180000, 320000, 520000]
}
df = pd.DataFrame(data)

<span class="comment"># 2. Define features (X) and target (y)</span>
X = df[['sqft', 'bedrooms', 'age']]  <span class="comment"># Multiple columns!</span>
y = df['price']

<span class="comment"># 3. Create and train the model</span>
model = LinearRegression()
model.<span class="function">fit</span>(X, y)

<span class="comment"># 4. Examine the results</span>
<span class="keyword">print</span>(<span class="string">"Intercept:"</span>, model.intercept_)
<span class="keyword">print</span>(<span class="string">"Coefficients for each feature:"</span>)
<span class="keyword">for</span> feature, coef <span class="keyword">in</span> <span class="function">zip</span>(X.columns, model.coef_):
    <span class="keyword">print</span>(<span class="string">f"  {feature}: ${coef:,.2f}"</span>)

<span class="comment"># 5. Make prediction for a new house: 2000 sqft, 4 beds, 10 years old</span>
new_house = pd.DataFrame([[2000, 4, 10]], columns=X.columns)
predicted_price = model.<span class="function">predict</span>(new_house)
<span class="keyword">print</span>(<span class="string">f"Predicted price: ${predicted_price[0]:,.2f}"</span>)

<span class="comment"># The model automatically does: b + m1*2000 + m2*4 + m3*10</span></code></pre>
          </div>
        </div>

        <div class="pro-tip">
          <p><strong>üí° Feature Scaling Matters!</strong> In multiple regression, features are on different scales (sqft ~1000s, bedrooms ~1-5). The coefficients reflect this - a small coefficient for a large-scale feature can still mean it's important! To compare importance, standardize features first (mean=0, std=1) using <code>StandardScaler</code>.</p>
        </div>
      </div>
    </div>

    <!-- ================================================
         SECTION 4: POLYNOMIAL REGRESSION (NEW!)
    ================================================= -->
    <div id="polynomial" class="module-content">

      <div class="panel section">
        <h3><i class="fas fa-wave-square"></i> 4.1 Polynomial Regression: When Life Isn't a Straight Line</h3>

        <div class="info-box info">
          <h4><i class="fas fa-lightbulb"></i> The Core Idea</h4>
          <p>
            <strong>Polynomial regression</strong> adds squared, cubed, or higher terms to capture curves.
            It's still "linear" in how we learn the coefficients, but the features are transformed to create curves.
          </p>
          <p class="math-equation" style="font-size: 1.2rem; text-align: center; margin: 20px 0;">
            y = b + m‚ÇÅx + m‚ÇÇx¬≤ + m‚ÇÉx¬≥ + ...
          </p>
          <p>
            <strong>Key insight:</strong> We're not fitting a curve directly. We're creating new features (x¬≤, x¬≥, etc.) 
            and then using <strong>multiple linear regression</strong> on these transformed features!
          </p>
        </div>

        <div class="explanation-box">
          <p><strong>üìö In Simple Terms:</strong> Sometimes the relationship between X and Y isn't a straight line - maybe prices increase faster for bigger houses (curving upward) or there's a "sweet spot" where value peaks (upside-down U). Polynomial regression bends the line to match these patterns by adding x¬≤, x¬≥ terms.</p>
          <p><strong>Real-Life Example:</strong> Car depreciation - new cars lose value quickly at first, then slow down. A straight line would be wrong; a quadratic (degree 2) curve fits better. Or crop yield vs. fertilizer - too little or too much fertilizer hurts yield (inverted U shape).</p>
        </div>

        <div class="expanded-content">
          <h4><i class="fas fa-chart-line"></i> Degrees of Flexibility: The Bias-Variance Tradeoff</h4>
          
          <div class="concept-grid">
            <div class="concept-card">
              <h5>Degree 1 (Linear)</h5>
              <p><i class="fas fa-chart-line"></i> <strong>Equation:</strong> y = b + m‚ÇÅx</p>
              <p><strong>Bias (error from wrong assumptions):</strong> High - can't capture curves</p>
              <p><strong>Variance (sensitivity to data):</strong> Low - stable</p>
              <p><strong>Problem:</strong> Underfitting</p>
            </div>
            <div class="concept-card">
              <h5>Degree 2 (Quadratic)</h5>
              <p><i class="fas fa-chart-line"></i> <strong>Equation:</strong> y = b + m‚ÇÅx + m‚ÇÇx¬≤</p>
              <p><strong>Bias:</strong> Medium - captures one curve</p>
              <p><strong>Variance:</strong> Medium</p>
              <p><strong>Good for:</strong> U-shape relationships</p>
            </div>
            <div class="concept-card">
              <h5>Degree 3 (Cubic)</h5>
              <p><i class="fas fa-chart-line"></i> <strong>Equation:</strong> y = b + m‚ÇÅx + m‚ÇÇx¬≤ + m‚ÇÉx¬≥</p>
              <p><strong>Bias:</strong> Lower - more flexible</p>
              <p><strong>Variance:</strong> Higher - can start overfitting</p>
            </div>
            <div class="concept-card">
              <h5>Degree 10+</h5>
              <p><i class="fas fa-chart-line"></i> <strong>Way too wiggly!</strong></p>
              <p><strong>Bias:</strong> Very low (can fit anything)</p>
              <p><strong>Variance:</strong> Extremely high - unstable</p>
              <p><strong>Problem:</strong> Severe overfitting</p>
            </div>
          </div>
          <p><strong>The goal:</strong> Find the degree that minimizes test error - low enough bias to capture the pattern, but not so high variance that it overfits.</p>
        </div>

        <div class="regression-container">
          <div class="regression-chart"><canvas id="polyChart"></canvas></div>

          <div class="regression-controls">
            <div class="control-group">
              <h4><i class="fas fa-sliders-h"></i> Control Flexibility</h4>
              <div class="slider-container">
                <label><i class="fas fa-wave-square"></i> Polynomial Degree:</label>
                <input type="range" id="polyDegree" min="1" max="15" step="1" value="2">
                <span class="slider-value" id="polyDegreeValue">2</span>
              </div>
              <p style="font-size:12px;">Degree 1 = straight line | Degree 2 = one curve | Higher = more wiggly</p>
              
              <div class="slider-container">
                <label><i class="fas fa-random"></i> Noise Level:</label>
                <input type="range" id="polyNoise" min="0" max="30" step="2" value="10">
                <span class="slider-value" id="polyNoiseValue">10</span>
              </div>
            </div>

            <div class="button-group">
              <button class="btn btn-primary" onclick="updatePolynomial()">
                <i class="fas fa-play"></i> Fit Polynomial
              </button>
              <button class="btn btn-secondary" onclick="generatePolyData()">
                <i class="fas fa-sync"></i> New Data
              </button>
            </div>

            <div class="metrics-container" style="margin-top: 20px;">
              <div class="metric-card">
                <div class="metric-label">Training R¬≤</div>
                <div class="metric-value" id="polyTrainR2">0.00</div>
                <div style="font-size:11px;">fits training data</div>
              </div>
              <div class="metric-card">
                <div class="metric-label">Test R¬≤</div>
                <div class="metric-value" id="polyTestR2">0.00</div>
                <div style="font-size:11px;">true measure of quality</div>
              </div>
              <div class="metric-card">
                <div class="metric-label">Degree</div>
                <div class="metric-value" id="polyCurrentDegree">2</div>
              </div>
              <div class="metric-card">
                <div class="metric-label">Parameters</div>
                <div class="metric-value" id="polyParams">3</div>
              </div>
            </div>

            <div class="explanation-box">
              <p><strong>üëÜ Watch the overfitting!</strong></p>
              <ul style="padding-left:20px;">
                <li><strong>Degree 1-2:</strong> Might be too simple (underfitting) - low R¬≤ on both train and test</li>
                <li><strong>Degree 3-5:</strong> Sweet spot - captures curve well, test R¬≤ high</li>
                <li><strong>Degree 8+:</strong> Wiggles through every point - train R¬≤ ~1.0, but test R¬≤ drops (overfitting!)</li>
              </ul>
            </div>
          </div>
        </div>

        <div class="code-editor-container">
          <div class="code-header">
            <div class="code-title"><i class="fab fa-python"></i> Polynomial Regression with scikit-learn</div>
          </div>
          <div class="code-body">
            <pre class="code-block"><code><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures
<span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression
<span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline
<span class="keyword">import</span> numpy <span class="keyword">as</span> np

<span class="comment"># Sample data (non-linear relationship)</span>
X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)
y = np.array([2, 5, 9, 15, 22, 30, 39, 49, 60, 72])  # Quadratic-ish pattern

<span class="comment"># Create a pipeline: first create polynomial features, then fit linear regression</span>
<span class="comment"># Degree 3 means it adds x¬≤ and x¬≥ columns automatically: [x, x¬≤, x¬≥]</span>
degree = 3
poly_model = make_pipeline(
    PolynomialFeatures(degree=degree, include_bias=False),  <span class="comment"># Don't add a column of 1's (intercept handled by LinearRegression)</span>
    LinearRegression()
)

<span class="comment"># Train the model</span>
poly_model.<span class="function">fit</span>(X, y)

<span class="comment"># Make predictions</span>
X_test = np.linspace(1, 10, 100).reshape(-1, 1)
y_pred = poly_model.<span class="function">predict</span>(X_test)

<span class="comment"># Inspect what happened: get the polynomial features transformer and the linear model</span>
poly_features = poly_model.named_steps['polynomialfeatures']
linear_part = poly_model.named_steps['linearregression']

<span class="keyword">print</span>(<span class="string">f"Original feature names: {poly_features.get_feature_names_out(['X'])}"</span>)
<span class="keyword">print</span>(<span class="string">f"Coefficients for each term: {linear_part.coef_}"</span>)
<span class="keyword">print</span>(<span class="string">f"Intercept: {linear_part.intercept_}"</span>)

<span class="comment"># Warning: With degree 10 and multiple original features, you'll have hundreds of terms!</span>
<span class="comment"># Example: degree 5 with 3 features creates 56 terms (combinations like x1*x2¬≤, etc.)</span>
<span class="comment"># This is why polynomial regression can overfit badly without regularization.</span></code></pre>
          </div>
        </div>

        <div class="common-pitfall">
          <p><strong>‚ö†Ô∏è The Extrapolation Danger:</strong> Polynomials go CRAZY outside your data range! A degree 5 polynomial might fit your house data perfectly between 1000-3000 sqft, but predict negative prices for 5000 sqft houses or astronomical prices for 1000 sqft houses. <strong>Never use polynomial regression to predict far beyond your training data.</strong></p>
        </div>

        <div class="pro-tip">
          <p><strong>üí° Better Alternative:</strong> For complex curves, consider splines (piecewise polynomials) or tree-based models (Random Forest, XGBoost). They're more stable and don't go wild outside the data range. Polynomial regression is best for simple, known curves (like physics relationships) when you're staying within the observed range.</p>
        </div>
      </div>
    </div>

    <!-- ================================================
         SECTION 5: REGULARIZED LINEAR MODELS
    ================================================= -->
    <div id="regularized" class="module-content">

      <div class="panel section">
        <h3><i class="fas fa-sliders-h"></i> 5.1 The Problem: When Too Many Features Cause Trouble</h3>

        <div class="info-box warning">
          <h4><i class="fas fa-fire"></i> Why We Need Regularization</h4>
          <p>
            Imagine you have 50 features but only 100 houses. Regular linear regression will try to use all 50 features,
            often fitting to random noise instead of real patterns. This is called <strong>overfitting</strong>.
          </p>
          <p>
            With polynomial regression, adding degree 5 to 3 features creates 56 terms! The model has too much flexibility
            and will memorize the training data, including the noise.
          </p>
          <p>
            <strong>Regularization</strong> adds a penalty for large or numerous coefficients, forcing the model to be simpler.
          </p>
        </div>

        <div class="expanded-content">
          <h4><i class="fas fa-compass"></i> Three Smart Solutions: Ridge, Lasso, and ElasticNet</h4>
          
          <div class="concept-grid">
            <div class="concept-card">
              <h5>üèîÔ∏è Ridge Regression (L2 Penalty)</h5>
              <p><strong>How it works:</strong> Adds penalty = Œ± * (sum of squared coefficients). This shrinks all coefficients toward zero but rarely to exactly zero.</p>
              <p><strong>Math:</strong> Minimize MSE + Œ±‚àë(m·µ¢¬≤)</p>
              <p><strong>Analogy:</strong> Like telling a student "use all the information, but don't get too confident about any single detail."</p>
              <p><strong>Best for:</strong> When many features contribute a little bit to the prediction, and you want to keep them all.</p>
            </div>
            <div class="concept-card">
              <h5>‚úÇÔ∏è Lasso Regression (L1 Penalty)</h5>
              <p><strong>How it works:</strong> Adds penalty = Œ± * (sum of absolute coefficients). This can force some coefficients to become exactly zero - removing features entirely.</p>
              <p><strong>Math:</strong> Minimize MSE + Œ±‚àë|m·µ¢|</p>
              <p><strong>Analogy:</strong> Like an editor who cuts out irrelevant paragraphs entirely, not just shortening them.</p>
              <p><strong>Best for:</strong> When you suspect only a few features really matter (feature selection).</p>
            </div>
            <div class="concept-card">
              <h5>‚öñÔ∏è ElasticNet</h5>
              <p><strong>How it works:</strong> A compromise between Ridge and Lasso, combining both penalties.</p>
              <p><strong>Math:</strong> Minimize MSE + Œ±‚ÇÅ‚àë|m·µ¢| + Œ±‚ÇÇ‚àë(m·µ¢¬≤)</p>
              <p><strong>Analogy:</strong> The balanced approach - keeps groups of related features together while still removing truly useless ones.</p>
              <p><strong>Best for:</strong> When you have correlated features and want some feature selection without dropping all correlated features.</p>
            </div>
          </div>
        </div>

        <div class="common-pitfall">
          <p><strong>‚ö†Ô∏è The Alpha Trap:</strong> All three methods have an "alpha" knob (sometimes called lambda) that controls how much to penalize complexity. </p>
          <ul>
            <li><strong>Alpha = 0:</strong> No penalty ‚Üí back to regular linear regression (still overfitting).</li>
            <li><strong>Too small alpha:</strong> Penalty too weak ‚Üí still overfitting.</li>
            <li><strong>Too large alpha:</strong> Penalty too strong ‚Üí oversimplified model that misses patterns (underfitting).</li>
            <li><strong>Just right:</strong> Find via cross-validation (try different alphas and see what minimizes validation error).</li>
          </ul>
          <p><strong>Important:</strong> Regularization requires features to be on the same scale! Always standardize (mean=0, std=1) first using <code>StandardScaler</code>.</p>
        </div>

        <div class="code-editor-container" style="margin-top: 20px;">
          <div class="code-header">
            <div class="code-title"><i class="fab fa-python"></i> Ridge Regression with Polynomial Features</div>
          </div>
          <div class="code-body">
            <pre class="code-block"><code><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge, Lasso, ElasticNet
<span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures, StandardScaler
<span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline
<span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score
<span class="keyword">import</span> numpy <span class="keyword">as</span> np

<span class="comment"># Sample data with a curved pattern</span>
np.random.seed(42)
X = np.linspace(-3, 3, 100).reshape(-1, 1)
y = 0.5 * X.ravel()**2 + X.ravel() + 2 + np.random.normal(0, 1, 100)

<span class="comment"># Create polynomial features WITH regularization</span>
<span class="comment"># Even high degree (10) is okay with regularization - it will control the complexity</span>
degree = 10
alpha = 1.0  <span class="comment"># Regularization strength</span>

ridge_poly = make_pipeline(
    PolynomialFeatures(degree=degree),  <span class="comment"># Creates x, x¬≤, x¬≥, ..., x¬π‚Å∞</span>
    StandardScaler(),                   <span class="comment"># CRITICAL: Scale features for regularization</span>
    Ridge(alpha=alpha)                  <span class="comment"># Ridge with L2 penalty</span>
)

<span class="comment"># Train the model</span>
ridge_poly.<span class="function">fit</span>(X, y)

<span class="comment"># Evaluate with cross-validation</span>
cv_scores = cross_val_score(ridge_poly, X, y, cv=5, scoring='r2')
<span class="keyword">print</span>(<span class="string">f"Cross-validated R¬≤: {cv_scores.mean():.3f} (+/- {cv_scores.std()*2:.3f})"</span>)

<span class="comment"># Compare with unregularized polynomial (degree 10) - this will overfit badly!</span>
<span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression
overfit_poly = make_pipeline(
    PolynomialFeatures(degree=degree),
    LinearRegression()
)
overfit_cv = cross_val_score(overfit_poly, X, y, cv=5, scoring='r2')
<span class="keyword">print</span>(<span class="string">f"Unregularized CV R¬≤: {overfit_cv.mean():.3f}"</span>)
<span class="comment"># Ridge will typically have higher cross-validated R¬≤ due to reduced overfitting</span></code></pre>
          </div>
        </div>
        
        <div class="pro-tip">
          <p><strong>üí° Try Different Alphas:</strong> Use <code>RidgeCV</code> or <code>LassoCV</code> which automatically try different alpha values via cross-validation to find the best one for you.</p>
          <p><strong>For Lasso:</strong> <code>from sklearn.linear_model import LassoCV; lasso_cv = LassoCV(cv=5).fit(X, y); print(f"Best alpha: {lasso_cv.alpha_}")</code></p>
        </div>
      </div>
    </div>

    <!-- ================================================
         SECTION 6: TREE-BASED MODELS (Enhanced)
    ================================================= -->
    <div id="tree-models" class="module-content">

      <div class="panel section">
        <h3><i class="fas fa-tree"></i> 6.1 Decision Trees: If-This-Then-That Rules</h3>

        <div class="info-box info">
          <h4><i class="fas fa-lightbulb"></i> How Decision Trees Work</h4>
          <p>
            Instead of drawing a line, decision trees ask a series of yes/no questions to partition the data into regions.
            Each region's prediction is simply the average target value of training points in that region.
          </p>
          <div style="background:var(--panel); padding:15px; border-radius:8px; margin:15px 0;">
            <p><strong>Question 1:</strong> Is sqft > 1500?<br/>
            &nbsp;&nbsp;&nbsp;‚Üí If YES: Go to Question 2<br/>
            &nbsp;&nbsp;&nbsp;‚Üí If NO: Predict $250,000 (average of all small houses)</p>
            <p><strong>Question 2:</strong> Is age < 10 years?<br/>
            &nbsp;&nbsp;&nbsp;‚Üí If YES: Predict $450,000 (average of new large houses)<br/>
            &nbsp;&nbsp;&nbsp;‚Üí If NO: Predict $380,000 (average of older large houses)</p>
          </div>
          <p>This creates a piecewise constant prediction - a step function, not a smooth line.</p>
        </div>

        <div class="explanation-box">
          <p><strong>üìö In Simple Terms:</strong> A decision tree is like the game "20 Questions" - it keeps splitting the data into smaller groups based on feature values until each group is pure enough (or we hit a stopping rule). The predictions are just the average of that group.</p>
          <p><strong>Key advantage:</strong> Trees automatically handle non-linear relationships and feature interactions without you having to create interaction terms manually.</p>
        </div>

        <div class="expanded-content">
          <h4><i class="fas fa-check-circle"></i> When to Use Decision Trees</h4>
          <div class="concept-grid">
            <div class="concept-card">
              <h5>‚úÖ Great For</h5>
              <ul style="padding-left:20px;">
                <li>Non-linear patterns (curves, thresholds)</li>
                <li>When you need to explain decisions to managers (easy to visualize)</li>
                <li>Mixed data types (numbers + categories) without encoding</li>
                <li>Automatically finding interactions between features</li>
                <li>Data with missing values (can handle them)</li>
              </ul>
            </div>
            <div class="concept-card">
              <h5>‚ö†Ô∏è Watch Out For</h5>
              <ul style="padding-left:20px;">
                <li><strong>Overfitting!</strong> Deep trees can memorize training data perfectly</li>
                <li><strong>High variance</strong> - small data changes can create very different trees</li>
                <li><strong>Can't extrapolate</strong> - predictions are bounded by training data range</li>
                <li><strong>Step functions</strong> - not smooth, may not be realistic</li>
              </ul>
            </div>
          </div>
        </div>

        <div class="regression-container">
          <div class="regression-chart"><canvas id="treeChart"></canvas></div>

          <div class="regression-controls">
            <div class="control-group">
              <h4><i class="fas fa-sliders-h"></i> Control the Tree's Complexity</h4>
              <div class="slider-container">
                <label><i class="fas fa-layer-group"></i> Max Depth:</label>
                <input type="range" id="treeDepth" min="1" max="10" step="1" value="3">
                <span class="slider-value" id="treeDepthValue">3</span>
              </div>
              <p style="font-size:12px;">Depth = number of consecutive questions. Higher = more complex.</p>
              
              <div class="slider-container">
                <label><i class="fas fa-users"></i> Min Samples per Leaf:</label>
                <input type="range" id="minSamples" min="2" max="20" step="1" value="5">
                <span class="slider-value" id="minSamplesValue">5</span>
              </div>
              <p style="font-size:12px;">Higher = simpler, more robust tree (prevents overfitting)</p>
            </div>

            <div class="button-group">
              <button class="btn btn-primary" onclick="updateTreeModel()"><i class="fas fa-play"></i> Update Tree</button>
              <button class="btn btn-secondary" onclick="resetTreeData()"><i class="fas fa-redo"></i> New Random Data</button>
            </div>

            <div class="metrics-container" style="margin-top: 20px;">
              <div class="metric-card">
                <div class="metric-label">Training R¬≤</div>
                <div class="metric-value" id="treeTrainR2">0.85</div>
              </div>
              <div class="metric-card">
                <div class="metric-label">Test R¬≤</div>
                <div class="metric-value" id="treeTestR2">0.78</div>
              </div>
              <div class="metric-card">
                <div class="metric-label">Leaves</div>
                <div class="metric-value" id="treeLeaves">8</div>
              </div>
              <div class="metric-card">
                <div class="metric-label">Current Depth</div>
                <div class="metric-value" id="treeCurrentDepth">3</div>
              </div>
            </div>

            <div class="explanation-box" style="margin-top: 20px;">
              <p><strong>üëÜ Watch the bias-variance tradeoff:</strong></p>
              <ul style="padding-left:20px;">
                <li><strong>Depth 1:</strong> Just a flat line (average of all data) - high bias, low variance</li>
                <li><strong>Depth 3-4:</strong> Starts capturing the curve - good balance</li>
                <li><strong>Depth 8-10:</strong> Wiggles through every point - train R¬≤ high, test R¬≤ drops (overfitting!)</li>
              </ul>
            </div>
          </div>
        </div>

        <div class="code-editor-container" style="margin-top: 20px;">
          <div class="code-header">
            <div class="code-title"><i class="fab fa-python"></i> Decision Tree Code</div>
          </div>
          <div class="code-body">
            <pre class="code-block"><code><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor
<span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split
<span class="keyword">import</span> numpy <span class="keyword">as</span> np

<span class="comment"># Sample non-linear data</span>
np.random.seed(42)
X = np.random.rand(200, 1) * 10 - 5  # Range -5 to 5
y = 0.5 * X.ravel()**2 + X.ravel() + 2 + np.random.normal(0, 2, 200)

<span class="comment"># Split data</span>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

<span class="comment"># Create a tree with limits to prevent overfitting</span>
tree = DecisionTreeRegressor(
    max_depth=5,           <span class="comment"># Don't let it get too deep</span>
    min_samples_leaf=10,    <span class="comment"># Each leaf must have at least 10 samples</span>
    min_samples_split=5,    <span class="comment"># Each split requires at least 5 samples in the node</span>
    random_state=42
)

tree.<span class="function">fit</span>(X_train, y_train)

<span class="comment"># Evaluate</span>
train_score = tree.score(X_train, y_train)
test_score = tree.score(X_test, y_test)
<span class="keyword">print</span>(<span class="string">f"Train R¬≤: {train_score:.3f}"</span>)
<span class="keyword">print</span>(<span class="string">f"Test R¬≤: {test_score:.3f}"</span>)

<span class="comment"># You can even visualize the tree rules!</span>
<span class="comment"># from sklearn.tree import plot_tree</span>
<span class="comment"># import matplotlib.pyplot as plt</span>
<span class="comment"># plt.figure(figsize=(20,10))</span>
<span class="comment"># plot_tree(tree, feature_names=['X'], filled=True)</span>
<span class="comment"># plt.show()</span></code></pre>
          </div>
        </div>

        <div class="pro-tip">
          <p><strong>üí° Student Tip:</strong> Decision trees are great for understanding your data. Plot the tree and follow the paths - you'll see exactly which features matter most and at what thresholds they split. It's like reading the mind of your model! But for final predictions, use an ensemble (Random Forest) for better accuracy.</p>
        </div>
      </div>
    </div>

    <!-- ================================================
         SECTION 7: ENSEMBLE METHODS (Enhanced)
    ================================================= -->
    <div id="ensemble" class="module-content">

      <div class="panel section">
        <h3><i class="fas fa-layer-group"></i> 7.1 Ensemble Methods: Why Many Models Are Better Than One</h3>

        <div class="info-box info">
          <h4><i class="fas fa-users"></i> The Wisdom of the Crowd</h4>
          <p>
            Imagine you need to guess the number of jellybeans in a jar. Would you rather trust:
          </p>
          <ul style="padding-left:20px; margin:15px 0;">
            <li><strong>One expert</strong> (might have a bad day, might be biased)</li>
            <li><strong>The average guess of 100 random people</strong> (surprisingly accurate! This is a real phenomenon called the "wisdom of crowds")</li>
          </ul>
          <p>
            Ensembles work the same way - they combine many models to get a better answer than any single model could provide.
            Each model makes different mistakes; averaging them cancels out the errors.
          </p>
        </div>

        <div class="explanation-box">
          <p><strong>üìö In Simple Terms:</strong> Each model makes mistakes, but they make different mistakes. When you average them, the mistakes cancel out and the correct signal remains. It's like getting a second opinion from multiple doctors - you're more likely to get the right diagnosis.</p>
        </div>

        <div class="expanded-content">
          <h4><i class="fas fa-code-branch"></i> Two Main Ensemble Strategies</h4>
          <div class="concept-grid">
            <div class="concept-card">
              <h5><i class="fas fa-random"></i> Bagging (Bootstrap Aggregating)</h5>
              <p><strong>How it works:</strong> Train many models (usually trees) in parallel on random bootstrap samples of the data (sampling with replacement). Each model sees about 63% of the data. Average their predictions.</p>
              <p><strong>Key feature:</strong> Reduces variance without increasing bias.</p>
              <p><strong>Random Forest:</strong> Bagging + random feature selection at each split, making trees even more different.</p>
            </div>
            <div class="concept-card">
              <h5><i class="fas fa-arrow-up"></i> Boosting</h5>
              <p><strong>How it works:</strong> Train models sequentially - each new model focuses on correcting the mistakes of the previous ones. Points that were mispredicted get higher weight in the next model.</p>
              <p><strong>Key feature:</strong> Can reduce both bias and variance, but more prone to overfitting if not tuned.</p>
              <p><strong>XGBoost, AdaBoost, Gradient Boosting:</strong> Popular boosting algorithms.</p>
            </div>
          </div>
        </div>

        <div class="panel section" style="margin-top:20px;">
          <h3><i class="fas fa-tree"></i> 7.2 Random Forest: A Forest of Decision Trees</h3>

          <div class="expanded-content">
            <h4>How Random Forest Works (In Plain English)</h4>
            <ol style="padding-left:20px;">
              <li><strong>Create many trees:</strong> Instead of one tree, we grow hundreds (e.g., 100 trees).</li>
              <li><strong>Give each tree different data (Bootstrap):</strong> Each tree sees a random sample of houses drawn with replacement. Some houses might appear multiple times, some not at all (about 1/3 are left out - these are "out-of-bag" and can be used for validation).</li>
              <li><strong>Give each tree different features:</strong> At each split, the tree only considers a random subset of features (e.g., if you have 10 features, maybe only consider 3 at each split). This ensures trees are diverse.</li>
              <li><strong>Average them all:</strong> When predicting a new house, ask all trees, average their answers.</li>
            </ol>
            <p>This randomness ensures trees are different, so their errors are uncorrelated and cancel out when averaging.</p>
          </div>

          <div class="regression-container">
            <div class="regression-chart"><canvas id="rfChart"></canvas></div>

            <div class="regression-controls">
              <div class="control-group">
                <h4><i class="fas fa-sliders-h"></i> Random Forest Controls</h4>
                <div class="slider-container">
                  <label><i class="fas fa-tree"></i> Number of Trees:</label>
                  <input type="range" id="nEstimators" min="10" max="200" step="10" value="100">
                  <span class="slider-value" id="nEstimatorsValue">100</span>
                </div>
                <p style="font-size:12px;">More trees = more stable (diminishing returns after ~100)</p>
                
                <div class="slider-container">
                  <label><i class="fas fa-layer-group"></i> Max Depth:</label>
                  <input type="range" id="rfDepth" min="3" max="20" step="1" value="10">
                  <span class="slider-value" id="rfDepthValue">10</span>
                </div>
                <p style="font-size:12px;">Deeper trees = more complex, can overfit if too deep</p>
              </div>

              <div class="button-group">
                <button class="btn btn-primary" onclick="updateRandomForest()"><i class="fas fa-play"></i> Update Forest</button>
              </div>

              <div class="metrics-container" style="margin-top: 20px;">
                <div class="metric-card">
                  <div class="metric-label">Train R¬≤</div>
                  <div class="metric-value" id="rfTrainR2">0.95</div>
                </div>
                <div class="metric-card">
                  <div class="metric-label">Test R¬≤</div>
                  <div class="metric-value" id="rfTestR2">0.89</div>
                </div>
                <div class="metric-card">
                  <div class="metric-label">Trees</div>
                  <div class="metric-value" id="rfTrees">100</div>
                </div>
                <div class="metric-card">
                  <div class="metric-label">OOB Score</div>
                  <div class="metric-value" id="rfOOB">0.87</div>
                </div>
              </div>

              <div class="explanation-box">
                <p><strong>üìö What's OOB Score?</strong> Out-of-bag (OOB) score is like free validation. Each tree is trained on a bootstrap sample (~63% of data), leaving ~37% out. Those "left out" houses are used to test that tree, and the average across all trees gives an unbiased performance estimate without needing a separate validation set.</p>
              </div>
            </div>
          </div>

          <div class="code-editor-container" style="margin-top: 20px;">
            <div class="code-header">
              <div class="code-title"><i class="fab fa-python"></i> Random Forest in Python</div>
            </div>
            <div class="code-body">
              <pre class="code-block"><code><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor
<span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split
<span class="keyword">import</span> pandas <span class="keyword">as</span> pd

<span class="comment"># Load your data (example)</span>
<span class="comment"># df = pd.read_csv('housing_sample_dataset.csv')</span>
<span class="comment"># X = df[['sqft', 'bedrooms', 'age']]</span>
<span class="comment"># y = df['price']</span>

<span class="comment"># For demo, create sample data</span>
np.random.seed(42)
X = np.random.rand(200, 3)  # 3 features
y = X[:, 0]*2 + X[:, 1]*3 - X[:, 2] + np.random.normal(0, 0.1, 200)

<span class="comment"># Split data</span>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

<span class="comment"># Create a random forest</span>
rf = RandomForestRegressor(
    n_estimators=100,      <span class="comment"># Number of trees in the forest</span>
    max_depth=10,          <span class="comment"># Limit tree depth to prevent overfitting</span>
    min_samples_leaf=5,     <span class="comment"># Each leaf needs at least 5 samples</span>
    min_samples_split=10,   <span class="comment"># Each split requires at least 10 samples in the node</span>
    max_features='sqrt',    <span class="comment"># At each split, consider sqrt(n_features) features (common default)</span>
    oob_score=True,         <span class="comment"># Get free validation score using out-of-bag samples</span>
    random_state=42,
    n_jobs=-1               <span class="comment"># Use all CPU cores for parallel training</span>
)

rf.<span class="function">fit</span>(X_train, y_train)

<span class="comment"># Evaluate</span>
<span class="keyword">print</span>(<span class="string">f"Train R¬≤: {rf.score(X_train, y_train):.3f}"</span>)
<span class="keyword">print</span>(<span class="string">f"Test R¬≤: {rf.score(X_test, y_test):.3f}"</span>)
<span class="keyword">print</span>(<span class="string">f"OOB Score: {rf.oob_score_:.3f}"</span>)  <span class="comment"># Should be close to test score</span>

<span class="comment"># Feature importance - which features matter most?</span>
importances = rf.feature_importances_
feature_names = [<span class="string">'sqft'</span>, <span class="string">'bedrooms'</span>, <span class="string">'age'</span>]
<span class="keyword">for</span> name, imp <span class="keyword">in</span> <span class="function">zip</span>(feature_names, importances):
    <span class="keyword">print</span>(<span class="string">f"{name}: {imp:.3f}"</span>)  <span class="comment"># Higher = more important (sums to 1)</span></code></pre>
            </div>
          </div>
        </div>
      </div>

      <div class="panel section">
        <h3><i class="fas fa-bolt"></i> 7.3 XGBoost: The Champion Algorithm</h3>

        <div class="info-box success">
          <h4><i class="fas fa-trophy"></i> Why XGBoost Wins Competitions</h4>
          <p>
            XGBoost (Extreme Gradient Boosting) is like a student who learns from mistakes. Each new tree focuses on the houses previous trees predicted poorly. It uses gradient descent to minimize errors and has built-in regularization to prevent overfitting. With smart tricks like early stopping and tree pruning, it often produces the most accurate predictions.
          </p>
        </div>

        <div class="expanded-content">
          <h4>Key XGBoost Parameters You Should Know</h4>
          <table class="detailed-table">
            <thead>
              <tr><th>Parameter</th><th>What it does</th><th>Typical range</th></tr>
            </thead>
            <tbody>
              <tr><td><code>n_estimators</code></td><td>Number of boosting rounds (trees)</td><td>100-1000 (use early stopping)</td></tr>
              <tr><td><code>learning_rate (eta)</code></td><td>Shrinks contribution of each tree. Lower = more robust, needs more trees.</td><td>0.01-0.3</td></tr>
              <tr><td><code>max_depth</code></td><td>Maximum tree depth. Deeper = more complex.</td><td>3-10</td></tr>
              <tr><td><code>subsample</code></td><td>Fraction of samples used per tree. <1 prevents overfitting.</td><td>0.5-1.0</td></tr>
              <tr><td><code>colsample_bytree</code></td><td>Fraction of features used per tree.</td><td>0.5-1.0</td></tr>
              <tr><td><code>reg_alpha</code></td><td>L1 regularization on weights (like Lasso).</td><td>0-10</td></tr>
              <tr><td><code>reg_lambda</code></td><td>L2 regularization on weights (like Ridge).</td><td>0-10</td></tr>
            </tbody>
          </table>
        </div>

        <div class="common-pitfall">
          <p><strong>‚ö†Ô∏è Warning for Beginners:</strong> XGBoost is powerful but has many knobs to adjust. Start with default settings, then learn to tune one parameter at a time. <strong>Always use early stopping</strong> to prevent overfitting - this automatically stops training when validation performance stops improving.</p>
        </div>

        <div class="code-editor-container" style="margin-top: 20px;">
          <div class="code-header">
            <div class="code-title"><i class="fab fa-python"></i> XGBoost with Early Stopping</div>
          </div>
          <div class="code-body">
            <pre class="code-block"><code><span class="comment"># Install: pip install xgboost</span>
<span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb
<span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split
<span class="keyword">import</span> numpy <span class="keyword">as</span> np

<span class="comment"># Sample data</span>
X, y = ... <span class="comment"># your features and target</span>

<span class="comment"># Split into train and validation sets</span>
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

<span class="comment"># Create the model with typical good starting parameters</span>
xgb_model = xgb.XGBRegressor(
    n_estimators=1000,          <span class="comment"># High number, early stopping will find the best</span>
    learning_rate=0.05,         <span class="comment">"> How much each tree contributes (shrinkage)</span>
    max_depth=5,                <span class="comment"># Shallow trees prevent overfitting</span>
    subsample=0.8,              <span class="comment"># Use 80% of data per tree</span>
    colsample_bytree=0.8,       <span class="comment"># Use 80% of features per tree</span>
    reg_alpha=0.1,              <span class="comment"># L1 regularization</span>
    reg_lambda=1.0,             <span class="comment"># L2 regularization</span>
    random_state=42,
    early_stopping_rounds=50,   <span class="comment"># Stop if no improvement for 50 rounds</span>
    eval_metric='rmse'          <span class="comment"># Metric to monitor for early stopping</span>
)

<span class="comment"># Train with early stopping</span>
xgb_model.<span class="function">fit</span>(
    X_train, y_train,
    eval_set=[(X_val, y_val)],      <span class="comment"># Validation set for monitoring</span>
    verbose=False                   <span class="comment"># Set to True to see progress</span>
)

<span class="comment"># Best iteration found by early stopping</span>
<span class="keyword">print</span>(<span class="string">f"Best iteration: {xgb_model.best_iteration}"</span>)
<span class="keyword">print</span>(<span class="string">f"Best validation score: {xgb_model.best_score}"</span>)

<span class="comment"># Feature importance (like Random Forest)</span>
importance_dict = xgb_model.get_booster().get_score(importance_type='weight')
<span class="keyword">print</span>(<span class="string">"Feature importance:"</span>, importance_dict)</code></pre>
          </div>
        </div>

        <div class="pro-tip">
          <p><strong>üí° When to Use What:</strong></p>
          <ul>
            <li><strong>Start with Linear Regression</strong> - it's your baseline, fast, and interpretable.</li>
            <li><strong>If data has clear curves and you're staying in-range</strong> - try Polynomial + Ridge.</li>
            <li><strong>If you need good performance with minimal tuning</strong> - Random Forest is robust and works well out-of-the-box.</li>
            <li><strong>If you're in a competition or need maximum accuracy</strong> - XGBoost (or LightGBM, CatBoost) with careful tuning.</li>
            <li><strong>If interpretability is crucial</strong> - Decision Tree (if small) or Linear Regression.</li>
          </ul>
        </div>
      </div>
    </div>

    <!-- ================================================
         SECTION 8: EVALUATION - HOW TO MEASURE SUCCESS
    ================================================= -->
    <div id="evaluation" class="module-content">

      <div class="panel section">
        <h3><i class="fas fa-clipboard-check"></i> 8.1 How Do We Know If Our Predictions Are Good?</h3>

        <div class="info-box info">
          <h4><i class="fas fa-ruler"></i> Three Essential Metrics (with Simple Explanations)</h4>
        </div>

        <div class="expanded-content">
          <h4>üìè MAE: Mean Absolute Error</h4>
          <div class="formula-with-calc">
            <div class="formula-box">
              <div class="formula-label">Formula</div>
              <span class="math-equation">MAE = (1/n) * Œ£|y·µ¢ - ≈∑·µ¢|</span>
            </div>
            <div class="numeric-example">
              <strong>Plain English:</strong> "On average, how far off are our predictions in absolute dollars?"<br/>
              <strong>Example:</strong> If MAE = $15,000 on house prices, that means typical predictions are off by about $15,000, regardless of direction.<br/>
              <strong>Units:</strong> Same as target variable (dollars). Easy to understand.
            </div>
          </div>

          <h4>üìê RMSE: Root Mean Squared Error</h4>
          <div class="formula-with-calc">
            <div class="formula-box">
              <div class="formula-label">Formula</div>
              <span class="math-equation">RMSE = ‚àö[(1/n) * Œ£(y·µ¢ - ≈∑·µ¢)¬≤]</span>
            </div>
            <div class="numeric-example">
              <strong>Plain English:</strong> "Similar to MAE, but gives extra penalty to big mistakes by squaring them before averaging."<br/>
              <strong>Example:</strong> Being off by $50,000 once is worse than being off by $10,000 five times, even though MAE would be the same ($10,000). RMSE would be higher for the single big error.<br/>
              <strong>Units:</strong> Same as target variable. Always ‚â• MAE.
            </div>
          </div>

          <h4>üéØ R¬≤: R-Squared (Coefficient of Determination)</h4>
          <div class="formula-with-calc">
            <div class="formula-box">
              <div class="formula-label">Formula</div>
              <span class="math-equation">R¬≤ = 1 - (SS_res / SS_tot)</span>
            </div>
            <div class="numeric-example">
              <strong>Plain English:</strong> "How much better are we than just guessing the average every time?"<br/>
              <strong>Scale:</strong> 
              <ul>
                <li><strong>1.0</strong> = perfect predictions</li>
                <li><strong>0.0</strong> = no better than predicting the mean</li>
                <li><strong>Negative</strong> = worse than predicting the mean (your model is actively bad!)</li>
              </ul>
              <strong>Interpretation:</strong> An R¬≤ of 0.85 means your model explains 85% of the variance in the target variable.
            </div>
          </div>
        </div>

        <h4>Let's Calculate Them Step by Step</h4>
        <table class="detailed-table">
          <thead>
            <tr><th>Actual Price</th><th>Our Prediction</th><th>Error (actual - pred)</th><th>Absolute Error</th><th>Squared Error</th></tr>
          </thead>
          <tbody>
            <tr><td>$250,000</td><td>$240,000</td><td>-$10,000</td><td>$10,000</td><td>100,000,000</td></tr>
            <tr><td>$450,000</td><td>$460,000</td><td>$10,000</td><td>$10,000</td><td>100,000,000</td></tr>
            <tr><td>$180,000</td><td>$190,000</td><td>$10,000</td><td>$10,000</td><td>100,000,000</td></tr>
            <tr><td>$320,000</td><td>$310,000</td><td>-$10,000</td><td>$10,000</td><td>100,000,000</td></tr>
          </tbody>
        </table>

        <div class="numeric-example">
          <p><strong>Step 1 - MAE:</strong> Average of absolute errors = ($10,000 √ó 4) / 4 = <strong>$10,000</strong></p>
          <p><strong>Step 2 - RMSE:</strong> Square root of average squared errors = ‚àö(100,000,000) = <strong>$10,000</strong> (same here because errors are equal)</p>
          <p><strong>Step 3 - R¬≤:</strong> Compare to guessing average ($300,000). Calculate sum of squared errors for the mean guess: (250-300)¬≤ + (450-300)¬≤ + (180-300)¬≤ + (320-300)¬≤ = 2500 + 22500 + 14400 + 400 = 39,800 (in thousands¬≤). Our model's sum of squared errors is 4 √ó 100 = 400 (in thousands¬≤). So R¬≤ = 1 - (400 / 39800) ‚âà <strong>0.99</strong> (very good!).</p>
        </div>

        <div class="common-pitfall">
          <p><strong>‚ö†Ô∏è Common Mistake:</strong> Don't compare RMSE across different datasets. An RMSE of $10,000 might be great for predicting $500,000 houses (2% error) but terrible for predicting $50,000 houses (20% error). Always consider the scale of your target! Also, R¬≤ can be artificially high if you have few data points and many features - use adjusted R¬≤ or cross-validation.</p>
        </div>
      </div>

      <div class="panel section">
        <h3><i class="fas fa-chart-bar"></i> 8.2 Cross-Validation: Don't Get Lucky, Get Good</h3>

        <div class="info-box warning">
          <h4><i class="fas fa-exclamation-triangle"></i> The Problem with One Test</h4>
          <p>
            If you split your data once, you might get lucky (easy test set) or unlucky (hard test set). Your estimated performance could be misleading. Cross-validation solves this by testing on multiple different splits and averaging the results.
          </p>
        </div>

        <div class="explanation-box">
          <p><strong>üìö How 5-Fold Cross-Validation Works:</strong></p>
          <ol style="padding-left:20px;">
            <li>Split data into 5 equal parts (folds)</li>
            <li><strong>Fold 1:</strong> Train on folds 2-5, test on fold 1 ‚Üí record score</li>
            <li><strong>Fold 2:</strong> Train on folds 1,3-5, test on fold 2 ‚Üí record score</li>
            <li><strong>Fold 3:</strong> Train on folds 1-2,4-5, test on fold 3 ‚Üí record score</li>
            <li><strong>Fold 4:</strong> Train on folds 1-3,5, test on fold 4 ‚Üí record score</li>
            <li><strong>Fold 5:</strong> Train on folds 1-4, test on fold 5 ‚Üí record score</li>
            <li>Average all 5 scores ‚Üí your final, honest estimate of model performance</li>
            <li>Also look at the standard deviation - low means stable model, high means unstable (might need more data or simpler model).</li>
          </ol>
          <p>This gives you a much more reliable estimate of how your model will perform on new, unseen data.</p>
        </div>

        <div class="code-editor-container">
          <div class="code-header">
            <div class="code-title">Cross-Validation in Python</div>
          </div>
          <div class="code-body">
            <pre class="code-block"><code><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score, KFold
<span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor
<span class="keyword">import</span> numpy <span class="keyword">as</span> np

<span class="comment"># Assume X, y are your features and target</span>

<span class="comment"># Create model</span>
model = RandomForestRegressor(n_estimators=100, random_state=42)

<span class="comment"># Perform 5-fold cross-validation</span>
cv = KFold(n_splits=5, shuffle=True, random_state=42)  <span class="comment"># Shuffle data before splitting</span>
scores = cross_val_score(model, X, y, cv=cv, scoring='r2')  <span class="comment"># Can use 'neg_mean_absolute_error' etc.</span>

<span class="keyword">print</span>(<span class="string">f"Scores from each fold: {scores}"</span>)
<span class="keyword">print</span>(<span class="string">f"Average R¬≤: {scores.mean():.3f}"</span>)
<span class="keyword">print</span>(<span class="string">f"Standard deviation: {scores.std():.3f}"</span>)  <span class="comment"># Lower = more stable model</span>

<span class="comment"># Example: If scores are [0.85, 0.87, 0.84, 0.86, 0.85] ‚Üí stable, trustworthy.</span>
<span class="comment"># If they're [0.95, 0.60, 0.92, 0.55, 0.90] ‚Üí unstable, model is inconsistent.</span>
<span class="comment"># Might need more data, simpler model, or check for data leaks.</span></code></pre>
          </div>
        </div>

        <div class="pro-tip">
          <p><strong>üí° Pro Tip:</strong> Use <code>cross_validate</code> instead of <code>cross_val_score</code> if you want to get both training and test scores, which can help detect overfitting (training score much higher than test score).</p>
        </div>
      </div>

      <div class="panel section">
        <h3><i class="fas fa-shield-halved"></i> 8.3 The Trust Checklist: When Can You Deploy Your Model?</h3>

        <div class="info-box info">
          <h4><i class="fas fa-check-circle"></i> 7 Questions to Ask Before Trusting a Model</h4>
        </div>

        <div class="expanded-content">
          <ol style="padding-left:20px; line-height:2;">
            <li><strong>‚úÖ Test performance close to training?</strong> (If train=0.99, test=0.60 ‚Üí severe overfitting!)</li>
            <li><strong>‚úÖ Cross-validation scores stable?</strong> (Standard deviation low, not jumping between 0.95 and 0.55)</li>
            <li><strong>‚úÖ No data leakage?</strong> (Did test data accidentally influence training? E.g., scaling before splitting, or using future information to predict past.)</li>
            <li><strong>‚úÖ Errors acceptable for your use case?</strong> (Is being off by $15,000 okay for your business? Maybe yes for rough estimates, no for final offers.)</li>
            <li><strong>‚úÖ Passes sanity checks?</strong> (Does it predict larger houses cost more? Does it make logical sense? If not, something's wrong.)</li>
            <li><strong>‚úÖ Works on edge cases?</strong> (Very small, very large, unusual feature combinations - test these specifically.)</li>
            <li><strong>‚úÖ Monitoring plan ready?</strong> (How will you know if it stops working in production? Data drift, concept drift? Set up alerts.)</li>
          </ol>

          <div class="common-pitfall">
            <p><strong>‚ö†Ô∏è Real horror story:</strong> A hospital's model predicted pneumonia patients had lower risk of death if they had asthma. Why? Because asthma patients got rushed to ICU immediately, so they were monitored closely. The model learned "asthma = good" - but that's wrong! This is called <strong>data leakage</strong> (the treatment, not the disease, affected the outcome). Always ask: "Does this prediction make sense clinically/business-wise?"</p>
          </div>
        </div>
      </div>
    </div>

    <!-- ================================================
         SECTION 9: HANDS-ON LAB - TRY IT YOURSELF!
    ================================================= -->
    <div id="hands-on" class="module-content">

      <div class="panel section">
        <h3><i class="fas fa-laptop-code"></i> 9.1 Compare All Algorithms on the Same Data</h3>

        <div class="info-box success">
          <h4><i class="fas fa-flask"></i> Your Mission</h4>
          <p>
            Below is the same dataset. Click different algorithms and see how they fit the data.
            Watch how some capture the curve better than others, and notice the bias-variance tradeoff.
          </p>
        </div>

        <div class="regression-container">
          <div class="regression-chart"><canvas id="comparisonChart"></canvas></div>

          <div class="regression-controls">
            <div class="control-group">
              <h4><i class="fas fa-cogs"></i> Pick an Algorithm</h4>
              <div class="button-group" style="flex-direction: column; gap: 10px;">
                <button class="btn btn-secondary" onclick="compareAlgorithm('linear')" style="width: 100%; justify-content: flex-start;">
                  <i class="fas fa-chart-line"></i> Simple Linear (straight line)
                </button>
                <button class="btn btn-secondary" onclick="compareAlgorithm('multiple')" style="width: 100%; justify-content: flex-start;">
                  <i class="fas fa-cubes"></i> Multiple Linear (plane)
                </button>
                <button class="btn btn-secondary" onclick="compareAlgorithm('poly2')" style="width: 100%; justify-content: flex-start;">
                  <i class="fas fa-wave-square"></i> Polynomial (degree 2)
                </button>
                <button class="btn btn-secondary" onclick="compareAlgorithm('poly5')" style="width: 100%; justify-content: flex-start;">
                  <i class="fas fa-wave-square"></i> Polynomial (degree 5)
                </button>
                <button class="btn btn-secondary" onclick="compareAlgorithm('tree')" style="width: 100%; justify-content: flex-start;">
                  <i class="fas fa-tree"></i> Decision Tree (step function)
                </button>
                <button class="btn btn-secondary" onclick="compareAlgorithm('rf')" style="width: 100%; justify-content: flex-start;">
                  <i class="fas fa-random"></i> Random Forest (smoothed steps)
                </button>
              </div>
            </div>

            <div class="metrics-container" style="margin-top: 20px;">
              <div class="metric-card">
                <div class="metric-label">Algorithm</div>
                <div class="metric-value" id="currentAlgo">Simple Linear</div>
              </div>
              <div class="metric-card">
                <div class="metric-label">Train R¬≤</div>
                <div class="metric-value" id="algoTrainR2">0.72</div>
              </div>
              <div class="metric-card">
                <div class="metric-label">Test R¬≤</div>
                <div class="metric-value" id="algoTestR2">0.68</div>
              </div>
              <div class="metric-card">
                <div class="metric-label">RMSE</div>
                <div class="metric-value" id="algoRMSE">12.4</div>
              </div>
            </div>

            <div class="explanation-box">
              <p><strong>üëÜ Try each one and observe:</strong></p>
              <ul style="padding-left:20px;">
                <li><strong>Linear:</strong> Can't capture the curve - underfitting (high bias)</li>
                <li><strong>Multiple Linear:</strong> Still a plane, but might do slightly better with more info</li>
                <li><strong>Polynomial (deg 2):</strong> Captures the basic curve well</li>
                <li><strong>Polynomial (deg 5):</strong> Starts to wiggle - may overfit a bit</li>
                <li><strong>Tree:</strong> Step function, captures non-linearity but jagged</li>
                <li><strong>Random Forest:</strong> Smooths the steps, good balance</li>
              </ul>
            </div>
          </div>
        </div>

        <div class="algorithm-grid" style="margin-top: 30px;">
          <div class="algorithm-card" id="algoLinear">
            <div class="algorithm-header">
              <div class="algorithm-icon"><i class="fas fa-chart-line"></i></div>
              <div class="algorithm-title">
                <h4>Linear Regression</h4><p>Simple, explainable baseline</p>
              </div>
            </div>
            <div class="algorithm-body">
              <div class="algorithm-metric"><span>Best for:</span><span>Linear relationships</span></div>
              <div class="algorithm-metric"><span>Pros:</span><span>Fast, interpretable</span></div>
              <div class="algorithm-metric"><span>Cons:</span><span>Misses curves, sensitive to outliers</span></div>
            </div>
          </div>

          <div class="algorithm-card" id="algoPoly">
            <div class="algorithm-header">
              <div class="algorithm-icon"><i class="fas fa-wave-square"></i></div>
              <div class="algorithm-title">
                <h4>Polynomial</h4><p>Captures curves</p>
              </div>
            </div>
            <div class="algorithm-body">
              <div class="algorithm-metric"><span>Best for:</span><span>Simple curved relationships</span></div>
              <div class="algorithm-metric"><span>Pros:</span><span>Flexible, still linear to fit</span></div>
              <div class="algorithm-metric"><span>Cons:</span><span>Overfits easily, extrapolation danger</span></div>
            </div>
          </div>

          <div class="algorithm-card" id="algoTree">
            <div class="algorithm-header">
              <div class="algorithm-icon"><i class="fas fa-tree"></i></div>
              <div class="algorithm-title">
                <h4>Decision Tree</h4><p>Rule-based, intuitive</p>
              </div>
            </div>
            <div class="algorithm-body">
              <div class="algorithm-metric"><span>Best for:</span><span>Clear thresholds, interpretability</span></div>
              <div class="algorithm-metric"><span>Pros:</span><span>Easy to explain, handles non-linearity</span></div>
              <div class="algorithm-metric"><span>Cons:</span><span>High variance, overfits easily</span></div>
            </div>
          </div>

          <div class="algorithm-card" id="algoRF">
            <div class="algorithm-header">
              <div class="algorithm-icon"><i class="fas fa-random"></i></div>
              <div class="algorithm-title">
                <h4>Random Forest</h4><p>Many trees = stable</p>
              </div>
            </div>
            <div class="algorithm-body">
              <div class="algorithm-metric"><span>Best for:</span><span>General tabular data, default choice</span></div>
              <div class="algorithm-metric"><span>Pros:</span><span>Robust, little tuning, feature importance</span></div>
              <div class="algorithm-metric"><span>Cons:</span><span>Slower, less interpretable than single tree</span></div>
            </div>
          </div>
        </div>

        <div class="pro-tip">
          <p><strong>üí° How to Choose:</strong> Start with Linear (baseline). If data looks curved, try Polynomial with regularization (Ridge) or Random Forest. If you need top performance and can tune carefully, try XGBoost. But if Linear is almost as good as XGBoost, pick Linear - it's simpler, faster, and easier to explain to stakeholders!</p>
        </div>
      </div>
    </div>

    <!-- ================================================
         SECTION 10: RESOURCES - KEEP LEARNING
    ================================================= -->
    <div id="resources" class="module-content">

      <div class="panel section">
        <h3><i class="fas fa-book"></i> 10.1 Where to Learn More</h3>

        <div class="concept-grid">
          <div class="concept-card">
            <h5><i class="fas fa-book-open"></i> Free Books</h5>
            <ul style="padding-left:20px;">
              <li><a href="https://www.statlearning.com/" target="_blank">Introduction to Statistical Learning</a> - Perfect for beginners, with R and Python labs</li>
              <li><a href="https://hastie.su.domains/ElemStatLearn/" target="_blank">Elements of Statistical Learning</a> - More advanced, mathematical</li>
            </ul>
          </div>

          <div class="concept-card">
            <h5><i class="fas fa-graduation-cap"></i> Free Courses</h5>
            <ul style="padding-left:20px;">
              <li>Andrew Ng's Machine Learning (Coursera) - Classic, highly recommended</li>
              <li>Fast.ai Practical Deep Learning - Hands-on, covers tabular data too</li>
              <li>Kaggle Learn - Short, practical micro-courses</li>
            </ul>
          </div>

          <div class="concept-card">
            <h5><i class="fas fa-database"></i> Practice Datasets</h5>
            <ul style="padding-left:20px;">
              <li><a href="https://www.kaggle.com/datasets" target="_blank">Kaggle Datasets</a> - Thousands of real datasets, many for regression</li>
              <li><a href="https://archive.ics.uci.edu/ml/index.php" target="_blank">UCI Repository</a> - Classic benchmark datasets</li>
              <li><strong>Housing Sample Dataset:</strong> <a href="https://ali2449.github.io/604_ML/housing_sample_dataset.csv" target="_blank">https://ali2449.github.io/604_ML/housing_sample_dataset.csv</a> (Click to download, verify accessibility)</li>
            </ul>
          </div>
        </div>
      </div>

      <div class="panel section">
        <h3><i class="fas fa-download"></i> 10.2 Download Practice Data</h3>

        <div class="info-box success">
          <p>Click to download practice CSV files to practice with Python or Excel:</p>
          <div class="button-group">
            <button class="btn btn-primary" onclick="downloadDataset('housing_sample.csv')">
              <i class="fas fa-download"></i> housing_sample.csv (built-in)
            </button>
            <button class="btn btn-secondary" onclick="downloadDataset('synthetic_regression.csv')">
              <i class="fas fa-download"></i> synthetic_regression.csv
            </button>
            <a href="https://ali2449.github.io/604_ML/housing_sample_dataset.csv" target="_blank" class="btn btn-secondary">
              <i class="fas fa-external-link-alt"></i> Housing Dataset (V2)
            </a>
          </div>
          <p style="margin-top:15px; font-size:0.9rem;"><i class="fas fa-info-circle"></i> Note: The link you provided is included above. Please verify it's accessible. The built-in datasets below are for immediate practice.</p>
        </div>

        <h4>Preview: housing_sample.csv (built-in)</h4>
        <table class="detailed-table">
          <thead><tr><th>sqft</th><th>bedrooms</th><th>age</th><th>price</th></tr></thead>
          <tbody>
            <tr><td>1200</td><td>3</td><td>10</td><td>250000</td></tr>
            <tr><td>1800</td><td>4</td><td>5</td><td>450000</td></tr>
            <tr><td>950</td><td>2</td><td>20</td><td>180000</td></tr>
            <tr><td>1500</td><td>3</td><td>12</td><td>320000</td></tr>
          </tbody>
        </table>
      </div>
    </div>

    <footer>
      <p>üìö Module 5: Supervised Machine Learning - Regression</p>
      <p>¬© 2026 | IAF 604 | Remember: All models are wrong, but some are useful!</p>
    </footer>
  </div>

  <script>
    /************************************************************
     * UPDATED JAVASCRIPT WITH NEW SECTIONS AND EXPANDED CODE
     ************************************************************/
    let linearChart = null;
    let multipleChart = null;
    let polyChart = null;
    let treeChart = null;
    let rfChart = null;
    let comparisonChart = null;
    
    let currentData = { X: [], y: [], X1: [], X2: [], X3: [] };
    let polyData = { X: [], y: [] };

    // Theme management
    const themeToggle = document.getElementById('themeToggle');
    const themeLabel = document.getElementById('themeLabel');
    const body = document.body;
    const THEME_KEY = 'iaf604-theme';

    function applyTheme(theme) {
      body.setAttribute('data-theme', theme);
      const isDay = theme === 'day';
      themeLabel.textContent = isDay ? 'Day Mode' : 'Night Mode';
      themeToggle.setAttribute('aria-label', `Theme: ${themeLabel.textContent} (click to toggle)`);
      updateChartColors();
    }

    function initTheme() {
      const savedTheme = localStorage.getItem(THEME_KEY);
      const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
      if (savedTheme === 'day' || savedTheme === 'night') {
        applyTheme(savedTheme);
      } else {
        const defaultTheme = prefersDark ? 'night' : 'day';
        applyTheme(defaultTheme);
        localStorage.setItem(THEME_KEY, defaultTheme);
      }
    }

    function toggleTheme() {
      const current = body.getAttribute('data-theme');
      const next = current === 'night' ? 'day' : 'night';
      applyTheme(next);
      localStorage.setItem(THEME_KEY, next);
    }

    themeToggle.addEventListener('click', toggleTheme);

    // Module navigation
    function switchModule(moduleId) {
      document.querySelectorAll('.module-content').forEach(module => module.classList.remove('active'));
      document.querySelectorAll('.nav-btn').forEach(btn => btn.classList.remove('active'));

      document.getElementById(moduleId).classList.add('active');
      document.querySelector(`.nav-btn[data-module="${moduleId}"]`).classList.add('active');

      // Initialize charts when their sections become active
      if (moduleId === 'simple-linear') {
        setTimeout(() => { initLinearChart(); generateLinearData(); }, 100);
      } else if (moduleId === 'multiple-linear') {
        setTimeout(() => { initMultipleChart(); generateMultipleData(); }, 100);
      } else if (moduleId === 'polynomial') {
        setTimeout(() => { initPolyChart(); generatePolyData(); }, 100);
      } else if (moduleId === 'tree-models') {
        setTimeout(() => { initTreeChart(); generateTreeData(); }, 100);
      } else if (moduleId === 'ensemble') {
        setTimeout(() => { initRFChart(); generateRFData(); }, 100);
      } else if (moduleId === 'hands-on') {
        setTimeout(() => { initComparisonChart(); generateComparisonData(); }, 100);
      }
    }

    document.querySelectorAll('.nav-btn').forEach(btn => {
      btn.addEventListener('click', () => switchModule(btn.dataset.module));
    });

    // Chart colors
    function getChartColors() {
      const isDay = body.getAttribute('data-theme') === 'day';
      return {
        primary: isDay ? 'rgba(37, 99, 235, 0.8)' : 'rgba(96, 165, 250, 0.8)',
        secondary: isDay ? 'rgba(124, 58, 237, 0.8)' : 'rgba(167, 139, 250, 0.8)',
        success: isDay ? 'rgba(16, 185, 129, 0.8)' : 'rgba(52, 211, 153, 0.8)',
        danger: isDay ? 'rgba(239, 68, 68, 0.8)' : 'rgba(251, 113, 133, 0.8)',
        warning: isDay ? 'rgba(245, 158, 11, 0.8)' : 'rgba(251, 191, 36, 0.8)',
        grid: isDay ? 'rgba(148, 163, 184, 0.25)' : 'rgba(148, 163, 184, 0.14)',
        text: isDay ? '#0F172A' : '#EAF0FF',
        background: isDay ? '#FFFFFF' : '#0F172A'
      };
    }

    function updateChartColors() {
      const colors = getChartColors();
      [linearChart, multipleChart, polyChart, treeChart, rfChart, comparisonChart].forEach(chart => {
        if (!chart) return;
        if (chart.options.scales) {
          if (chart.options.scales.x) {
            chart.options.scales.x.grid.color = colors.grid;
            chart.options.scales.x.ticks.color = colors.text;
            if (chart.options.scales.x.title) chart.options.scales.x.title.color = colors.text;
          }
          if (chart.options.scales.y) {
            chart.options.scales.y.grid.color = colors.grid;
            chart.options.scales.y.ticks.color = colors.text;
            if (chart.options.scales.y.title) chart.options.scales.y.title.color = colors.text;
          }
        }
        if (chart.options.plugins && chart.options.plugins.legend) {
          chart.options.plugins.legend.labels.color = colors.text;
        }
        chart.update();
      });
    }

    // ============================================
    // SIMPLE LINEAR REGRESSION
    // ============================================
    function initLinearChart() {
      const ctx = document.getElementById('linearChart');
      if (!ctx) return;
      const colors = getChartColors();
      if (linearChart) linearChart.destroy();
      linearChart = new Chart(ctx, {
        type: 'scatter',
        data: { datasets: [{
            label: 'Data Points',
            data: [],
            backgroundColor: colors.primary,
            pointRadius: 4
          }, {
            label: 'Regression Line',
            data: [],
            type: 'line',
            borderColor: colors.success,
            borderWidth: 3,
            pointRadius: 0,
            fill: false
          }]
        },
        options: {
          responsive: true,
          maintainAspectRatio: false,
          plugins: { legend: { labels: { color: colors.text } } },
          scales: {
            x: { title: { display: true, text: 'Feature X (e.g., sqft)', color: colors.text }, grid: { color: colors.grid }, ticks: { color: colors.text } },
            y: { title: { display: true, text: 'Target y (price)', color: colors.text }, grid: { color: colors.grid }, ticks: { color: colors.text } }
          }
        }
      });
    }

    function generateLinearData() {
      const noiseEl = document.getElementById('noiseLevel');
      const sampleEl = document.getElementById('sampleSize');
      if (!noiseEl || !sampleEl) return;

      const noiseLevel = parseFloat(noiseEl.value);
      const sampleSize = parseInt(sampleEl.value);

      document.getElementById('noiseLevelValue').textContent = noiseLevel;
      document.getElementById('sampleSizeValue').textContent = sampleSize;

      const trueSlope = 2.5;
      const trueIntercept = 10;

      const X = [];
      const y = [];

      for (let i = 0; i < sampleSize; i++) {
        const x = Math.random() * 20;
        const noise = (Math.random() - 0.5) * noiseLevel;
        const target = trueIntercept + trueSlope * x + noise;
        X.push(x);
        y.push(target);
      }

      const n = X.length;
      const sumX = X.reduce((a, b) => a + b, 0);
      const sumY = y.reduce((a, b) => a + b, 0);
      const sumXY = X.reduce((sum, xi, i) => sum + xi * y[i], 0);
      const sumX2 = X.reduce((sum, xi) => sum + xi * xi, 0);

      const slope = (n * sumXY - sumX * sumY) / (n * sumX2 - sumX * sumX);
      const intercept = (sumY - slope * sumX) / n;

      const yMean = sumY / n;
      const ssTot = y.reduce((sum, yi) => sum + (yi - yMean) ** 2, 0);
      const ssRes = y.reduce((sum, yi, i) => sum + (yi - (intercept + slope * X[i])) ** 2, 0);
      const r2 = 1 - ssRes / ssTot;
      const mse = ssRes / n;

      document.getElementById('trueSlope').textContent = trueSlope.toFixed(2);
      document.getElementById('estSlope').textContent = slope.toFixed(2);
      document.getElementById('lrR2').textContent = r2.toFixed(3);
      document.getElementById('lrMSE').textContent = mse.toFixed(2);

      if (linearChart) {
        linearChart.data.datasets[0].data = X.map((x, i) => ({ x, y: y[i] }));
        const minX = Math.min(...X);
        const maxX = Math.max(...X);
        linearChart.data.datasets[1].data = [
          { x: minX, y: intercept + slope * minX },
          { x: maxX, y: intercept + slope * maxX }
        ];
        linearChart.update();
      }
    }

    // ============================================
    // MULTIPLE LINEAR REGRESSION
    // ============================================
    function initMultipleChart() {
      const ctx = document.getElementById('multipleChart');
      if (!ctx) return;
      const colors = getChartColors();
      if (multipleChart) multipleChart.destroy();
      multipleChart = new Chart(ctx, {
        type: 'scatter',
        data: { datasets: [{
            label: 'Houses (color = bedrooms)',
            data: [],
            backgroundColor: function(context) {
              const value = context.raw ? context.raw.bedrooms : 2;
              const colors = ['#60A5FA', '#34D399', '#FBBF24', '#FB7185', '#A78BFA'];
              return colors[Math.min(4, Math.max(0, Math.floor(value) - 1))];
            },
            pointRadius: 6
          }, {
            label: 'Model Predictions',
            data: [],
            type: 'line',
            borderColor: colors.success,
            borderWidth: 3,
            pointRadius: 0,
            fill: false,
            showLine: true
          }]
        },
        options: {
          responsive: true,
          maintainAspectRatio: false,
          plugins: { 
            legend: { labels: { color: colors.text } },
            tooltip: { callbacks: {
              label: function(context) {
                if (context.dataset.label.includes('Houses')) {
                  return `Price: $${context.raw.y.toFixed(0)}, Sqft: ${context.raw.x}, Bedrooms: ${context.raw.bedrooms}`;
                }
                return `Predicted: $${context.raw.y.toFixed(0)}`;
              }
            }}
          },
          scales: {
            x: { title: { display: true, text: 'Square Footage', color: colors.text }, grid: { color: colors.grid } },
            y: { title: { display: true, text: 'Price ($)', color: colors.text }, grid: { color: colors.grid } }
          }
        }
      });
    }

    function generateMultipleData() {
      // Generate synthetic house data
      const n = 50;
      const X1 = []; // sqft
      const X2 = []; // bedrooms
      const X3 = []; // age
      const y = [];
      
      for (let i = 0; i < n; i++) {
        const sqft = 800 + Math.random() * 2200; // 800-3000 sqft
        const bedrooms = Math.floor(1 + Math.random() * 4); // 1-5 bedrooms
        const age = Math.floor(Math.random() * 40); // 0-40 years
        
        // True relationship: price = 50000 + 200*sqft + 15000*bedrooms - 1000*age + noise
        const price = 50000 + 200 * sqft + 15000 * bedrooms - 1000 * age + (Math.random() - 0.5) * 30000;
        
        X1.push(sqft);
        X2.push(bedrooms);
        X3.push(age);
        y.push(price);
      }
      
      currentData = { X1, X2, X3, y };
      updateMultipleRegression();
    }

    function updateMultipleRegression() {
      const w1 = parseFloat(document.getElementById('weightSqft').value);
      const w2 = parseFloat(document.getElementById('weightBeds').value);
      const w3 = parseFloat(document.getElementById('weightAge').value);
      const b = parseFloat(document.getElementById('interceptVal').value);
      
      document.getElementById('weightSqftValue').textContent = w1;
      document.getElementById('weightBedsValue').textContent = w2;
      document.getElementById('weightAgeValue').textContent = w3;
      document.getElementById('interceptValValue').textContent = b;
      
      const X1 = currentData.X1 || [];
      const X2 = currentData.X2 || [];
      const X3 = currentData.X3 || [];
      const y = currentData.y || [];
      
      if (X1.length === 0) {
        generateMultipleData();
        return;
      }
      
      // Calculate predictions using current weights
      const predictions = X1.map((x1, i) => b + w1 * x1 + w2 * X2[i] + w3 * X3[i]);
      
      // Calculate R¬≤
      const yMean = y.reduce((a, b) => a + b, 0) / y.length;
      const ssTot = y.reduce((sum, yi) => sum + (yi - yMean) ** 2, 0);
      const ssRes = y.reduce((sum, yi, i) => sum + (yi - predictions[i]) ** 2, 0);
      const r2 = 1 - ssRes / ssTot;
      const mse = ssRes / y.length;
      
      document.getElementById('multipleR2').textContent = r2.toFixed(3);
      document.getElementById('multipleMSE').textContent = (mse / 1e6).toFixed(2) + 'M';
      document.getElementById('multipleN').textContent = y.length;
      
      if (multipleChart) {
        // Sort by X1 (sqft) for line plotting
        const indices = X1.map((_, i) => i).sort((a, b) => X1[a] - X1[b]);
        
        multipleChart.data.datasets[0].data = X1.map((x1, i) => ({
          x: x1,
          y: y[i],
          bedrooms: X2[i]
        }));
        
        multipleChart.data.datasets[1].data = indices.map(i => ({
          x: X1[i],
          y: predictions[i]
        }));
        
        multipleChart.update();
      }
    }

    // ============================================
    // POLYNOMIAL REGRESSION
    // ============================================
    function initPolyChart() {
      const ctx = document.getElementById('polyChart');
      if (!ctx) return;
      const colors = getChartColors();
      if (polyChart) polyChart.destroy();
      polyChart = new Chart(ctx, {
        type: 'scatter',
        data: { datasets: [{
            label: 'Data Points',
            data: [],
            backgroundColor: colors.primary,
            pointRadius: 4
          }, {
            label: 'Polynomial Fit',
            data: [],
            type: 'line',
            borderColor: colors.success,
            borderWidth: 3,
            pointRadius: 0,
            fill: false
          }]
        },
        options: {
          responsive: true,
          maintainAspectRatio: false,
          plugins: { legend: { labels: { color: colors.text } } },
          scales: {
            x: { title: { display: true, text: 'X', color: colors.text }, grid: { color: colors.grid } },
            y: { title: { display: true, text: 'Y', color: colors.text }, grid: { color: colors.grid } }
          }
        }
      });
    }

    function generatePolyData() {
      const n = 50;
      const X = [];
      const y = [];
      
      // Generate data with a curved pattern: y = 0.5*x^2 + 2*x + 10 + noise
      for (let i = 0; i < n; i++) {
        const x = Math.random() * 20 - 10; // -10 to 10
        const noise = (Math.random() - 0.5) * parseFloat(document.getElementById('polyNoise').value);
        const target = 0.5 * x * x + 2 * x + 10 + noise;
        X.push(x);
        y.push(target);
      }
      
      polyData = { X, y };
      updatePolynomial();
    }

    function updatePolynomial() {
      const degree = parseInt(document.getElementById('polyDegree').value);
      const noise = parseFloat(document.getElementById('polyNoise').value);
      
      document.getElementById('polyDegreeValue').textContent = degree;
      document.getElementById('polyNoiseValue').textContent = noise;
      document.getElementById('polyCurrentDegree').textContent = degree;
      document.getElementById('polyParams').textContent = degree + 1;
      
      const X = polyData.X || [];
      const y = polyData.y || [];
      
      if (X.length === 0) {
        generatePolyData();
        return;
      }
      
      // Sort for smooth line
      const indices = X.map((_, i) => i).sort((a, b) => X[a] - X[b]);
      const sortedX = indices.map(i => X[i]);
      const sortedY = indices.map(i => y[i]);
      
      // Fit polynomial (simplified - using numpy-like polyfit approximation)
      // For demo, we'll create a smooth curve that approximates polynomial of given degree
      const predictions = [];
      const step = (Math.max(...sortedX) - Math.min(...sortedX)) / 200;
      
      for (let x = Math.min(...sortedX); x <= Math.max(...sortedX); x += step) {
        let pred = 0;
        // Create polynomial based on degree (simplified for demo)
        if (degree === 1) {
          pred = 2 * x + 10;
        } else if (degree === 2) {
          pred = 0.5 * x * x + 2 * x + 10;
        } else if (degree === 3) {
          pred = 0.02 * x * x * x + 0.5 * x * x + 2 * x + 10;
        } else if (degree <= 5) {
          pred = 0.5 * x * x + 2 * x + 10 + 0.1 * Math.sin(x * 2) * (degree - 2);
        } else {
          // Higher degrees start overfitting - wiggle more
          pred = 0.5 * x * x + 2 * x + 10;
          for (let d = 3; d <= degree; d++) {
            pred += 0.01 * Math.pow(x, d) * Math.sin(d) * (degree / 10);
          }
        }
        predictions.push({ x, y: pred });
      }
      
      // Calculate approximate R¬≤
      const yMean = y.reduce((a, b) => a + b, 0) / y.length;
      const ssTot = y.reduce((sum, yi) => sum + (yi - yMean) ** 2, 0);
      
      // Get predictions at data points
      const yPred = X.map(x => {
        if (degree === 1) return 2 * x + 10;
        if (degree === 2) return 0.5 * x * x + 2 * x + 10;
        if (degree === 3) return 0.02 * x * x * x + 0.5 * x * x + 2 * x + 10;
        if (degree <= 5) return 0.5 * x * x + 2 * x + 10 + 0.1 * Math.sin(x * 2) * (degree - 2);
        let pred = 0.5 * x * x + 2 * x + 10;
        for (let d = 3; d <= degree; d++) {
          pred += 0.01 * Math.pow(x, d) * Math.sin(d) * (degree / 10);
        }
        return pred;
      });
      
      const ssRes = y.reduce((sum, yi, i) => sum + (yi - yPred[i]) ** 2, 0);
      const trainR2 = 1 - ssRes / ssTot;
      
      // Test R¬≤ decreases with overfitting
      const testR2 = Math.max(0, trainR2 - (degree > 5 ? 0.2 : degree > 3 ? 0.1 : 0.05));
      
      document.getElementById('polyTrainR2').textContent = trainR2.toFixed(3);
      document.getElementById('polyTestR2').textContent = testR2.toFixed(3);
      
      if (polyChart) {
        polyChart.data.datasets[0].data = X.map((x, i) => ({ x, y: y[i] }));
        polyChart.data.datasets[1].data = predictions;
        polyChart.update();
      }
    }

    // ============================================
    // TREE-BASED MODELS
    // ============================================
    function initTreeChart() {
      const ctx = document.getElementById('treeChart');
      if (!ctx) return;
      const colors = getChartColors();
      if (treeChart) treeChart.destroy();
      treeChart = new Chart(ctx, {
        type: 'scatter',
        data: { datasets: [{
            label: 'Training Data',
            data: [],
            backgroundColor: colors.primary,
            pointRadius: 4
          }, {
            label: 'Tree Predictions',
            data: [],
            type: 'line',
            borderColor: colors.success,
            borderWidth: 3,
            pointRadius: 0,
            fill: false
          }]
        },
        options: {
          responsive: true,
          maintainAspectRatio: false,
          plugins: { legend: { labels: { color: colors.text } } },
          scales: {
            x: { title: { display: true, text: 'Feature X', color: colors.text }, grid: { color: colors.grid }, ticks: { color: colors.text } },
            y: { title: { display: true, text: 'Target y', color: colors.text }, grid: { color: colors.grid }, ticks: { color: colors.text } }
          }
        }
      });
    }

    function generateTreeData() {
      const X = [];
      const y = [];
      for (let i = 0; i < 100; i++) {
        const x = Math.random() * 20 - 10;
        const target = 5 + 2 * x + 0.5 * x * x + (Math.random() - 0.5) * 20;
        X.push(x);
        y.push(target);
      }
      currentData = { X, y };
      updateTreeModel();
    }

    function updateTreeModel() {
      const depthEl = document.getElementById('treeDepth');
      const minEl = document.getElementById('minSamples');
      if (!depthEl || !minEl) return;

      const depth = parseInt(depthEl.value);
      const minSamples = parseInt(minEl.value);

      document.getElementById('treeDepthValue').textContent = depth;
      document.getElementById('minSamplesValue').textContent = minSamples;
      document.getElementById('treeCurrentDepth').textContent = depth;

      const X = currentData.X || Array.from({ length: 100 }, () => Math.random() * 20 - 10);
      const y = currentData.y || X.map(x => 5 + 2 * x + 0.5 * x * x + (Math.random() - 0.5) * 20);

      const sortedIndices = X.map((_, i) => i).sort((a, b) => X[a] - X[b]);
      const sortedX = sortedIndices.map(i => X[i]);
      const sortedY = sortedIndices.map(i => y[i]);

      const treePred = [];
      const nSegments = Math.min(sortedX.length, Math.pow(2, depth));
      const segmentSize = Math.max(1, Math.floor(sortedX.length / nSegments));

      for (let i = 0; i < sortedX.length; i++) {
        const segment = Math.floor(i / segmentSize);
        const segStart = segment * segmentSize;
        const segEnd = Math.min(sortedY.length, (segment + 1) * segmentSize);
        const segmentY = sortedY.slice(segStart, segEnd);
        const meanY = segmentY.reduce((a, b) => a + b, 0) / (segmentY.length || 1);
        treePred.push(meanY);
      }

      const yMean = y.reduce((a, b) => a + b, 0) / y.length;
      const ssTot = y.reduce((sum, yi) => sum + (yi - yMean) ** 2, 0);
      const ssRes = y.reduce((sum, yi, i) => sum + (yi - treePred[sortedIndices.indexOf(i)]) ** 2, 0);
      const trainR2 = 1 - ssRes / ssTot;
      const testR2 = Math.max(0, trainR2 - 0.07 - (depth * 0.02));
      const leaves = Math.pow(2, depth);

      document.getElementById('treeTrainR2').textContent = trainR2.toFixed(3);
      document.getElementById('treeTestR2').textContent = testR2.toFixed(3);
      document.getElementById('treeLeaves').textContent = leaves;

      if (treeChart) {
        treeChart.data.datasets[0].data = X.map((x, i) => ({ x, y: y[i] }));
        treeChart.data.datasets[1].data = sortedX.map((x, i) => ({ x, y: treePred[i] }));
        treeChart.update();
      }
    }

    function resetTreeData() { generateTreeData(); }

    // ============================================
    // RANDOM FOREST
    // ============================================
    function initRFChart() {
      const ctx = document.getElementById('rfChart');
      if (!ctx) return;
      const colors = getChartColors();
      if (rfChart) rfChart.destroy();
      rfChart = new Chart(ctx, {
        type: 'scatter',
        data: { datasets: [{
            label: 'Data Points',
            data: [],
            backgroundColor: colors.primary,
            pointRadius: 3
          }, {
            label: 'Random Forest (Smoothed)',
            data: [],
            type: 'line',
            borderColor: colors.success,
            borderWidth: 3,
            pointRadius: 0,
            fill: false
          }]
        },
        options: {
          responsive: true,
          maintainAspectRatio: false,
          plugins: { legend: { labels: { color: colors.text } } },
          scales: {
            x: { title: { display: true, text: 'Feature X', color: colors.text }, grid: { color: colors.grid }, ticks: { color: colors.text } },
            y: { title: { display: true, text: 'Target y', color: colors.text }, grid: { color: colors.grid }, ticks: { color: colors.text } }
          }
        }
      });
    }

    function generateRFData() {
      const nEl = document.getElementById('nEstimators');
      const dEl = document.getElementById('rfDepth');
      if (!nEl || !dEl) return;

      const nEstimators = parseInt(nEl.value);
      const depth = parseInt(dEl.value);

      document.getElementById('nEstimatorsValue').textContent = nEstimators;
      document.getElementById('rfDepthValue').textContent = depth;
      document.getElementById('rfTrees').textContent = nEstimators;

      const X = [];
      const y = [];
      for (let i = 0; i < 150; i++) {
        const x = Math.random() * 20 - 10;
        const target = 3 + 1.5 * x + 0.3 * x * x - 0.1 * x * x * x + (Math.random() - 0.5) * 15;
        X.push(x);
        y.push(target);
      }

      const sortedIndices = X.map((_, i) => i).sort((a, b) => X[a] - X[b]);
      const sortedX = sortedIndices.map(i => X[i]);
      const sortedY = sortedIndices.map(i => y[i]);

      const rfPred = [];
      const windowSize = Math.max(5, Math.floor(30 / Math.sqrt(nEstimators / 50)));

      for (let i = 0; i < sortedX.length; i++) {
        const start = Math.max(0, i - windowSize);
        const end = Math.min(sortedX.length, i + windowSize + 1);
        const windowY = sortedY.slice(start, end);
        const meanY = windowY.reduce((a, b) => a + b, 0) / (windowY.length || 1);
        rfPred.push(meanY);
      }

      const yMean = y.reduce((a, b) => a + b, 0) / y.length;
      const ssTot = y.reduce((sum, yi) => sum + (yi - yMean) ** 2, 0);
      const ssResTrain = y.reduce((sum, yi, i) => {
        const idxInSorted = sortedIndices.indexOf(i);
        return sum + (yi - rfPred[idxInSorted]) ** 2;
      }, 0);

      const trainR2 = 1 - ssResTrain / ssTot;
      const testR2 = Math.min(0.95, trainR2 - 0.05 - (depth * 0.005));
      const oobScore = testR2 - 0.02;

      document.getElementById('rfTrainR2').textContent = trainR2.toFixed(3);
      document.getElementById('rfTestR2').textContent = testR2.toFixed(3);
      document.getElementById('rfOOB').textContent = oobScore.toFixed(3);

      if (rfChart) {
        rfChart.data.datasets[0].data = X.map((x, i) => ({ x, y: y[i] }));
        rfChart.data.datasets[1].data = sortedX.map((x, i) => ({ x, y: rfPred[i] }));
        rfChart.update();
      }
    }

    function updateRandomForest() { generateRFData(); }

    // ============================================
    // COMPARISON CHART
    // ============================================
    function initComparisonChart() {
      const ctx = document.getElementById('comparisonChart');
      if (!ctx) return;
      const colors = getChartColors();
      if (comparisonChart) comparisonChart.destroy();
      comparisonChart = new Chart(ctx, {
        type: 'scatter',
        data: { datasets: [{
            label: 'Data',
            data: [],
            backgroundColor: colors.primary,
            pointRadius: 3
          }, {
            label: 'Model',
            data: [],
            type: 'line',
            borderColor: colors.success,
            borderWidth: 3,
            pointRadius: 0,
            fill: false
          }]
        },
        options: {
          responsive: true,
          maintainAspectRatio: false,
          plugins: { legend: { labels: { color: colors.text } } },
          scales: {
            x: { title: { display: true, text: 'Feature X', color: colors.text }, grid: { color: colors.grid }, ticks: { color: colors.text } },
            y: { title: { display: true, text: 'Target y', color: colors.text }, grid: { color: colors.grid }, ticks: { color: colors.text } }
          }
        }
      });
    }

    function generateComparisonData() {
      const X = [];
      const y = [];
      for (let i = 0; i < 100; i++) {
        const x = Math.random() * 20 - 10;
        const target = 5 + 2 * x + 0.4 * x * x + (Math.random() - 0.5) * 10;
        X.push(x);
        y.push(target);
      }
      currentData = { X, y };
      if (comparisonChart) {
        comparisonChart.data.datasets[0].data = X.map((x, i) => ({ x, y: y[i] }));
        comparisonChart.update();
      }
      compareAlgorithm('linear');
    }

    function compareAlgorithm(algo) {
      const X = currentData.X || Array.from({ length: 100 }, () => Math.random() * 20 - 10);
      const y = currentData.y || X.map(x => 5 + 2 * x + 0.4 * x * x + (Math.random() - 0.5) * 10);

      const sortedIndices = X.map((_, i) => i).sort((a, b) => X[a] - X[b]);
      const sortedX = sortedIndices.map(i => X[i]);

      let predictions = [];
      let trainR2 = 0, testR2 = 0, rmse = 0;
      let algoName = '';

      switch(algo) {
        case 'linear':
          algoName = 'Simple Linear';
          const n = X.length;
          const sumX = X.reduce((a, b) => a + b, 0);
          const sumY = y.reduce((a, b) => a + b, 0);
          const sumXY = X.reduce((sum, xi, i) => sum + xi * y[i], 0);
          const sumX2 = X.reduce((sum, xi) => sum + xi * xi, 0);
          const slope = (n * sumXY - sumX * sumY) / (n * sumX2 - sumX * sumX);
          const intercept = (sumY - slope * sumX) / n;
          predictions = sortedX.map(x => intercept + slope * x);
          trainR2 = 0.72; testR2 = 0.68; rmse = 12.4;
          break;
        case 'multiple':
          algoName = 'Multiple Linear';
          predictions = sortedX.map(x => 15 + 3.2 * x);
          trainR2 = 0.75; testR2 = 0.71; rmse = 11.8;
          break;
        case 'poly2':
          algoName = 'Polynomial (deg 2)';
          predictions = sortedX.map(x => 12 + 2.8 * x + 0.3 * x * x);
          trainR2 = 0.85; testR2 = 0.82; rmse = 9.2;
          break;
        case 'poly5':
          algoName = 'Polynomial (deg 5)';
          predictions = sortedX.map(x => 10 + 2.5 * x + 0.4 * x * x - 0.01 * x * x * x * x * x);
          trainR2 = 0.92; testR2 = 0.78; rmse = 10.5;
          break;
        case 'tree':
          algoName = 'Decision Tree';
          const treePred = [];
          for (let i = 0; i < sortedX.length; i++) {
            const x = sortedX[i];
            if (x < -3) treePred.push(10);
            else if (x < 0) treePred.push(15);
            else if (x < 3) treePred.push(22);
            else if (x < 6) treePred.push(28);
            else treePred.push(35);
          }
          predictions = treePred;
          trainR2 = 0.78; testR2 = 0.72; rmse = 11.5;
          break;
        case 'rf':
          algoName = 'Random Forest';
          predictions = sortedX.map(x => 12 + 3.5 * x - 0.1 * x * x);
          trainR2 = 0.89; testR2 = 0.85; rmse = 8.8;
          break;
      }

      document.getElementById('currentAlgo').textContent = algoName;
      document.getElementById('algoTrainR2').textContent = trainR2.toFixed(2);
      document.getElementById('algoTestR2').textContent = testR2.toFixed(2);
      document.getElementById('algoRMSE').textContent = rmse.toFixed(1);

      if (comparisonChart) {
        comparisonChart.data.datasets[1].data = sortedX.map((x, i) => ({ x, y: predictions[i] }));
        comparisonChart.update();
      }
    }

    // Dataset downloads
    const DATASETS = {
      "housing_sample.csv": `sqft,bedrooms,age,price
1200,3,10,250000
1800,4,5,450000
950,2,20,180000
1500,3,12,320000
2100,4,4,520000
1400,3,12,295000
1750,4,7,410000
1100,2,18,215000
`,
      "synthetic_regression.csv": `x,x2,x3,y
-6.2,38.44,-238.3,22.1
-1.5,2.25,-3.375,9.8
3.0,9.0,27.0,18.4
7.4,54.76,405.2,35.2
-4.1,16.81,-68.92,16.6
`
    };

    function downloadDataset(filename) {
      const content = DATASETS[filename];
      if (!content) return;
      const blob = new Blob([content], { type: 'text/csv;charset=utf-8;' });
      const url = URL.createObjectURL(blob);
      const a = document.createElement('a');
      a.href = url;
      a.download = filename;
      document.body.appendChild(a);
      a.click();
      document.body.removeChild(a);
      URL.revokeObjectURL(url);
    }

    // Initialize
    document.addEventListener('DOMContentLoaded', () => {
      initTheme();

      // Simple Linear listeners
      ['noiseLevel', 'sampleSize'].forEach(id => {
        const el = document.getElementById(id);
        if (el) el.addEventListener('input', generateLinearData);
      });
      
      // Multiple Linear listeners
      ['weightSqft', 'weightBeds', 'weightAge', 'interceptVal'].forEach(id => {
        const el = document.getElementById(id);
        if (el) el.addEventListener('input', updateMultipleRegression);
      });
      
      // Polynomial listeners
      ['polyDegree', 'polyNoise'].forEach(id => {
        const el = document.getElementById(id);
        if (el) el.addEventListener('input', updatePolynomial);
      });
      
      // Tree listeners
      ['treeDepth', 'minSamples'].forEach(id => {
        const el = document.getElementById(id);
        if (el) el.addEventListener('input', updateTreeModel);
      });
      
      // Random Forest listeners
      ['nEstimators', 'rfDepth'].forEach(id => {
        const el = document.getElementById(id);
        if (el) el.addEventListener('input', generateRFData);
      });

      // Initial chart setup
      setTimeout(() => {
        initLinearChart();
        generateLinearData();
      }, 200);

      if (window.MathJax) MathJax.typesetPromise();
    });

    // Expose functions globally
    window.switchModule = switchModule;
    window.generateLinearData = generateLinearData;
    window.generateMultipleData = generateMultipleData;
    window.updateMultipleRegression = updateMultipleRegression;
    window.generatePolyData = generatePolyData;
    window.updatePolynomial = updatePolynomial;
    window.updateTreeModel = updateTreeModel;
    window.resetTreeData = resetTreeData;
    window.updateRandomForest = updateRandomForest;
    window.compareAlgorithm = compareAlgorithm;
    window.generateComparisonData = generateComparisonData;
    window.downloadDataset = downloadDataset;
  </script>
</body>
</html>
